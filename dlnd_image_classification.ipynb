{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 图像分类\n",
    "\n",
    "在此项目中，你将对 [CIFAR-10 数据集](https://www.cs.toronto.edu/~kriz/cifar.html) 中的图片进行分类。该数据集包含飞机、猫狗和其他物体。你需要预处理这些图片，然后用所有样本训练一个卷积神经网络。图片需要标准化（normalized），标签需要采用 one-hot 编码。你需要应用所学的知识构建卷积的、最大池化（max pooling）、丢弃（dropout）和完全连接（fully connected）的层。最后，你需要在样本图片上看到神经网络的预测结果。\n",
    "\n",
    "\n",
    "## 获取数据\n",
    "\n",
    "请运行以下单元，以下载 [CIFAR-10 数据集（Python版）](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据\n",
    "\n",
    "该数据集分成了几部分／批次（batches），以免你的机器在计算时内存不足。CIFAR-10 数据集包含 5 个部分，名称分别为 `data_batch_1`、`data_batch_2`，以此类推。每个部分都包含以下某个类别的标签和图片：\n",
    "\n",
    "* 飞机\n",
    "* 汽车\n",
    "* 鸟类\n",
    "* 猫\n",
    "* 鹿\n",
    "* 狗\n",
    "* 青蛙\n",
    "* 马\n",
    "* 船只\n",
    "* 卡车\n",
    "\n",
    "了解数据集也是对数据进行预测的必经步骤。你可以通过更改 `batch_id` 和 `sample_id` 探索下面的代码单元。`batch_id` 是数据集一个部分的 ID（1 到 5）。`sample_id` 是该部分中图片和标签对（label pair）的 ID。\n",
    "\n",
    "问问你自己：“可能的标签有哪些？”、“图片数据的值范围是多少？”、“标签是按顺序排列，还是随机排列的？”。思考类似的问题，有助于你预处理数据，并使预测结果更准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 2:\n",
      "Image - Min Value: 0 Max Value: 240\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKEzRA0Bh/xOcxDplVQV0VQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2952c647c88>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 2\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现预处理函数\n",
    "\n",
    "### 标准化\n",
    "\n",
    "在下面的单元中，实现 `normalize` 函数，传入图片数据 `x`，并返回标准化 Numpy 数组。值应该在 0 到 1 的范围内（含 0 和 1）。返回对象应该和 `x` 的形状一样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a = 0\n",
    "    b = 1\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    \n",
    "    a += ((x - grayscale_min)*(b - a))/(grayscale_max - grayscale_min)\n",
    "    return a\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot 编码\n",
    "\n",
    "和之前的代码单元一样，你将为预处理实现一个函数。这次，你将实现 `one_hot_encode` 函数。输入，也就是 `x`，是一个标签列表。实现该函数，以返回为 one_hot 编码的 Numpy 数组的标签列表。标签的可能值为 0 到 9。每次调用 `one_hot_encode` 时，对于每个值，one_hot 编码函数应该返回相同的编码。确保将编码映射保存到该函数外面。\n",
    "\n",
    "提示：不要重复发明轮子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x = np.array(x).reshape((-1,1))\n",
    "    onehot = helper.LabelBinarizer()\n",
    "    onehot.fit([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    a = onehot.transform(x)\n",
    "    return a\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机化数据\n",
    "\n",
    "之前探索数据时，你已经了解到，样本的顺序是随机的。再随机化一次也不会有什么关系，但是对于这个数据集没有必要。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理所有数据并保存\n",
    "\n",
    "运行下方的代码单元，将预处理所有 CIFAR-10 数据，并保存到文件中。下面的代码还使用了 10% 的训练数据，用来验证。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "这是你的第一个检查点。如果你什么时候决定再回到该记事本，或需要重新启动该记事本，你可以从这里开始。预处理的数据已保存到本地。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels[122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.54901961,  0.49019608,  0.45098039],\n",
       "         [ 0.57254902,  0.50980392,  0.47843137],\n",
       "         [ 0.56078431,  0.49803922,  0.47843137],\n",
       "         ..., \n",
       "         [ 0.66666667,  0.56862745,  0.51372549],\n",
       "         [ 0.69019608,  0.58823529,  0.5254902 ],\n",
       "         [ 0.66666667,  0.57647059,  0.52156863]],\n",
       "\n",
       "        [[ 0.4745098 ,  0.42352941,  0.50588235],\n",
       "         [ 0.50980392,  0.4627451 ,  0.54509804],\n",
       "         [ 0.5254902 ,  0.4745098 ,  0.56078431],\n",
       "         ..., \n",
       "         [ 0.63921569,  0.55294118,  0.61568627],\n",
       "         [ 0.66666667,  0.57254902,  0.63137255],\n",
       "         [ 0.66666667,  0.58039216,  0.63137255]],\n",
       "\n",
       "        [[ 0.59607843,  0.54509804,  0.68235294],\n",
       "         [ 0.61568627,  0.56862745,  0.70196078],\n",
       "         [ 0.60784314,  0.56078431,  0.68627451],\n",
       "         ..., \n",
       "         [ 0.69411765,  0.60392157,  0.75686275],\n",
       "         [ 0.70980392,  0.61176471,  0.76078431],\n",
       "         [ 0.71764706,  0.62745098,  0.76078431]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.49019608,  0.43137255,  0.4       ],\n",
       "         [ 0.50588235,  0.43921569,  0.40392157],\n",
       "         [ 0.29803922,  0.2627451 ,  0.18431373],\n",
       "         ..., \n",
       "         [ 0.65882353,  0.5372549 ,  0.47058824],\n",
       "         [ 0.61960784,  0.49411765,  0.40392157],\n",
       "         [ 0.57254902,  0.45490196,  0.34117647]],\n",
       "\n",
       "        [[ 0.33333333,  0.30196078,  0.2745098 ],\n",
       "         [ 0.36862745,  0.31764706,  0.27843137],\n",
       "         [ 0.29019608,  0.25490196,  0.17647059],\n",
       "         ..., \n",
       "         [ 0.63529412,  0.51764706,  0.41568627],\n",
       "         [ 0.65098039,  0.5254902 ,  0.39215686],\n",
       "         [ 0.61960784,  0.50196078,  0.36078431]],\n",
       "\n",
       "        [[ 0.49019608,  0.43921569,  0.43529412],\n",
       "         [ 0.50980392,  0.44313725,  0.43529412],\n",
       "         [ 0.41176471,  0.35686275,  0.29411765],\n",
       "         ..., \n",
       "         [ 0.51764706,  0.41568627,  0.30588235],\n",
       "         [ 0.50980392,  0.39607843,  0.25098039],\n",
       "         [ 0.55686275,  0.45098039,  0.30588235]]],\n",
       "\n",
       "\n",
       "       [[[ 0.39215686,  0.42745098,  0.32941176],\n",
       "         [ 0.47843137,  0.49411765,  0.42745098],\n",
       "         [ 0.34117647,  0.34117647,  0.29803922],\n",
       "         ..., \n",
       "         [ 0.29411765,  0.30588235,  0.27058824],\n",
       "         [ 0.2745098 ,  0.28627451,  0.25098039],\n",
       "         [ 0.2745098 ,  0.28627451,  0.25098039]],\n",
       "\n",
       "        [[ 0.3372549 ,  0.38823529,  0.27843137],\n",
       "         [ 0.29803922,  0.32941176,  0.25882353],\n",
       "         [ 0.23529412,  0.25098039,  0.21176471],\n",
       "         ..., \n",
       "         [ 0.30588235,  0.31764706,  0.28235294],\n",
       "         [ 0.29803922,  0.30980392,  0.2745098 ],\n",
       "         [ 0.32156863,  0.33333333,  0.29803922]],\n",
       "\n",
       "        [[ 0.32941176,  0.39215686,  0.28627451],\n",
       "         [ 0.3254902 ,  0.37254902,  0.29411765],\n",
       "         [ 0.30196078,  0.3372549 ,  0.28235294],\n",
       "         ..., \n",
       "         [ 0.29019608,  0.30196078,  0.26666667],\n",
       "         [ 0.28627451,  0.29803922,  0.2627451 ],\n",
       "         [ 0.3254902 ,  0.3372549 ,  0.30196078]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.25098039,  0.30196078,  0.30980392],\n",
       "         [ 0.47843137,  0.52156863,  0.56470588],\n",
       "         [ 0.5254902 ,  0.56862745,  0.61176471],\n",
       "         ..., \n",
       "         [ 0.41176471,  0.48235294,  0.47058824],\n",
       "         [ 0.32941176,  0.40392157,  0.35686275],\n",
       "         [ 0.23529412,  0.34509804,  0.24705882]],\n",
       "\n",
       "        [[ 0.17254902,  0.2       ,  0.21960784],\n",
       "         [ 0.30588235,  0.32941176,  0.36862745],\n",
       "         [ 0.37647059,  0.39607843,  0.43137255],\n",
       "         ..., \n",
       "         [ 0.57647059,  0.64705882,  0.69803922],\n",
       "         [ 0.49411765,  0.56078431,  0.58431373],\n",
       "         [ 0.36862745,  0.45882353,  0.44313725]],\n",
       "\n",
       "        [[ 0.14117647,  0.1372549 ,  0.15294118],\n",
       "         [ 0.23137255,  0.22745098,  0.25882353],\n",
       "         [ 0.32156863,  0.31764706,  0.33333333],\n",
       "         ..., \n",
       "         [ 0.5254902 ,  0.6       ,  0.62745098],\n",
       "         [ 0.54117647,  0.59607843,  0.61960784],\n",
       "         [ 0.50980392,  0.58039216,  0.58823529]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0745098 ,  0.1254902 ,  0.05882353],\n",
       "         [ 0.08235294,  0.14901961,  0.08235294],\n",
       "         [ 0.10588235,  0.19215686,  0.1254902 ],\n",
       "         ..., \n",
       "         [ 0.29411765,  0.48627451,  0.51372549],\n",
       "         [ 0.29803922,  0.48627451,  0.50980392],\n",
       "         [ 0.27843137,  0.4627451 ,  0.48627451]],\n",
       "\n",
       "        [[ 0.09019608,  0.12156863,  0.05490196],\n",
       "         [ 0.08235294,  0.11764706,  0.04705882],\n",
       "         [ 0.09019608,  0.1372549 ,  0.05490196],\n",
       "         ..., \n",
       "         [ 0.28235294,  0.4627451 ,  0.49411765],\n",
       "         [ 0.29411765,  0.46666667,  0.48627451],\n",
       "         [ 0.26666667,  0.43529412,  0.44705882]],\n",
       "\n",
       "        [[ 0.09411765,  0.14509804,  0.06666667],\n",
       "         [ 0.08627451,  0.1372549 ,  0.0627451 ],\n",
       "         [ 0.09411765,  0.14117647,  0.07058824],\n",
       "         ..., \n",
       "         [ 0.25098039,  0.42745098,  0.43921569],\n",
       "         [ 0.2627451 ,  0.42745098,  0.43529412],\n",
       "         [ 0.25098039,  0.41176471,  0.41176471]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.24313725,  0.18039216,  0.09019608],\n",
       "         [ 0.23529412,  0.18039216,  0.10588235],\n",
       "         [ 0.21568627,  0.18823529,  0.10980392],\n",
       "         ..., \n",
       "         [ 0.05098039,  0.02352941,  0.01568627],\n",
       "         [ 0.04705882,  0.05490196,  0.03137255],\n",
       "         [ 0.09803922,  0.15686275,  0.11764706]],\n",
       "\n",
       "        [[ 0.24705882,  0.20784314,  0.11764706],\n",
       "         [ 0.19215686,  0.17647059,  0.08627451],\n",
       "         [ 0.17647059,  0.18039216,  0.09019608],\n",
       "         ..., \n",
       "         [ 0.11372549,  0.1372549 ,  0.12156863],\n",
       "         [ 0.11764706,  0.16470588,  0.14509804],\n",
       "         [ 0.10588235,  0.19607843,  0.16862745]],\n",
       "\n",
       "        [[ 0.27058824,  0.20392157,  0.11372549],\n",
       "         [ 0.19215686,  0.14901961,  0.07843137],\n",
       "         [ 0.21176471,  0.18039216,  0.10588235],\n",
       "         ..., \n",
       "         [ 0.25882353,  0.34509804,  0.33333333],\n",
       "         [ 0.15686275,  0.26666667,  0.25098039],\n",
       "         [ 0.11372549,  0.24313725,  0.22745098]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.1372549 ,  0.69803922,  0.92156863],\n",
       "         [ 0.15686275,  0.69019608,  0.9372549 ],\n",
       "         [ 0.16470588,  0.69019608,  0.94509804],\n",
       "         ..., \n",
       "         [ 0.38823529,  0.69411765,  0.85882353],\n",
       "         [ 0.30980392,  0.57647059,  0.77254902],\n",
       "         [ 0.34901961,  0.58039216,  0.74117647]],\n",
       "\n",
       "        [[ 0.22352941,  0.71372549,  0.91764706],\n",
       "         [ 0.17254902,  0.72156863,  0.98039216],\n",
       "         [ 0.19607843,  0.71764706,  0.94117647],\n",
       "         ..., \n",
       "         [ 0.61176471,  0.71372549,  0.78431373],\n",
       "         [ 0.55294118,  0.69411765,  0.80784314],\n",
       "         [ 0.45490196,  0.58431373,  0.68627451]],\n",
       "\n",
       "        [[ 0.38431373,  0.77254902,  0.92941176],\n",
       "         [ 0.25098039,  0.74117647,  0.98823529],\n",
       "         [ 0.27058824,  0.75294118,  0.96078431],\n",
       "         ..., \n",
       "         [ 0.7372549 ,  0.76470588,  0.80784314],\n",
       "         [ 0.46666667,  0.52941176,  0.57647059],\n",
       "         [ 0.23921569,  0.30980392,  0.35294118]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.28627451,  0.30980392,  0.30196078],\n",
       "         [ 0.20784314,  0.24705882,  0.26666667],\n",
       "         [ 0.21176471,  0.26666667,  0.31372549],\n",
       "         ..., \n",
       "         [ 0.06666667,  0.15686275,  0.25098039],\n",
       "         [ 0.08235294,  0.14117647,  0.2       ],\n",
       "         [ 0.12941176,  0.18823529,  0.19215686]],\n",
       "\n",
       "        [[ 0.23921569,  0.26666667,  0.29411765],\n",
       "         [ 0.21568627,  0.2745098 ,  0.3372549 ],\n",
       "         [ 0.22352941,  0.30980392,  0.40392157],\n",
       "         ..., \n",
       "         [ 0.09411765,  0.18823529,  0.28235294],\n",
       "         [ 0.06666667,  0.1372549 ,  0.20784314],\n",
       "         [ 0.02745098,  0.09019608,  0.1254902 ]],\n",
       "\n",
       "        [[ 0.17254902,  0.21960784,  0.28627451],\n",
       "         [ 0.18039216,  0.25882353,  0.34509804],\n",
       "         [ 0.19215686,  0.30196078,  0.41176471],\n",
       "         ..., \n",
       "         [ 0.10588235,  0.20392157,  0.30196078],\n",
       "         [ 0.08235294,  0.16862745,  0.25882353],\n",
       "         [ 0.04705882,  0.12156863,  0.19607843]]],\n",
       "\n",
       "\n",
       "       [[[ 0.74117647,  0.82745098,  0.94117647],\n",
       "         [ 0.72941176,  0.81568627,  0.9254902 ],\n",
       "         [ 0.7254902 ,  0.81176471,  0.92156863],\n",
       "         ..., \n",
       "         [ 0.68627451,  0.76470588,  0.87843137],\n",
       "         [ 0.6745098 ,  0.76078431,  0.87058824],\n",
       "         [ 0.6627451 ,  0.76078431,  0.8627451 ]],\n",
       "\n",
       "        [[ 0.76078431,  0.82352941,  0.9372549 ],\n",
       "         [ 0.74901961,  0.81176471,  0.9254902 ],\n",
       "         [ 0.74509804,  0.80784314,  0.92156863],\n",
       "         ..., \n",
       "         [ 0.67843137,  0.75294118,  0.8627451 ],\n",
       "         [ 0.67058824,  0.74901961,  0.85490196],\n",
       "         [ 0.65490196,  0.74509804,  0.84705882]],\n",
       "\n",
       "        [[ 0.81568627,  0.85882353,  0.95686275],\n",
       "         [ 0.80392157,  0.84705882,  0.94117647],\n",
       "         [ 0.8       ,  0.84313725,  0.9372549 ],\n",
       "         ..., \n",
       "         [ 0.68627451,  0.74901961,  0.85098039],\n",
       "         [ 0.6745098 ,  0.74509804,  0.84705882],\n",
       "         [ 0.6627451 ,  0.74901961,  0.84313725]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.81176471,  0.78039216,  0.70980392],\n",
       "         [ 0.79607843,  0.76470588,  0.68627451],\n",
       "         [ 0.79607843,  0.76862745,  0.67843137],\n",
       "         ..., \n",
       "         [ 0.52941176,  0.51764706,  0.49803922],\n",
       "         [ 0.63529412,  0.61960784,  0.58823529],\n",
       "         [ 0.65882353,  0.63921569,  0.59215686]],\n",
       "\n",
       "        [[ 0.77647059,  0.74509804,  0.66666667],\n",
       "         [ 0.74117647,  0.70980392,  0.62352941],\n",
       "         [ 0.70588235,  0.6745098 ,  0.57647059],\n",
       "         ..., \n",
       "         [ 0.69803922,  0.67058824,  0.62745098],\n",
       "         [ 0.68627451,  0.6627451 ,  0.61176471],\n",
       "         [ 0.68627451,  0.6627451 ,  0.60392157]],\n",
       "\n",
       "        [[ 0.77647059,  0.74117647,  0.67843137],\n",
       "         [ 0.74117647,  0.70980392,  0.63529412],\n",
       "         [ 0.69803922,  0.66666667,  0.58431373],\n",
       "         ..., \n",
       "         [ 0.76470588,  0.72156863,  0.6627451 ],\n",
       "         [ 0.76862745,  0.74117647,  0.67058824],\n",
       "         [ 0.76470588,  0.74509804,  0.67058824]]],\n",
       "\n",
       "\n",
       "       [[[ 0.89803922,  0.89803922,  0.9372549 ],\n",
       "         [ 0.9254902 ,  0.92941176,  0.96862745],\n",
       "         [ 0.91764706,  0.9254902 ,  0.96862745],\n",
       "         ..., \n",
       "         [ 0.85098039,  0.85882353,  0.91372549],\n",
       "         [ 0.86666667,  0.8745098 ,  0.91764706],\n",
       "         [ 0.87058824,  0.8745098 ,  0.91372549]],\n",
       "\n",
       "        [[ 0.87058824,  0.86666667,  0.89803922],\n",
       "         [ 0.9372549 ,  0.9372549 ,  0.97647059],\n",
       "         [ 0.91372549,  0.91764706,  0.96470588],\n",
       "         ..., \n",
       "         [ 0.8745098 ,  0.8745098 ,  0.9254902 ],\n",
       "         [ 0.89019608,  0.89411765,  0.93333333],\n",
       "         [ 0.82352941,  0.82745098,  0.8627451 ]],\n",
       "\n",
       "        [[ 0.83529412,  0.80784314,  0.82745098],\n",
       "         [ 0.91764706,  0.90980392,  0.9372549 ],\n",
       "         [ 0.90588235,  0.91372549,  0.95686275],\n",
       "         ..., \n",
       "         [ 0.8627451 ,  0.8627451 ,  0.90980392],\n",
       "         [ 0.8627451 ,  0.85882353,  0.90980392],\n",
       "         [ 0.79215686,  0.79607843,  0.84313725]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.58823529,  0.56078431,  0.52941176],\n",
       "         [ 0.54901961,  0.52941176,  0.49803922],\n",
       "         [ 0.51764706,  0.49803922,  0.47058824],\n",
       "         ..., \n",
       "         [ 0.87843137,  0.87058824,  0.85490196],\n",
       "         [ 0.90196078,  0.89411765,  0.88235294],\n",
       "         [ 0.94509804,  0.94509804,  0.93333333]],\n",
       "\n",
       "        [[ 0.5372549 ,  0.51764706,  0.49411765],\n",
       "         [ 0.50980392,  0.49803922,  0.47058824],\n",
       "         [ 0.49019608,  0.4745098 ,  0.45098039],\n",
       "         ..., \n",
       "         [ 0.70980392,  0.70588235,  0.69803922],\n",
       "         [ 0.79215686,  0.78823529,  0.77647059],\n",
       "         [ 0.83137255,  0.82745098,  0.81176471]],\n",
       "\n",
       "        [[ 0.47843137,  0.46666667,  0.44705882],\n",
       "         [ 0.4627451 ,  0.45490196,  0.43137255],\n",
       "         [ 0.47058824,  0.45490196,  0.43529412],\n",
       "         ..., \n",
       "         [ 0.70196078,  0.69411765,  0.67843137],\n",
       "         [ 0.64313725,  0.64313725,  0.63529412],\n",
       "         [ 0.63921569,  0.63921569,  0.63137255]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "\n",
    "对于该神经网络，你需要将每层都构建为一个函数。你看到的大部分代码都位于函数外面。要更全面地测试你的代码，我们需要你将每层放入一个函数中。这样使我们能够提供更好的反馈，并使用我们的统一测试检测简单的错误，然后再提交项目。\n",
    "\n",
    ">**注意**：如果你觉得每周很难抽出足够的时间学习这门课程，我们为此项目提供了一个小捷径。对于接下来的几个问题，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 程序包中的类来构建每个层级，但是“卷积和最大池化层级”部分的层级除外。TF Layers 和 Keras 及 TFLearn 层级类似，因此很容易学会。\n",
    "\n",
    ">但是，如果你想充分利用这门课程，请尝试自己解决所有问题，不使用 TF Layers 程序包中的任何类。你依然可以使用其他程序包中的类，这些类和你在 TF Layers 中的类名称是一样的！例如，你可以使用 TF Neural Network 版本的 `conv2d` 类 [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)，而不是 TF Layers 版本的 `conv2d` 类 [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d)。\n",
    "\n",
    "我们开始吧！\n",
    "\n",
    "\n",
    "### 输入\n",
    "\n",
    "神经网络需要读取图片数据、one-hot 编码标签和丢弃保留概率（dropout keep probability）。请实现以下函数：\n",
    "\n",
    "* 实现 `neural_net_image_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `image_shape` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"x\" 命名\n",
    "* 实现 `neural_net_label_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `n_classes` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"y\" 命名\n",
    "* 实现 `neural_net_keep_prob_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)，用于丢弃保留概率\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"keep_prob\" 命名\n",
    "\n",
    "这些名称将在项目结束时，用于加载保存的模型。\n",
    "\n",
    "注意：TensorFlow 中的 `None` 表示形状可以是动态大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x = tf.placeholder(tf.float32, [None, image_shape[0],image_shape[1], image_shape[2]],name = 'x')\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes], name = 'y')\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积和最大池化层\n",
    "\n",
    "卷积层级适合处理图片。对于此代码单元，你应该实现函数 `conv2d_maxpool` 以便应用卷积然后进行最大池化：\n",
    "\n",
    "* 使用 `conv_ksize`、`conv_num_outputs` 和 `x_tensor` 的形状创建权重（weight）和偏置（bias）。\n",
    "* 使用权重和 `conv_strides` 对 `x_tensor` 应用卷积。\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "* 添加偏置\n",
    "* 向卷积中添加非线性激活（nonlinear activation）\n",
    "* 使用 `pool_ksize` 和 `pool_strides` 应用最大池化\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "\n",
    "**注意**：对于**此层**，**请勿使用** [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers)，但是仍然可以使用 TensorFlow 的 [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) 包。对于所有**其他层**，你依然可以使用快捷方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    a = x_tensor.get_shape()[3]\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1],int(a), conv_num_outputs], stddev=0.1))\n",
    "    \n",
    "    biases = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    conv = tf.nn.conv2d(x_tensor, weight, strides=[1,conv_strides[0],conv_strides[1],1], padding = 'SAME')\n",
    "    conv = tf.nn.bias_add(conv, biases)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    pool = tf.nn.max_pool(conv,ksize=[1,pool_ksize[0],pool_ksize[1],1] ,strides=[1,pool_strides[0],pool_strides[1],1], padding = 'SAME')\n",
    "    # TODO: Implement Function\n",
    "    return pool\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扁平化层\n",
    "\n",
    "实现 `flatten` 函数，将 `x_tensor` 的维度从四维张量（4-D tensor）变成二维张量。输出应该是形状（*部分大小（Batch Size）*，*扁平化图片大小（Flattened Image Size）*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    shape = x_tensor.get_shape().as_list() \n",
    "    dim = np.prod(shape[1:])            \n",
    "    x_tensor = tf.reshape(x_tensor, [-1, dim])\n",
    "    \n",
    "    #x_tensor = tf.contrib.layers.flatten(x_tensor)     # 或者这么写\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层\n",
    "\n",
    "实现 `fully_conn` 函数，以向 `x_tensor` 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    fc = tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "    # TODO: Implement Function\n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出层\n",
    "\n",
    "实现 `output` 函数，向 x_tensor 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n",
    "\n",
    "**注意**：该层级不应应用 Activation、softmax 或交叉熵（cross entropy）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    fc = tf.contrib.layers.fully_connected(x_tensor, num_outputs,activation_fn=None)\n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建卷积模型\n",
    "\n",
    "实现函数 `conv_net`， 创建卷积神经网络模型。该函数传入一批图片 `x`，并输出对数（logits）。使用你在上方创建的层创建此模型：\n",
    "\n",
    "* 应用 1、2 或 3 个卷积和最大池化层（Convolution and Max Pool layers）\n",
    "* 应用一个扁平层（Flatten Layer）\n",
    "* 应用 1、2 或 3 个完全连接层（Fully Connected Layers）\n",
    "* 应用一个输出层（Output Layer）\n",
    "* 返回输出\n",
    "* 使用 `keep_prob` 向模型中的一个或多个层应用 [TensorFlow 的 Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 32, (3,3), (1,1),(2,2),(2,2))\n",
    "    \n",
    "    conv2 = conv2d_maxpool(conv1, 64,(3,3), (1,1),(2,2),(2,2))\n",
    "    \n",
    "    conv2 = flatten(conv2)\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc1 = fully_conn(conv2, 512)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    fc2 = fully_conn(fc1, 256)\n",
    "    fc3 = fully_conn(fc2, 128)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    logits = output(fc3, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络\n",
    "\n",
    "### 单次优化\n",
    "\n",
    "实现函数 `train_neural_network` 以进行单次优化（single optimization）。该优化应该使用 `optimizer` 优化 `session`，其中 `feed_dict` 具有以下参数：\n",
    "\n",
    "* `x` 表示图片输入\n",
    "* `y` 表示标签\n",
    "* `keep_prob` 表示丢弃的保留率\n",
    "\n",
    "每个部分都会调用该函数，所以 `tf.global_variables_initializer()` 已经被调用。\n",
    "\n",
    "注意：不需要返回任何内容。该函数只是用来优化神经网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    # TODO: Implement Function\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示数据\n",
    "\n",
    "实现函数 `print_stats` 以输出损失和验证准确率。使用全局变量 `valid_features` 和 `valid_labels` 计算验证准确率。使用保留率 `1.0` 计算损失和验证准确率（loss and validation accuracy）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = session.run(cost, feed_dict={x: feature_batch,\n",
    "                                        y: label_batch,\n",
    "                                        keep_prob: 1.0})\n",
    "    \n",
    "    accuracy = session.run(accuracy, feed_dict={ x: valid_features,\n",
    "                                                 y: valid_labels,\n",
    "                                                 keep_prob: 1.0})\n",
    "\n",
    "\n",
    "    print( \"Minibatch Loss= \" +\"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(accuracy ))\n",
    "\n",
    "    # TODO: Implement Function\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数\n",
    "\n",
    "调试以下超参数：\n",
    "* 设置 `epochs` 表示神经网络停止学习或开始过拟合的迭代次数\n",
    "* 设置 `batch_size`，表示机器内存允许的部分最大体积。大部分人设为以下常见内存大小：\n",
    "\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* 设置 `keep_probability` 表示使用丢弃时保留节点的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs =300\n",
    "batch_size = 128\n",
    "keep_probability =0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在单个 CIFAR-10 部分上训练\n",
    "\n",
    "我们先用单个部分，而不是用所有的 CIFAR-10 批次训练神经网络。这样可以节省时间，并对模型进行迭代，以提高准确率。最终验证准确率达到 50% 或以上之后，在下一部分对所有数据运行模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Minibatch Loss= 2.1680, Training Accuracy= 0.290\n",
      "Epoch  2, CIFAR-10 Batch 1:  Minibatch Loss= 2.1013, Training Accuracy= 0.360\n",
      "Epoch  3, CIFAR-10 Batch 1:  Minibatch Loss= 2.0453, Training Accuracy= 0.391\n",
      "Epoch  4, CIFAR-10 Batch 1:  Minibatch Loss= 1.9363, Training Accuracy= 0.423\n",
      "Epoch  5, CIFAR-10 Batch 1:  Minibatch Loss= 1.8470, Training Accuracy= 0.436\n",
      "Epoch  6, CIFAR-10 Batch 1:  Minibatch Loss= 1.7607, Training Accuracy= 0.451\n",
      "Epoch  7, CIFAR-10 Batch 1:  Minibatch Loss= 1.6903, Training Accuracy= 0.472\n",
      "Epoch  8, CIFAR-10 Batch 1:  Minibatch Loss= 1.6087, Training Accuracy= 0.484\n",
      "Epoch  9, CIFAR-10 Batch 1:  Minibatch Loss= 1.5530, Training Accuracy= 0.489\n",
      "Epoch 10, CIFAR-10 Batch 1:  Minibatch Loss= 1.4907, Training Accuracy= 0.501\n",
      "Epoch 11, CIFAR-10 Batch 1:  Minibatch Loss= 1.4214, Training Accuracy= 0.507\n",
      "Epoch 12, CIFAR-10 Batch 1:  Minibatch Loss= 1.3279, Training Accuracy= 0.517\n",
      "Epoch 13, CIFAR-10 Batch 1:  Minibatch Loss= 1.2774, Training Accuracy= 0.519\n",
      "Epoch 14, CIFAR-10 Batch 1:  Minibatch Loss= 1.2425, Training Accuracy= 0.530\n",
      "Epoch 15, CIFAR-10 Batch 1:  Minibatch Loss= 1.2059, Training Accuracy= 0.530\n",
      "Epoch 16, CIFAR-10 Batch 1:  Minibatch Loss= 1.1362, Training Accuracy= 0.535\n",
      "Epoch 17, CIFAR-10 Batch 1:  Minibatch Loss= 1.0561, Training Accuracy= 0.545\n",
      "Epoch 18, CIFAR-10 Batch 1:  Minibatch Loss= 1.0071, Training Accuracy= 0.544\n",
      "Epoch 19, CIFAR-10 Batch 1:  Minibatch Loss= 0.9671, Training Accuracy= 0.551\n",
      "Epoch 20, CIFAR-10 Batch 1:  Minibatch Loss= 0.9425, Training Accuracy= 0.555\n",
      "Epoch 21, CIFAR-10 Batch 1:  Minibatch Loss= 0.8923, Training Accuracy= 0.555\n",
      "Epoch 22, CIFAR-10 Batch 1:  Minibatch Loss= 0.8512, Training Accuracy= 0.559\n",
      "Epoch 23, CIFAR-10 Batch 1:  Minibatch Loss= 0.7950, Training Accuracy= 0.562\n",
      "Epoch 24, CIFAR-10 Batch 1:  Minibatch Loss= 0.7556, Training Accuracy= 0.569\n",
      "Epoch 25, CIFAR-10 Batch 1:  Minibatch Loss= 0.7095, Training Accuracy= 0.571\n",
      "Epoch 26, CIFAR-10 Batch 1:  Minibatch Loss= 0.6698, Training Accuracy= 0.577\n",
      "Epoch 27, CIFAR-10 Batch 1:  Minibatch Loss= 0.6106, Training Accuracy= 0.576\n",
      "Epoch 28, CIFAR-10 Batch 1:  Minibatch Loss= 0.5967, Training Accuracy= 0.577\n",
      "Epoch 29, CIFAR-10 Batch 1:  Minibatch Loss= 0.5586, Training Accuracy= 0.580\n",
      "Epoch 30, CIFAR-10 Batch 1:  Minibatch Loss= 0.5348, Training Accuracy= 0.585\n",
      "Epoch 31, CIFAR-10 Batch 1:  Minibatch Loss= 0.4843, Training Accuracy= 0.584\n",
      "Epoch 32, CIFAR-10 Batch 1:  Minibatch Loss= 0.4672, Training Accuracy= 0.586\n",
      "Epoch 33, CIFAR-10 Batch 1:  Minibatch Loss= 0.4335, Training Accuracy= 0.587\n",
      "Epoch 34, CIFAR-10 Batch 1:  Minibatch Loss= 0.4097, Training Accuracy= 0.589\n",
      "Epoch 35, CIFAR-10 Batch 1:  Minibatch Loss= 0.3696, Training Accuracy= 0.590\n",
      "Epoch 36, CIFAR-10 Batch 1:  Minibatch Loss= 0.3586, Training Accuracy= 0.599\n",
      "Epoch 37, CIFAR-10 Batch 1:  Minibatch Loss= 0.3284, Training Accuracy= 0.598\n",
      "Epoch 38, CIFAR-10 Batch 1:  Minibatch Loss= 0.2869, Training Accuracy= 0.599\n",
      "Epoch 39, CIFAR-10 Batch 1:  Minibatch Loss= 0.2846, Training Accuracy= 0.606\n",
      "Epoch 40, CIFAR-10 Batch 1:  Minibatch Loss= 0.2608, Training Accuracy= 0.605\n",
      "Epoch 41, CIFAR-10 Batch 1:  Minibatch Loss= 0.2512, Training Accuracy= 0.608\n",
      "Epoch 42, CIFAR-10 Batch 1:  Minibatch Loss= 0.2222, Training Accuracy= 0.607\n",
      "Epoch 43, CIFAR-10 Batch 1:  Minibatch Loss= 0.1901, Training Accuracy= 0.605\n",
      "Epoch 44, CIFAR-10 Batch 1:  Minibatch Loss= 0.1851, Training Accuracy= 0.610\n",
      "Epoch 45, CIFAR-10 Batch 1:  Minibatch Loss= 0.1716, Training Accuracy= 0.614\n",
      "Epoch 46, CIFAR-10 Batch 1:  Minibatch Loss= 0.1522, Training Accuracy= 0.607\n",
      "Epoch 47, CIFAR-10 Batch 1:  Minibatch Loss= 0.1496, Training Accuracy= 0.610\n",
      "Epoch 48, CIFAR-10 Batch 1:  Minibatch Loss= 0.1321, Training Accuracy= 0.614\n",
      "Epoch 49, CIFAR-10 Batch 1:  Minibatch Loss= 0.1274, Training Accuracy= 0.614\n",
      "Epoch 50, CIFAR-10 Batch 1:  Minibatch Loss= 0.1361, Training Accuracy= 0.610\n",
      "Epoch 51, CIFAR-10 Batch 1:  Minibatch Loss= 0.1199, Training Accuracy= 0.604\n",
      "Epoch 52, CIFAR-10 Batch 1:  Minibatch Loss= 0.1001, Training Accuracy= 0.612\n",
      "Epoch 53, CIFAR-10 Batch 1:  Minibatch Loss= 0.0893, Training Accuracy= 0.608\n",
      "Epoch 54, CIFAR-10 Batch 1:  Minibatch Loss= 0.0748, Training Accuracy= 0.605\n",
      "Epoch 55, CIFAR-10 Batch 1:  Minibatch Loss= 0.0676, Training Accuracy= 0.602\n",
      "Epoch 56, CIFAR-10 Batch 1:  Minibatch Loss= 0.0619, Training Accuracy= 0.609\n",
      "Epoch 57, CIFAR-10 Batch 1:  Minibatch Loss= 0.0551, Training Accuracy= 0.611\n",
      "Max_Acc = 61.440  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    accuracy_max = 0.0\n",
    "    max_ep = 15\n",
    "    i = 0\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        current_acc = print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "        if(accuracy_max <= current_acc):\n",
    "            accuracy_max = current_acc\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1\n",
    "            if(i==max_ep):\n",
    "                break     \n",
    "    print('Max_Acc ='+' {:.3f}  '.format(accuracy_max*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全训练模型\n",
    "\n",
    "现在，单个 CIFAR-10 部分的准确率已经不错了，试试所有五个部分吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Minibatch Loss= 2.2224, Training Accuracy= 0.222\n",
      "Epoch  1, CIFAR-10 Batch 2:  Minibatch Loss= 1.9716, Training Accuracy= 0.323\n",
      "Epoch  1, CIFAR-10 Batch 3:  Minibatch Loss= 1.6804, Training Accuracy= 0.351\n",
      "Epoch  1, CIFAR-10 Batch 4:  Minibatch Loss= 1.6512, Training Accuracy= 0.394\n",
      "Epoch  1, CIFAR-10 Batch 5:  Minibatch Loss= 1.6249, Training Accuracy= 0.423\n",
      "Epoch  2, CIFAR-10 Batch 1:  Minibatch Loss= 1.9171, Training Accuracy= 0.439\n",
      "Epoch  2, CIFAR-10 Batch 2:  Minibatch Loss= 1.5884, Training Accuracy= 0.451\n",
      "Epoch  2, CIFAR-10 Batch 3:  Minibatch Loss= 1.2886, Training Accuracy= 0.456\n",
      "Epoch  2, CIFAR-10 Batch 4:  Minibatch Loss= 1.4850, Training Accuracy= 0.477\n",
      "Epoch  2, CIFAR-10 Batch 5:  Minibatch Loss= 1.3735, Training Accuracy= 0.489\n",
      "Epoch  3, CIFAR-10 Batch 1:  Minibatch Loss= 1.7123, Training Accuracy= 0.503\n",
      "Epoch  3, CIFAR-10 Batch 2:  Minibatch Loss= 1.3925, Training Accuracy= 0.503\n",
      "Epoch  3, CIFAR-10 Batch 3:  Minibatch Loss= 1.1235, Training Accuracy= 0.507\n",
      "Epoch  3, CIFAR-10 Batch 4:  Minibatch Loss= 1.3505, Training Accuracy= 0.520\n",
      "Epoch  3, CIFAR-10 Batch 5:  Minibatch Loss= 1.2465, Training Accuracy= 0.526\n",
      "Epoch  4, CIFAR-10 Batch 1:  Minibatch Loss= 1.5317, Training Accuracy= 0.534\n",
      "Epoch  4, CIFAR-10 Batch 2:  Minibatch Loss= 1.2370, Training Accuracy= 0.540\n",
      "Epoch  4, CIFAR-10 Batch 3:  Minibatch Loss= 0.9960, Training Accuracy= 0.531\n",
      "Epoch  4, CIFAR-10 Batch 4:  Minibatch Loss= 1.2237, Training Accuracy= 0.551\n",
      "Epoch  4, CIFAR-10 Batch 5:  Minibatch Loss= 1.1778, Training Accuracy= 0.555\n",
      "Epoch  5, CIFAR-10 Batch 1:  Minibatch Loss= 1.3897, Training Accuracy= 0.565\n",
      "Epoch  5, CIFAR-10 Batch 2:  Minibatch Loss= 1.1406, Training Accuracy= 0.568\n",
      "Epoch  5, CIFAR-10 Batch 3:  Minibatch Loss= 0.8915, Training Accuracy= 0.562\n",
      "Epoch  5, CIFAR-10 Batch 4:  Minibatch Loss= 1.1138, Training Accuracy= 0.575\n",
      "Epoch  5, CIFAR-10 Batch 5:  Minibatch Loss= 1.0501, Training Accuracy= 0.578\n",
      "Epoch  6, CIFAR-10 Batch 1:  Minibatch Loss= 1.3124, Training Accuracy= 0.579\n",
      "Epoch  6, CIFAR-10 Batch 2:  Minibatch Loss= 1.0378, Training Accuracy= 0.583\n",
      "Epoch  6, CIFAR-10 Batch 3:  Minibatch Loss= 0.8231, Training Accuracy= 0.580\n",
      "Epoch  6, CIFAR-10 Batch 4:  Minibatch Loss= 1.0203, Training Accuracy= 0.587\n",
      "Epoch  6, CIFAR-10 Batch 5:  Minibatch Loss= 0.9771, Training Accuracy= 0.595\n",
      "Epoch  7, CIFAR-10 Batch 1:  Minibatch Loss= 1.2285, Training Accuracy= 0.595\n",
      "Epoch  7, CIFAR-10 Batch 2:  Minibatch Loss= 0.9741, Training Accuracy= 0.598\n",
      "Epoch  7, CIFAR-10 Batch 3:  Minibatch Loss= 0.7594, Training Accuracy= 0.593\n",
      "Epoch  7, CIFAR-10 Batch 4:  Minibatch Loss= 0.9327, Training Accuracy= 0.610\n",
      "Epoch  7, CIFAR-10 Batch 5:  Minibatch Loss= 0.8926, Training Accuracy= 0.609\n",
      "Epoch  8, CIFAR-10 Batch 1:  Minibatch Loss= 1.1293, Training Accuracy= 0.615\n",
      "Epoch  8, CIFAR-10 Batch 2:  Minibatch Loss= 0.9030, Training Accuracy= 0.615\n",
      "Epoch  8, CIFAR-10 Batch 3:  Minibatch Loss= 0.7031, Training Accuracy= 0.615\n",
      "Epoch  8, CIFAR-10 Batch 4:  Minibatch Loss= 0.8531, Training Accuracy= 0.619\n",
      "Epoch  8, CIFAR-10 Batch 5:  Minibatch Loss= 0.7919, Training Accuracy= 0.617\n",
      "Epoch  9, CIFAR-10 Batch 1:  Minibatch Loss= 1.0701, Training Accuracy= 0.622\n",
      "Epoch  9, CIFAR-10 Batch 2:  Minibatch Loss= 0.8446, Training Accuracy= 0.622\n",
      "Epoch  9, CIFAR-10 Batch 3:  Minibatch Loss= 0.6185, Training Accuracy= 0.620\n",
      "Epoch  9, CIFAR-10 Batch 4:  Minibatch Loss= 0.7894, Training Accuracy= 0.626\n",
      "Epoch  9, CIFAR-10 Batch 5:  Minibatch Loss= 0.7579, Training Accuracy= 0.628\n",
      "Epoch 10, CIFAR-10 Batch 1:  Minibatch Loss= 1.0167, Training Accuracy= 0.638\n",
      "Epoch 10, CIFAR-10 Batch 2:  Minibatch Loss= 0.7524, Training Accuracy= 0.636\n",
      "Epoch 10, CIFAR-10 Batch 3:  Minibatch Loss= 0.5973, Training Accuracy= 0.638\n",
      "Epoch 10, CIFAR-10 Batch 4:  Minibatch Loss= 0.7130, Training Accuracy= 0.641\n",
      "Epoch 10, CIFAR-10 Batch 5:  Minibatch Loss= 0.6827, Training Accuracy= 0.634\n",
      "Epoch 11, CIFAR-10 Batch 1:  Minibatch Loss= 0.9143, Training Accuracy= 0.646\n",
      "Epoch 11, CIFAR-10 Batch 2:  Minibatch Loss= 0.6795, Training Accuracy= 0.644\n",
      "Epoch 11, CIFAR-10 Batch 3:  Minibatch Loss= 0.5468, Training Accuracy= 0.642\n",
      "Epoch 11, CIFAR-10 Batch 4:  Minibatch Loss= 0.6588, Training Accuracy= 0.653\n",
      "Epoch 11, CIFAR-10 Batch 5:  Minibatch Loss= 0.6197, Training Accuracy= 0.647\n",
      "Epoch 12, CIFAR-10 Batch 1:  Minibatch Loss= 0.8561, Training Accuracy= 0.656\n",
      "Epoch 12, CIFAR-10 Batch 2:  Minibatch Loss= 0.6434, Training Accuracy= 0.650\n",
      "Epoch 12, CIFAR-10 Batch 3:  Minibatch Loss= 0.4975, Training Accuracy= 0.649\n",
      "Epoch 12, CIFAR-10 Batch 4:  Minibatch Loss= 0.6221, Training Accuracy= 0.653\n",
      "Epoch 12, CIFAR-10 Batch 5:  Minibatch Loss= 0.5554, Training Accuracy= 0.657\n",
      "Epoch 13, CIFAR-10 Batch 1:  Minibatch Loss= 0.8130, Training Accuracy= 0.661\n",
      "Epoch 13, CIFAR-10 Batch 2:  Minibatch Loss= 0.5986, Training Accuracy= 0.661\n",
      "Epoch 13, CIFAR-10 Batch 3:  Minibatch Loss= 0.4701, Training Accuracy= 0.662\n",
      "Epoch 13, CIFAR-10 Batch 4:  Minibatch Loss= 0.5407, Training Accuracy= 0.665\n",
      "Epoch 13, CIFAR-10 Batch 5:  Minibatch Loss= 0.5242, Training Accuracy= 0.664\n",
      "Epoch 14, CIFAR-10 Batch 1:  Minibatch Loss= 0.7522, Training Accuracy= 0.666\n",
      "Epoch 14, CIFAR-10 Batch 2:  Minibatch Loss= 0.5561, Training Accuracy= 0.669\n",
      "Epoch 14, CIFAR-10 Batch 3:  Minibatch Loss= 0.4212, Training Accuracy= 0.664\n",
      "Epoch 14, CIFAR-10 Batch 4:  Minibatch Loss= 0.4890, Training Accuracy= 0.669\n",
      "Epoch 14, CIFAR-10 Batch 5:  Minibatch Loss= 0.4911, Training Accuracy= 0.669\n",
      "Epoch 15, CIFAR-10 Batch 1:  Minibatch Loss= 0.6843, Training Accuracy= 0.675\n",
      "Epoch 15, CIFAR-10 Batch 2:  Minibatch Loss= 0.4998, Training Accuracy= 0.670\n",
      "Epoch 15, CIFAR-10 Batch 3:  Minibatch Loss= 0.3863, Training Accuracy= 0.672\n",
      "Epoch 15, CIFAR-10 Batch 4:  Minibatch Loss= 0.4367, Training Accuracy= 0.674\n",
      "Epoch 15, CIFAR-10 Batch 5:  Minibatch Loss= 0.4309, Training Accuracy= 0.680\n",
      "Epoch 16, CIFAR-10 Batch 1:  Minibatch Loss= 0.6221, Training Accuracy= 0.676\n",
      "Epoch 16, CIFAR-10 Batch 2:  Minibatch Loss= 0.4562, Training Accuracy= 0.678\n",
      "Epoch 16, CIFAR-10 Batch 3:  Minibatch Loss= 0.3500, Training Accuracy= 0.679\n",
      "Epoch 16, CIFAR-10 Batch 4:  Minibatch Loss= 0.4246, Training Accuracy= 0.675\n",
      "Epoch 16, CIFAR-10 Batch 5:  Minibatch Loss= 0.4071, Training Accuracy= 0.683\n",
      "Epoch 17, CIFAR-10 Batch 1:  Minibatch Loss= 0.6192, Training Accuracy= 0.685\n",
      "Epoch 17, CIFAR-10 Batch 2:  Minibatch Loss= 0.4343, Training Accuracy= 0.683\n",
      "Epoch 17, CIFAR-10 Batch 3:  Minibatch Loss= 0.3344, Training Accuracy= 0.678\n",
      "Epoch 17, CIFAR-10 Batch 4:  Minibatch Loss= 0.3830, Training Accuracy= 0.681\n",
      "Epoch 17, CIFAR-10 Batch 5:  Minibatch Loss= 0.3595, Training Accuracy= 0.686\n",
      "Epoch 18, CIFAR-10 Batch 1:  Minibatch Loss= 0.5756, Training Accuracy= 0.687\n",
      "Epoch 18, CIFAR-10 Batch 2:  Minibatch Loss= 0.3944, Training Accuracy= 0.689\n",
      "Epoch 18, CIFAR-10 Batch 3:  Minibatch Loss= 0.2958, Training Accuracy= 0.685\n",
      "Epoch 18, CIFAR-10 Batch 4:  Minibatch Loss= 0.3319, Training Accuracy= 0.687\n",
      "Epoch 18, CIFAR-10 Batch 5:  Minibatch Loss= 0.3316, Training Accuracy= 0.687\n",
      "Epoch 19, CIFAR-10 Batch 1:  Minibatch Loss= 0.5174, Training Accuracy= 0.693\n",
      "Epoch 19, CIFAR-10 Batch 2:  Minibatch Loss= 0.3531, Training Accuracy= 0.688\n",
      "Epoch 19, CIFAR-10 Batch 3:  Minibatch Loss= 0.2894, Training Accuracy= 0.692\n",
      "Epoch 19, CIFAR-10 Batch 4:  Minibatch Loss= 0.2938, Training Accuracy= 0.690\n",
      "Epoch 19, CIFAR-10 Batch 5:  Minibatch Loss= 0.2789, Training Accuracy= 0.695\n",
      "Epoch 20, CIFAR-10 Batch 1:  Minibatch Loss= 0.4661, Training Accuracy= 0.693\n",
      "Epoch 20, CIFAR-10 Batch 2:  Minibatch Loss= 0.3265, Training Accuracy= 0.696\n",
      "Epoch 20, CIFAR-10 Batch 3:  Minibatch Loss= 0.2611, Training Accuracy= 0.693\n",
      "Epoch 20, CIFAR-10 Batch 4:  Minibatch Loss= 0.2972, Training Accuracy= 0.692\n",
      "Epoch 20, CIFAR-10 Batch 5:  Minibatch Loss= 0.2585, Training Accuracy= 0.695\n",
      "Epoch 21, CIFAR-10 Batch 1:  Minibatch Loss= 0.4397, Training Accuracy= 0.695\n",
      "Epoch 21, CIFAR-10 Batch 2:  Minibatch Loss= 0.2866, Training Accuracy= 0.698\n",
      "Epoch 21, CIFAR-10 Batch 3:  Minibatch Loss= 0.2372, Training Accuracy= 0.701\n",
      "Epoch 21, CIFAR-10 Batch 4:  Minibatch Loss= 0.2754, Training Accuracy= 0.691\n",
      "Epoch 21, CIFAR-10 Batch 5:  Minibatch Loss= 0.2515, Training Accuracy= 0.697\n",
      "Epoch 22, CIFAR-10 Batch 1:  Minibatch Loss= 0.3898, Training Accuracy= 0.701\n",
      "Epoch 22, CIFAR-10 Batch 2:  Minibatch Loss= 0.2533, Training Accuracy= 0.698\n",
      "Epoch 22, CIFAR-10 Batch 3:  Minibatch Loss= 0.2268, Training Accuracy= 0.696\n",
      "Epoch 22, CIFAR-10 Batch 4:  Minibatch Loss= 0.2546, Training Accuracy= 0.693\n",
      "Epoch 22, CIFAR-10 Batch 5:  Minibatch Loss= 0.2122, Training Accuracy= 0.700\n",
      "Epoch 23, CIFAR-10 Batch 1:  Minibatch Loss= 0.3671, Training Accuracy= 0.700\n",
      "Epoch 23, CIFAR-10 Batch 2:  Minibatch Loss= 0.2279, Training Accuracy= 0.703\n",
      "Epoch 23, CIFAR-10 Batch 3:  Minibatch Loss= 0.2042, Training Accuracy= 0.708\n",
      "Epoch 23, CIFAR-10 Batch 4:  Minibatch Loss= 0.2315, Training Accuracy= 0.696\n",
      "Epoch 23, CIFAR-10 Batch 5:  Minibatch Loss= 0.1957, Training Accuracy= 0.703\n",
      "Epoch 24, CIFAR-10 Batch 1:  Minibatch Loss= 0.3389, Training Accuracy= 0.706\n",
      "Epoch 24, CIFAR-10 Batch 2:  Minibatch Loss= 0.2027, Training Accuracy= 0.706\n",
      "Epoch 24, CIFAR-10 Batch 3:  Minibatch Loss= 0.1932, Training Accuracy= 0.702\n",
      "Epoch 24, CIFAR-10 Batch 4:  Minibatch Loss= 0.2134, Training Accuracy= 0.696\n",
      "Epoch 24, CIFAR-10 Batch 5:  Minibatch Loss= 0.1777, Training Accuracy= 0.707\n",
      "Epoch 25, CIFAR-10 Batch 1:  Minibatch Loss= 0.2925, Training Accuracy= 0.708\n",
      "Epoch 25, CIFAR-10 Batch 2:  Minibatch Loss= 0.1856, Training Accuracy= 0.705\n",
      "Epoch 25, CIFAR-10 Batch 3:  Minibatch Loss= 0.1537, Training Accuracy= 0.707\n",
      "Epoch 25, CIFAR-10 Batch 4:  Minibatch Loss= 0.1763, Training Accuracy= 0.708\n",
      "Epoch 25, CIFAR-10 Batch 5:  Minibatch Loss= 0.1583, Training Accuracy= 0.707\n",
      "Epoch 26, CIFAR-10 Batch 1:  Minibatch Loss= 0.2703, Training Accuracy= 0.706\n",
      "Epoch 26, CIFAR-10 Batch 2:  Minibatch Loss= 0.1594, Training Accuracy= 0.708\n",
      "Epoch 26, CIFAR-10 Batch 3:  Minibatch Loss= 0.1296, Training Accuracy= 0.712\n",
      "Epoch 26, CIFAR-10 Batch 4:  Minibatch Loss= 0.1584, Training Accuracy= 0.700\n",
      "Epoch 26, CIFAR-10 Batch 5:  Minibatch Loss= 0.1354, Training Accuracy= 0.706\n",
      "Epoch 27, CIFAR-10 Batch 1:  Minibatch Loss= 0.2445, Training Accuracy= 0.712\n",
      "Epoch 27, CIFAR-10 Batch 2:  Minibatch Loss= 0.1451, Training Accuracy= 0.712\n",
      "Epoch 27, CIFAR-10 Batch 3:  Minibatch Loss= 0.1346, Training Accuracy= 0.711\n",
      "Epoch 27, CIFAR-10 Batch 4:  Minibatch Loss= 0.1394, Training Accuracy= 0.705\n",
      "Epoch 27, CIFAR-10 Batch 5:  Minibatch Loss= 0.1230, Training Accuracy= 0.711\n",
      "Epoch 28, CIFAR-10 Batch 1:  Minibatch Loss= 0.2137, Training Accuracy= 0.711\n",
      "Epoch 28, CIFAR-10 Batch 2:  Minibatch Loss= 0.1417, Training Accuracy= 0.712\n",
      "Epoch 28, CIFAR-10 Batch 3:  Minibatch Loss= 0.1315, Training Accuracy= 0.712\n",
      "Epoch 28, CIFAR-10 Batch 4:  Minibatch Loss= 0.1418, Training Accuracy= 0.708\n",
      "Epoch 28, CIFAR-10 Batch 5:  Minibatch Loss= 0.1040, Training Accuracy= 0.712\n",
      "Epoch 29, CIFAR-10 Batch 1:  Minibatch Loss= 0.1879, Training Accuracy= 0.718\n",
      "Epoch 29, CIFAR-10 Batch 2:  Minibatch Loss= 0.1379, Training Accuracy= 0.710\n",
      "Epoch 29, CIFAR-10 Batch 3:  Minibatch Loss= 0.1195, Training Accuracy= 0.709\n",
      "Epoch 29, CIFAR-10 Batch 4:  Minibatch Loss= 0.1207, Training Accuracy= 0.708\n",
      "Epoch 29, CIFAR-10 Batch 5:  Minibatch Loss= 0.0999, Training Accuracy= 0.706\n",
      "Epoch 30, CIFAR-10 Batch 1:  Minibatch Loss= 0.1614, Training Accuracy= 0.718\n",
      "Epoch 30, CIFAR-10 Batch 2:  Minibatch Loss= 0.1270, Training Accuracy= 0.711\n",
      "Epoch 30, CIFAR-10 Batch 3:  Minibatch Loss= 0.0977, Training Accuracy= 0.717\n",
      "Epoch 30, CIFAR-10 Batch 4:  Minibatch Loss= 0.1079, Training Accuracy= 0.710\n",
      "Epoch 30, CIFAR-10 Batch 5:  Minibatch Loss= 0.0780, Training Accuracy= 0.712\n",
      "Epoch 31, CIFAR-10 Batch 1:  Minibatch Loss= 0.1509, Training Accuracy= 0.713\n",
      "Epoch 31, CIFAR-10 Batch 2:  Minibatch Loss= 0.1043, Training Accuracy= 0.714\n",
      "Epoch 31, CIFAR-10 Batch 3:  Minibatch Loss= 0.0960, Training Accuracy= 0.708\n",
      "Epoch 31, CIFAR-10 Batch 4:  Minibatch Loss= 0.1015, Training Accuracy= 0.709\n",
      "Epoch 31, CIFAR-10 Batch 5:  Minibatch Loss= 0.0595, Training Accuracy= 0.712\n",
      "Epoch 32, CIFAR-10 Batch 1:  Minibatch Loss= 0.1182, Training Accuracy= 0.717\n",
      "Epoch 32, CIFAR-10 Batch 2:  Minibatch Loss= 0.0968, Training Accuracy= 0.719\n",
      "Epoch 32, CIFAR-10 Batch 3:  Minibatch Loss= 0.0846, Training Accuracy= 0.717\n",
      "Epoch 32, CIFAR-10 Batch 4:  Minibatch Loss= 0.0964, Training Accuracy= 0.709\n",
      "Epoch 32, CIFAR-10 Batch 5:  Minibatch Loss= 0.0570, Training Accuracy= 0.718\n",
      "Epoch 33, CIFAR-10 Batch 1:  Minibatch Loss= 0.1274, Training Accuracy= 0.717\n",
      "Epoch 33, CIFAR-10 Batch 2:  Minibatch Loss= 0.0892, Training Accuracy= 0.716\n",
      "Epoch 33, CIFAR-10 Batch 3:  Minibatch Loss= 0.0766, Training Accuracy= 0.718\n",
      "Epoch 33, CIFAR-10 Batch 4:  Minibatch Loss= 0.0878, Training Accuracy= 0.714\n",
      "Epoch 33, CIFAR-10 Batch 5:  Minibatch Loss= 0.0639, Training Accuracy= 0.714\n",
      "Epoch 34, CIFAR-10 Batch 1:  Minibatch Loss= 0.1067, Training Accuracy= 0.718\n",
      "Epoch 34, CIFAR-10 Batch 2:  Minibatch Loss= 0.0824, Training Accuracy= 0.711\n",
      "Epoch 34, CIFAR-10 Batch 3:  Minibatch Loss= 0.0690, Training Accuracy= 0.706\n",
      "Epoch 34, CIFAR-10 Batch 4:  Minibatch Loss= 0.0569, Training Accuracy= 0.709\n",
      "Epoch 34, CIFAR-10 Batch 5:  Minibatch Loss= 0.0568, Training Accuracy= 0.715\n",
      "Epoch 35, CIFAR-10 Batch 1:  Minibatch Loss= 0.0841, Training Accuracy= 0.714\n",
      "Epoch 35, CIFAR-10 Batch 2:  Minibatch Loss= 0.0738, Training Accuracy= 0.717\n",
      "Epoch 35, CIFAR-10 Batch 3:  Minibatch Loss= 0.0766, Training Accuracy= 0.708\n",
      "Epoch 35, CIFAR-10 Batch 4:  Minibatch Loss= 0.0586, Training Accuracy= 0.714\n",
      "Epoch 35, CIFAR-10 Batch 5:  Minibatch Loss= 0.0464, Training Accuracy= 0.715\n",
      "Epoch 36, CIFAR-10 Batch 1:  Minibatch Loss= 0.0935, Training Accuracy= 0.716\n",
      "Epoch 36, CIFAR-10 Batch 2:  Minibatch Loss= 0.0529, Training Accuracy= 0.710\n",
      "Epoch 37, CIFAR-10 Batch 1:  Minibatch Loss= 0.0821, Training Accuracy= 0.711\n",
      "Epoch 37, CIFAR-10 Batch 2:  Minibatch Loss= 0.0475, Training Accuracy= 0.714\n",
      "Epoch 37, CIFAR-10 Batch 3:  Minibatch Loss= 0.0665, Training Accuracy= 0.707\n",
      "Epoch 37, CIFAR-10 Batch 4:  Minibatch Loss= 0.0577, Training Accuracy= 0.713\n",
      "Epoch 37, CIFAR-10 Batch 5:  Minibatch Loss= 0.0485, Training Accuracy= 0.717\n",
      "Epoch 38, CIFAR-10 Batch 1:  Minibatch Loss= 0.0735, Training Accuracy= 0.716\n",
      "Epoch 38, CIFAR-10 Batch 2:  Minibatch Loss= 0.0445, Training Accuracy= 0.707\n",
      "Epoch 38, CIFAR-10 Batch 3:  Minibatch Loss= 0.0423, Training Accuracy= 0.709\n",
      "Epoch 38, CIFAR-10 Batch 4:  Minibatch Loss= 0.0525, Training Accuracy= 0.722\n",
      "Epoch 38, CIFAR-10 Batch 5:  Minibatch Loss= 0.0328, Training Accuracy= 0.712\n",
      "Epoch 39, CIFAR-10 Batch 1:  Minibatch Loss= 0.0668, Training Accuracy= 0.718\n",
      "Epoch 39, CIFAR-10 Batch 2:  Minibatch Loss= 0.0441, Training Accuracy= 0.709\n",
      "Epoch 39, CIFAR-10 Batch 3:  Minibatch Loss= 0.0594, Training Accuracy= 0.703\n",
      "Epoch 39, CIFAR-10 Batch 4:  Minibatch Loss= 0.0428, Training Accuracy= 0.716\n",
      "Epoch 39, CIFAR-10 Batch 5:  Minibatch Loss= 0.0340, Training Accuracy= 0.714\n",
      "Epoch 40, CIFAR-10 Batch 1:  Minibatch Loss= 0.0653, Training Accuracy= 0.719\n",
      "Epoch 40, CIFAR-10 Batch 2:  Minibatch Loss= 0.0399, Training Accuracy= 0.719\n",
      "Epoch 40, CIFAR-10 Batch 3:  Minibatch Loss= 0.0503, Training Accuracy= 0.700\n",
      "Epoch 40, CIFAR-10 Batch 4:  Minibatch Loss= 0.0317, Training Accuracy= 0.718\n",
      "Epoch 40, CIFAR-10 Batch 5:  Minibatch Loss= 0.0274, Training Accuracy= 0.714\n",
      "Epoch 41, CIFAR-10 Batch 1:  Minibatch Loss= 0.0558, Training Accuracy= 0.720\n",
      "Epoch 41, CIFAR-10 Batch 2:  Minibatch Loss= 0.0320, Training Accuracy= 0.705\n",
      "Epoch 41, CIFAR-10 Batch 3:  Minibatch Loss= 0.0310, Training Accuracy= 0.706\n",
      "Epoch 41, CIFAR-10 Batch 4:  Minibatch Loss= 0.0279, Training Accuracy= 0.721\n",
      "Epoch 41, CIFAR-10 Batch 5:  Minibatch Loss= 0.0294, Training Accuracy= 0.706\n",
      "Epoch 42, CIFAR-10 Batch 1:  Minibatch Loss= 0.0369, Training Accuracy= 0.723\n",
      "Epoch 42, CIFAR-10 Batch 2:  Minibatch Loss= 0.0368, Training Accuracy= 0.714\n",
      "Epoch 42, CIFAR-10 Batch 3:  Minibatch Loss= 0.0288, Training Accuracy= 0.705\n",
      "Epoch 42, CIFAR-10 Batch 4:  Minibatch Loss= 0.0293, Training Accuracy= 0.719\n",
      "Epoch 42, CIFAR-10 Batch 5:  Minibatch Loss= 0.0256, Training Accuracy= 0.716\n",
      "Epoch 43, CIFAR-10 Batch 1:  Minibatch Loss= 0.0388, Training Accuracy= 0.717\n",
      "Epoch 43, CIFAR-10 Batch 2:  Minibatch Loss= 0.0273, Training Accuracy= 0.714\n",
      "Epoch 43, CIFAR-10 Batch 3:  Minibatch Loss= 0.0219, Training Accuracy= 0.707\n",
      "Epoch 43, CIFAR-10 Batch 4:  Minibatch Loss= 0.0253, Training Accuracy= 0.720\n",
      "Epoch 43, CIFAR-10 Batch 5:  Minibatch Loss= 0.0221, Training Accuracy= 0.713\n",
      "Epoch 44, CIFAR-10 Batch 1:  Minibatch Loss= 0.0345, Training Accuracy= 0.716\n",
      "Epoch 44, CIFAR-10 Batch 2:  Minibatch Loss= 0.0278, Training Accuracy= 0.714\n",
      "Epoch 44, CIFAR-10 Batch 3:  Minibatch Loss= 0.0221, Training Accuracy= 0.715\n",
      "Epoch 44, CIFAR-10 Batch 4:  Minibatch Loss= 0.0245, Training Accuracy= 0.715\n",
      "Epoch 44, CIFAR-10 Batch 5:  Minibatch Loss= 0.0226, Training Accuracy= 0.714\n",
      "Epoch 45, CIFAR-10 Batch 1:  Minibatch Loss= 0.0277, Training Accuracy= 0.717\n",
      "Epoch 45, CIFAR-10 Batch 2:  Minibatch Loss= 0.0231, Training Accuracy= 0.714\n",
      "Epoch 45, CIFAR-10 Batch 3:  Minibatch Loss= 0.0209, Training Accuracy= 0.703\n",
      "Epoch 45, CIFAR-10 Batch 4:  Minibatch Loss= 0.0206, Training Accuracy= 0.717\n",
      "Epoch 45, CIFAR-10 Batch 5:  Minibatch Loss= 0.0198, Training Accuracy= 0.713\n",
      "Epoch 46, CIFAR-10 Batch 1:  Minibatch Loss= 0.0223, Training Accuracy= 0.722\n",
      "Epoch 47, CIFAR-10 Batch 1:  Minibatch Loss= 0.0157, Training Accuracy= 0.716\n",
      "Epoch 47, CIFAR-10 Batch 2:  Minibatch Loss= 0.0228, Training Accuracy= 0.717\n",
      "Epoch 47, CIFAR-10 Batch 3:  Minibatch Loss= 0.0179, Training Accuracy= 0.718\n",
      "Epoch 47, CIFAR-10 Batch 4:  Minibatch Loss= 0.0191, Training Accuracy= 0.700\n",
      "Epoch 47, CIFAR-10 Batch 5:  Minibatch Loss= 0.0149, Training Accuracy= 0.714\n",
      "Epoch 48, CIFAR-10 Batch 1:  Minibatch Loss= 0.0222, Training Accuracy= 0.715\n",
      "Epoch 48, CIFAR-10 Batch 2:  Minibatch Loss= 0.0181, Training Accuracy= 0.720\n",
      "Epoch 48, CIFAR-10 Batch 3:  Minibatch Loss= 0.0160, Training Accuracy= 0.709\n",
      "Epoch 48, CIFAR-10 Batch 4:  Minibatch Loss= 0.0212, Training Accuracy= 0.719\n",
      "Epoch 48, CIFAR-10 Batch 5:  Minibatch Loss= 0.0137, Training Accuracy= 0.713\n",
      "Epoch 49, CIFAR-10 Batch 1:  Minibatch Loss= 0.0245, Training Accuracy= 0.715\n",
      "Epoch 49, CIFAR-10 Batch 2:  Minibatch Loss= 0.0161, Training Accuracy= 0.718\n",
      "Epoch 49, CIFAR-10 Batch 3:  Minibatch Loss= 0.0139, Training Accuracy= 0.717\n",
      "Epoch 49, CIFAR-10 Batch 4:  Minibatch Loss= 0.0183, Training Accuracy= 0.716\n",
      "Epoch 49, CIFAR-10 Batch 5:  Minibatch Loss= 0.0135, Training Accuracy= 0.713\n",
      "Epoch 50, CIFAR-10 Batch 1:  Minibatch Loss= 0.0123, Training Accuracy= 0.719\n",
      "Epoch 50, CIFAR-10 Batch 2:  Minibatch Loss= 0.0135, Training Accuracy= 0.718\n",
      "Epoch 50, CIFAR-10 Batch 3:  Minibatch Loss= 0.0107, Training Accuracy= 0.723\n",
      "Epoch 50, CIFAR-10 Batch 4:  Minibatch Loss= 0.0136, Training Accuracy= 0.713\n",
      "Epoch 50, CIFAR-10 Batch 5:  Minibatch Loss= 0.0095, Training Accuracy= 0.720\n",
      "Epoch 51, CIFAR-10 Batch 1:  Minibatch Loss= 0.0142, Training Accuracy= 0.717\n",
      "Epoch 51, CIFAR-10 Batch 2:  Minibatch Loss= 0.0128, Training Accuracy= 0.716\n",
      "Epoch 51, CIFAR-10 Batch 3:  Minibatch Loss= 0.0122, Training Accuracy= 0.717\n",
      "Epoch 51, CIFAR-10 Batch 4:  Minibatch Loss= 0.0112, Training Accuracy= 0.722\n",
      "Epoch 51, CIFAR-10 Batch 5:  Minibatch Loss= 0.0109, Training Accuracy= 0.715\n",
      "Epoch 52, CIFAR-10 Batch 1:  Minibatch Loss= 0.0099, Training Accuracy= 0.721\n",
      "Epoch 52, CIFAR-10 Batch 2:  Minibatch Loss= 0.0166, Training Accuracy= 0.710\n",
      "Epoch 52, CIFAR-10 Batch 3:  Minibatch Loss= 0.0112, Training Accuracy= 0.721\n",
      "Epoch 52, CIFAR-10 Batch 4:  Minibatch Loss= 0.0114, Training Accuracy= 0.719\n",
      "Epoch 52, CIFAR-10 Batch 5:  Minibatch Loss= 0.0081, Training Accuracy= 0.718\n",
      "Epoch 53, CIFAR-10 Batch 1:  Minibatch Loss= 0.0119, Training Accuracy= 0.721\n",
      "Epoch 53, CIFAR-10 Batch 2:  Minibatch Loss= 0.0103, Training Accuracy= 0.716\n",
      "Epoch 53, CIFAR-10 Batch 3:  Minibatch Loss= 0.0083, Training Accuracy= 0.721\n",
      "Epoch 53, CIFAR-10 Batch 4:  Minibatch Loss= 0.0116, Training Accuracy= 0.721\n",
      "Epoch 53, CIFAR-10 Batch 5:  Minibatch Loss= 0.0078, Training Accuracy= 0.716\n",
      "Epoch 54, CIFAR-10 Batch 1:  Minibatch Loss= 0.0162, Training Accuracy= 0.706\n",
      "Epoch 54, CIFAR-10 Batch 2:  Minibatch Loss= 0.0114, Training Accuracy= 0.711\n",
      "Epoch 54, CIFAR-10 Batch 3:  Minibatch Loss= 0.0121, Training Accuracy= 0.715\n",
      "Epoch 54, CIFAR-10 Batch 4:  Minibatch Loss= 0.0067, Training Accuracy= 0.721\n",
      "Epoch 54, CIFAR-10 Batch 5:  Minibatch Loss= 0.0069, Training Accuracy= 0.721\n",
      "Epoch 55, CIFAR-10 Batch 1:  Minibatch Loss= 0.0087, Training Accuracy= 0.714\n",
      "Epoch 55, CIFAR-10 Batch 2:  Minibatch Loss= 0.0088, Training Accuracy= 0.711\n",
      "Epoch 55, CIFAR-10 Batch 3:  Minibatch Loss= 0.0127, Training Accuracy= 0.721\n",
      "Epoch 55, CIFAR-10 Batch 4:  Minibatch Loss= 0.0101, Training Accuracy= 0.718\n",
      "Epoch 55, CIFAR-10 Batch 5:  Minibatch Loss= 0.0065, Training Accuracy= 0.714\n",
      "Epoch 56, CIFAR-10 Batch 1:  Minibatch Loss= 0.0094, Training Accuracy= 0.710\n",
      "Epoch 56, CIFAR-10 Batch 2:  Minibatch Loss= 0.0113, Training Accuracy= 0.714\n",
      "Epoch 56, CIFAR-10 Batch 3:  Minibatch Loss= 0.0096, Training Accuracy= 0.719\n",
      "Epoch 56, CIFAR-10 Batch 4:  Minibatch Loss= 0.0120, Training Accuracy= 0.714\n",
      "Epoch 56, CIFAR-10 Batch 5:  Minibatch Loss= 0.0053, Training Accuracy= 0.712\n",
      "Epoch 57, CIFAR-10 Batch 1:  Minibatch Loss= 0.0060, Training Accuracy= 0.714\n",
      "Epoch 57, CIFAR-10 Batch 2:  Minibatch Loss= 0.0073, Training Accuracy= 0.715\n",
      "Epoch 57, CIFAR-10 Batch 3:  Minibatch Loss= 0.0065, Training Accuracy= 0.719\n",
      "Epoch 57, CIFAR-10 Batch 4:  Minibatch Loss= 0.0092, Training Accuracy= 0.718\n",
      "Epoch 57, CIFAR-10 Batch 5:  Minibatch Loss= 0.0053, Training Accuracy= 0.718\n",
      "Epoch 58, CIFAR-10 Batch 1:  Minibatch Loss= 0.0090, Training Accuracy= 0.701\n",
      "Epoch 58, CIFAR-10 Batch 2:  Minibatch Loss= 0.0086, Training Accuracy= 0.708\n",
      "Epoch 58, CIFAR-10 Batch 3:  Minibatch Loss= 0.0073, Training Accuracy= 0.716\n",
      "Epoch 58, CIFAR-10 Batch 4:  Minibatch Loss= 0.0091, Training Accuracy= 0.713\n",
      "Epoch 58, CIFAR-10 Batch 5:  Minibatch Loss= 0.0045, Training Accuracy= 0.715\n",
      "Epoch 59, CIFAR-10 Batch 1:  Minibatch Loss= 0.0062, Training Accuracy= 0.703\n",
      "Epoch 59, CIFAR-10 Batch 2:  Minibatch Loss= 0.0062, Training Accuracy= 0.717\n",
      "Epoch 59, CIFAR-10 Batch 3:  Minibatch Loss= 0.0089, Training Accuracy= 0.718\n",
      "Epoch 59, CIFAR-10 Batch 4:  Minibatch Loss= 0.0082, Training Accuracy= 0.715\n",
      "Epoch 59, CIFAR-10 Batch 5:  Minibatch Loss= 0.0050, Training Accuracy= 0.715\n",
      "Epoch 60, CIFAR-10 Batch 1:  Minibatch Loss= 0.0056, Training Accuracy= 0.709\n",
      "Epoch 60, CIFAR-10 Batch 2:  Minibatch Loss= 0.0055, Training Accuracy= 0.719\n",
      "Epoch 60, CIFAR-10 Batch 3:  Minibatch Loss= 0.0049, Training Accuracy= 0.712\n",
      "Epoch 60, CIFAR-10 Batch 4:  Minibatch Loss= 0.0050, Training Accuracy= 0.710\n",
      "Epoch 60, CIFAR-10 Batch 5:  Minibatch Loss= 0.0032, Training Accuracy= 0.713\n",
      "Epoch 61, CIFAR-10 Batch 1:  Minibatch Loss= 0.0034, Training Accuracy= 0.715\n",
      "Epoch 61, CIFAR-10 Batch 2:  Minibatch Loss= 0.0037, Training Accuracy= 0.715\n",
      "Epoch 61, CIFAR-10 Batch 3:  Minibatch Loss= 0.0074, Training Accuracy= 0.713\n",
      "Epoch 61, CIFAR-10 Batch 4:  Minibatch Loss= 0.0064, Training Accuracy= 0.709\n",
      "Epoch 61, CIFAR-10 Batch 5:  Minibatch Loss= 0.0053, Training Accuracy= 0.714\n",
      "Epoch 62, CIFAR-10 Batch 1:  Minibatch Loss= 0.0063, Training Accuracy= 0.707\n",
      "Epoch 62, CIFAR-10 Batch 2:  Minibatch Loss= 0.0047, Training Accuracy= 0.714\n",
      "Epoch 62, CIFAR-10 Batch 3:  Minibatch Loss= 0.0049, Training Accuracy= 0.715\n",
      "Epoch 62, CIFAR-10 Batch 4:  Minibatch Loss= 0.0068, Training Accuracy= 0.715\n",
      "Epoch 62, CIFAR-10 Batch 5:  Minibatch Loss= 0.0024, Training Accuracy= 0.716\n",
      "Epoch 63, CIFAR-10 Batch 1:  Minibatch Loss= 0.0039, Training Accuracy= 0.710\n",
      "Epoch 63, CIFAR-10 Batch 2:  Minibatch Loss= 0.0024, Training Accuracy= 0.711\n",
      "Epoch 63, CIFAR-10 Batch 3:  Minibatch Loss= 0.0068, Training Accuracy= 0.715\n",
      "Epoch 63, CIFAR-10 Batch 4:  Minibatch Loss= 0.0060, Training Accuracy= 0.716\n",
      "Epoch 63, CIFAR-10 Batch 5:  Minibatch Loss= 0.0038, Training Accuracy= 0.719\n",
      "Epoch 64, CIFAR-10 Batch 1:  Minibatch Loss= 0.0055, Training Accuracy= 0.708\n",
      "Epoch 64, CIFAR-10 Batch 2:  Minibatch Loss= 0.0032, Training Accuracy= 0.722\n",
      "Epoch 64, CIFAR-10 Batch 3:  Minibatch Loss= 0.0048, Training Accuracy= 0.719\n",
      "Epoch 64, CIFAR-10 Batch 4:  Minibatch Loss= 0.0039, Training Accuracy= 0.713\n",
      "Epoch 64, CIFAR-10 Batch 5:  Minibatch Loss= 0.0043, Training Accuracy= 0.711\n",
      "Epoch 65, CIFAR-10 Batch 1:  Minibatch Loss= 0.0029, Training Accuracy= 0.711\n",
      "Epoch 65, CIFAR-10 Batch 2:  Minibatch Loss= 0.0018, Training Accuracy= 0.712\n",
      "Epoch 65, CIFAR-10 Batch 3:  Minibatch Loss= 0.0049, Training Accuracy= 0.713\n",
      "Epoch 65, CIFAR-10 Batch 4:  Minibatch Loss= 0.0038, Training Accuracy= 0.711\n",
      "Epoch 65, CIFAR-10 Batch 5:  Minibatch Loss= 0.0024, Training Accuracy= 0.714\n",
      "Epoch 66, CIFAR-10 Batch 1:  Minibatch Loss= 0.0077, Training Accuracy= 0.695\n",
      "Epoch 66, CIFAR-10 Batch 2:  Minibatch Loss= 0.0031, Training Accuracy= 0.717\n",
      "Epoch 66, CIFAR-10 Batch 3:  Minibatch Loss= 0.0042, Training Accuracy= 0.711\n",
      "Epoch 66, CIFAR-10 Batch 4:  Minibatch Loss= 0.0026, Training Accuracy= 0.714\n",
      "Epoch 66, CIFAR-10 Batch 5:  Minibatch Loss= 0.0017, Training Accuracy= 0.716\n",
      "Epoch 67, CIFAR-10 Batch 1:  Minibatch Loss= 0.0067, Training Accuracy= 0.706\n",
      "Epoch 67, CIFAR-10 Batch 2:  Minibatch Loss= 0.0028, Training Accuracy= 0.712\n",
      "Epoch 67, CIFAR-10 Batch 3:  Minibatch Loss= 0.0053, Training Accuracy= 0.705\n",
      "Epoch 67, CIFAR-10 Batch 4:  Minibatch Loss= 0.0043, Training Accuracy= 0.709\n",
      "Epoch 67, CIFAR-10 Batch 5:  Minibatch Loss= 0.0021, Training Accuracy= 0.714\n",
      "Epoch 68, CIFAR-10 Batch 1:  Minibatch Loss= 0.0030, Training Accuracy= 0.704\n",
      "Epoch 68, CIFAR-10 Batch 2:  Minibatch Loss= 0.0028, Training Accuracy= 0.716\n",
      "Epoch 68, CIFAR-10 Batch 3:  Minibatch Loss= 0.0031, Training Accuracy= 0.716\n",
      "Epoch 68, CIFAR-10 Batch 4:  Minibatch Loss= 0.0050, Training Accuracy= 0.708\n",
      "Epoch 68, CIFAR-10 Batch 5:  Minibatch Loss= 0.0018, Training Accuracy= 0.715\n",
      "Epoch 69, CIFAR-10 Batch 1:  Minibatch Loss= 0.0022, Training Accuracy= 0.705\n",
      "Epoch 69, CIFAR-10 Batch 2:  Minibatch Loss= 0.0021, Training Accuracy= 0.718\n",
      "Epoch 69, CIFAR-10 Batch 3:  Minibatch Loss= 0.0025, Training Accuracy= 0.716\n",
      "Epoch 69, CIFAR-10 Batch 4:  Minibatch Loss= 0.0037, Training Accuracy= 0.716\n",
      "Epoch 69, CIFAR-10 Batch 5:  Minibatch Loss= 0.0040, Training Accuracy= 0.719\n",
      "Epoch 70, CIFAR-10 Batch 1:  Minibatch Loss= 0.0017, Training Accuracy= 0.703\n",
      "Epoch 70, CIFAR-10 Batch 2:  Minibatch Loss= 0.0018, Training Accuracy= 0.712\n",
      "Epoch 70, CIFAR-10 Batch 3:  Minibatch Loss= 0.0024, Training Accuracy= 0.712\n",
      "Epoch 70, CIFAR-10 Batch 4:  Minibatch Loss= 0.0019, Training Accuracy= 0.713\n",
      "Epoch 70, CIFAR-10 Batch 5:  Minibatch Loss= 0.0018, Training Accuracy= 0.715\n",
      "Epoch 71, CIFAR-10 Batch 1:  Minibatch Loss= 0.0046, Training Accuracy= 0.701\n",
      "Epoch 71, CIFAR-10 Batch 2:  Minibatch Loss= 0.0012, Training Accuracy= 0.716\n",
      "Epoch 71, CIFAR-10 Batch 3:  Minibatch Loss= 0.0029, Training Accuracy= 0.722\n",
      "Epoch 71, CIFAR-10 Batch 4:  Minibatch Loss= 0.0028, Training Accuracy= 0.717\n",
      "Epoch 71, CIFAR-10 Batch 5:  Minibatch Loss= 0.0019, Training Accuracy= 0.717\n",
      "Epoch 72, CIFAR-10 Batch 1:  Minibatch Loss= 0.0049, Training Accuracy= 0.712\n",
      "Epoch 72, CIFAR-10 Batch 2:  Minibatch Loss= 0.0021, Training Accuracy= 0.714\n",
      "Epoch 72, CIFAR-10 Batch 3:  Minibatch Loss= 0.0023, Training Accuracy= 0.718\n",
      "Epoch 72, CIFAR-10 Batch 4:  Minibatch Loss= 0.0043, Training Accuracy= 0.722\n",
      "Epoch 72, CIFAR-10 Batch 5:  Minibatch Loss= 0.0018, Training Accuracy= 0.719\n",
      "Epoch 73, CIFAR-10 Batch 1:  Minibatch Loss= 0.0017, Training Accuracy= 0.715\n",
      "Epoch 73, CIFAR-10 Batch 2:  Minibatch Loss= 0.0039, Training Accuracy= 0.718\n",
      "Epoch 73, CIFAR-10 Batch 3:  Minibatch Loss= 0.0033, Training Accuracy= 0.715\n",
      "Epoch 73, CIFAR-10 Batch 4:  Minibatch Loss= 0.0023, Training Accuracy= 0.721\n",
      "Epoch 73, CIFAR-10 Batch 5:  Minibatch Loss= 0.0034, Training Accuracy= 0.717\n",
      "Epoch 74, CIFAR-10 Batch 1:  Minibatch Loss= 0.0018, Training Accuracy= 0.719\n",
      "Epoch 74, CIFAR-10 Batch 2:  Minibatch Loss= 0.0015, Training Accuracy= 0.718\n",
      "Epoch 74, CIFAR-10 Batch 3:  Minibatch Loss= 0.0018, Training Accuracy= 0.716\n",
      "Epoch 74, CIFAR-10 Batch 4:  Minibatch Loss= 0.0029, Training Accuracy= 0.719\n",
      "Epoch 74, CIFAR-10 Batch 5:  Minibatch Loss= 0.0011, Training Accuracy= 0.721\n",
      "Epoch 75, CIFAR-10 Batch 1:  Minibatch Loss= 0.0019, Training Accuracy= 0.718\n",
      "Epoch 75, CIFAR-10 Batch 2:  Minibatch Loss= 0.0018, Training Accuracy= 0.717\n",
      "Epoch 75, CIFAR-10 Batch 3:  Minibatch Loss= 0.0026, Training Accuracy= 0.717\n",
      "Epoch 75, CIFAR-10 Batch 4:  Minibatch Loss= 0.0014, Training Accuracy= 0.721\n",
      "Epoch 75, CIFAR-10 Batch 5:  Minibatch Loss= 0.0014, Training Accuracy= 0.721\n",
      "Epoch 76, CIFAR-10 Batch 1:  Minibatch Loss= 0.0040, Training Accuracy= 0.713\n",
      "Epoch 76, CIFAR-10 Batch 2:  Minibatch Loss= 0.0019, Training Accuracy= 0.724\n",
      "Epoch 76, CIFAR-10 Batch 3:  Minibatch Loss= 0.0013, Training Accuracy= 0.714\n",
      "Epoch 76, CIFAR-10 Batch 4:  Minibatch Loss= 0.0019, Training Accuracy= 0.719\n",
      "Epoch 76, CIFAR-10 Batch 5:  Minibatch Loss= 0.0013, Training Accuracy= 0.718\n",
      "Epoch 77, CIFAR-10 Batch 1:  Minibatch Loss= 0.0012, Training Accuracy= 0.714\n",
      "Epoch 77, CIFAR-10 Batch 2:  Minibatch Loss= 0.0017, Training Accuracy= 0.721\n",
      "Epoch 77, CIFAR-10 Batch 3:  Minibatch Loss= 0.0013, Training Accuracy= 0.718\n",
      "Epoch 77, CIFAR-10 Batch 4:  Minibatch Loss= 0.0020, Training Accuracy= 0.723\n",
      "Epoch 77, CIFAR-10 Batch 5:  Minibatch Loss= 0.0014, Training Accuracy= 0.719\n",
      "Epoch 78, CIFAR-10 Batch 1:  Minibatch Loss= 0.0020, Training Accuracy= 0.712\n",
      "Epoch 78, CIFAR-10 Batch 2:  Minibatch Loss= 0.0007, Training Accuracy= 0.720\n",
      "Epoch 78, CIFAR-10 Batch 3:  Minibatch Loss= 0.0008, Training Accuracy= 0.720\n",
      "Epoch 78, CIFAR-10 Batch 4:  Minibatch Loss= 0.0011, Training Accuracy= 0.717\n",
      "Epoch 78, CIFAR-10 Batch 5:  Minibatch Loss= 0.0008, Training Accuracy= 0.720\n",
      "Epoch 79, CIFAR-10 Batch 1:  Minibatch Loss= 0.0014, Training Accuracy= 0.714\n",
      "Epoch 79, CIFAR-10 Batch 2:  Minibatch Loss= 0.0011, Training Accuracy= 0.712\n",
      "Epoch 79, CIFAR-10 Batch 3:  Minibatch Loss= 0.0009, Training Accuracy= 0.721\n",
      "Epoch 79, CIFAR-10 Batch 4:  Minibatch Loss= 0.0024, Training Accuracy= 0.715\n",
      "Epoch 79, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 80, CIFAR-10 Batch 1:  Minibatch Loss= 0.0020, Training Accuracy= 0.704\n",
      "Epoch 80, CIFAR-10 Batch 2:  Minibatch Loss= 0.0010, Training Accuracy= 0.718\n",
      "Epoch 81, CIFAR-10 Batch 1:  Minibatch Loss= 0.0022, Training Accuracy= 0.715\n",
      "Epoch 81, CIFAR-10 Batch 2:  Minibatch Loss= 0.0008, Training Accuracy= 0.723\n",
      "Epoch 81, CIFAR-10 Batch 3:  Minibatch Loss= 0.0009, Training Accuracy= 0.722\n",
      "Epoch 81, CIFAR-10 Batch 4:  Minibatch Loss= 0.0010, Training Accuracy= 0.718\n",
      "Epoch 81, CIFAR-10 Batch 5:  Minibatch Loss= 0.0013, Training Accuracy= 0.716\n",
      "Epoch 82, CIFAR-10 Batch 1:  Minibatch Loss= 0.0012, Training Accuracy= 0.725\n",
      "Epoch 82, CIFAR-10 Batch 2:  Minibatch Loss= 0.0010, Training Accuracy= 0.712\n",
      "Epoch 82, CIFAR-10 Batch 3:  Minibatch Loss= 0.0009, Training Accuracy= 0.719\n",
      "Epoch 82, CIFAR-10 Batch 4:  Minibatch Loss= 0.0020, Training Accuracy= 0.722\n",
      "Epoch 82, CIFAR-10 Batch 5:  Minibatch Loss= 0.0007, Training Accuracy= 0.717\n",
      "Epoch 83, CIFAR-10 Batch 1:  Minibatch Loss= 0.0019, Training Accuracy= 0.718\n",
      "Epoch 83, CIFAR-10 Batch 2:  Minibatch Loss= 0.0009, Training Accuracy= 0.721\n",
      "Epoch 83, CIFAR-10 Batch 3:  Minibatch Loss= 0.0013, Training Accuracy= 0.725\n",
      "Epoch 83, CIFAR-10 Batch 4:  Minibatch Loss= 0.0013, Training Accuracy= 0.721\n",
      "Epoch 83, CIFAR-10 Batch 5:  Minibatch Loss= 0.0013, Training Accuracy= 0.716\n",
      "Epoch 84, CIFAR-10 Batch 1:  Minibatch Loss= 0.0006, Training Accuracy= 0.715\n",
      "Epoch 84, CIFAR-10 Batch 2:  Minibatch Loss= 0.0010, Training Accuracy= 0.712\n",
      "Epoch 84, CIFAR-10 Batch 3:  Minibatch Loss= 0.0012, Training Accuracy= 0.721\n",
      "Epoch 84, CIFAR-10 Batch 4:  Minibatch Loss= 0.0009, Training Accuracy= 0.713\n",
      "Epoch 84, CIFAR-10 Batch 5:  Minibatch Loss= 0.0007, Training Accuracy= 0.715\n",
      "Epoch 85, CIFAR-10 Batch 1:  Minibatch Loss= 0.0007, Training Accuracy= 0.712\n",
      "Epoch 85, CIFAR-10 Batch 2:  Minibatch Loss= 0.0031, Training Accuracy= 0.719\n",
      "Epoch 85, CIFAR-10 Batch 3:  Minibatch Loss= 0.0005, Training Accuracy= 0.725\n",
      "Epoch 85, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.722\n",
      "Epoch 85, CIFAR-10 Batch 5:  Minibatch Loss= 0.0010, Training Accuracy= 0.716\n",
      "Epoch 86, CIFAR-10 Batch 1:  Minibatch Loss= 0.0008, Training Accuracy= 0.722\n",
      "Epoch 86, CIFAR-10 Batch 2:  Minibatch Loss= 0.0007, Training Accuracy= 0.718\n",
      "Epoch 86, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.724\n",
      "Epoch 86, CIFAR-10 Batch 4:  Minibatch Loss= 0.0006, Training Accuracy= 0.724\n",
      "Epoch 86, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.722\n",
      "Epoch 87, CIFAR-10 Batch 1:  Minibatch Loss= 0.0011, Training Accuracy= 0.717\n",
      "Epoch 87, CIFAR-10 Batch 2:  Minibatch Loss= 0.0011, Training Accuracy= 0.725\n",
      "Epoch 87, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.722\n",
      "Epoch 87, CIFAR-10 Batch 4:  Minibatch Loss= 0.0010, Training Accuracy= 0.718\n",
      "Epoch 87, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.716\n",
      "Epoch 88, CIFAR-10 Batch 1:  Minibatch Loss= 0.0007, Training Accuracy= 0.720\n",
      "Epoch 88, CIFAR-10 Batch 2:  Minibatch Loss= 0.0006, Training Accuracy= 0.717\n",
      "Epoch 88, CIFAR-10 Batch 3:  Minibatch Loss= 0.0006, Training Accuracy= 0.714\n",
      "Epoch 88, CIFAR-10 Batch 4:  Minibatch Loss= 0.0015, Training Accuracy= 0.718\n",
      "Epoch 88, CIFAR-10 Batch 5:  Minibatch Loss= 0.0009, Training Accuracy= 0.718\n",
      "Epoch 89, CIFAR-10 Batch 1:  Minibatch Loss= 0.0007, Training Accuracy= 0.720\n",
      "Epoch 89, CIFAR-10 Batch 2:  Minibatch Loss= 0.0006, Training Accuracy= 0.723\n",
      "Epoch 89, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.723\n",
      "Epoch 90, CIFAR-10 Batch 1:  Minibatch Loss= 0.0008, Training Accuracy= 0.725\n",
      "Epoch 90, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 90, CIFAR-10 Batch 3:  Minibatch Loss= 0.0005, Training Accuracy= 0.718\n",
      "Epoch 90, CIFAR-10 Batch 4:  Minibatch Loss= 0.0010, Training Accuracy= 0.711\n",
      "Epoch 90, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.712\n",
      "Epoch 91, CIFAR-10 Batch 1:  Minibatch Loss= 0.0005, Training Accuracy= 0.719\n",
      "Epoch 91, CIFAR-10 Batch 2:  Minibatch Loss= 0.0004, Training Accuracy= 0.724\n",
      "Epoch 91, CIFAR-10 Batch 3:  Minibatch Loss= 0.0010, Training Accuracy= 0.711\n",
      "Epoch 91, CIFAR-10 Batch 4:  Minibatch Loss= 0.0020, Training Accuracy= 0.714\n",
      "Epoch 91, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 92, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.724\n",
      "Epoch 92, CIFAR-10 Batch 2:  Minibatch Loss= 0.0005, Training Accuracy= 0.719\n",
      "Epoch 92, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 92, CIFAR-10 Batch 4:  Minibatch Loss= 0.0012, Training Accuracy= 0.720\n",
      "Epoch 92, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.713\n",
      "Epoch 93, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 93, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 93, CIFAR-10 Batch 3:  Minibatch Loss= 0.0013, Training Accuracy= 0.718\n",
      "Epoch 93, CIFAR-10 Batch 4:  Minibatch Loss= 0.0011, Training Accuracy= 0.720\n",
      "Epoch 93, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.716\n",
      "Epoch 94, CIFAR-10 Batch 1:  Minibatch Loss= 0.0008, Training Accuracy= 0.715\n",
      "Epoch 94, CIFAR-10 Batch 2:  Minibatch Loss= 0.0005, Training Accuracy= 0.718\n",
      "Epoch 94, CIFAR-10 Batch 3:  Minibatch Loss= 0.0006, Training Accuracy= 0.709\n",
      "Epoch 94, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.710\n",
      "Epoch 94, CIFAR-10 Batch 5:  Minibatch Loss= 0.0008, Training Accuracy= 0.712\n",
      "Epoch 95, CIFAR-10 Batch 1:  Minibatch Loss= 0.0005, Training Accuracy= 0.719\n",
      "Epoch 95, CIFAR-10 Batch 2:  Minibatch Loss= 0.0009, Training Accuracy= 0.716\n",
      "Epoch 95, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 95, CIFAR-10 Batch 4:  Minibatch Loss= 0.0007, Training Accuracy= 0.707\n",
      "Epoch 95, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 96, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.715\n",
      "Epoch 96, CIFAR-10 Batch 2:  Minibatch Loss= 0.0005, Training Accuracy= 0.722\n",
      "Epoch 96, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 96, CIFAR-10 Batch 4:  Minibatch Loss= 0.0012, Training Accuracy= 0.720\n",
      "Epoch 96, CIFAR-10 Batch 5:  Minibatch Loss= 0.0009, Training Accuracy= 0.714\n",
      "Epoch 97, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.724\n",
      "Epoch 97, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 97, CIFAR-10 Batch 3:  Minibatch Loss= 0.0006, Training Accuracy= 0.718\n",
      "Epoch 97, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.716\n",
      "Epoch 97, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.721\n",
      "Epoch 98, CIFAR-10 Batch 1:  Minibatch Loss= 0.0008, Training Accuracy= 0.719\n",
      "Epoch 98, CIFAR-10 Batch 2:  Minibatch Loss= 0.0006, Training Accuracy= 0.725\n",
      "Epoch 98, CIFAR-10 Batch 3:  Minibatch Loss= 0.0005, Training Accuracy= 0.720\n",
      "Epoch 98, CIFAR-10 Batch 4:  Minibatch Loss= 0.0030, Training Accuracy= 0.717\n",
      "Epoch 98, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.715\n",
      "Epoch 99, CIFAR-10 Batch 1:  Minibatch Loss= 0.0006, Training Accuracy= 0.713\n",
      "Epoch 99, CIFAR-10 Batch 2:  Minibatch Loss= 0.0005, Training Accuracy= 0.726\n",
      "Epoch 99, CIFAR-10 Batch 3:  Minibatch Loss= 0.0005, Training Accuracy= 0.717\n",
      "Epoch 99, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.720\n",
      "Epoch 99, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 100, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 100, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.724\n",
      "Epoch 100, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.721\n",
      "Epoch 100, CIFAR-10 Batch 4:  Minibatch Loss= 0.0007, Training Accuracy= 0.718\n",
      "Epoch 100, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.713\n",
      "Epoch 101, CIFAR-10 Batch 1:  Minibatch Loss= 0.0008, Training Accuracy= 0.717\n",
      "Epoch 101, CIFAR-10 Batch 2:  Minibatch Loss= 0.0006, Training Accuracy= 0.719\n",
      "Epoch 101, CIFAR-10 Batch 3:  Minibatch Loss= 0.0006, Training Accuracy= 0.724\n",
      "Epoch 101, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.723\n",
      "Epoch 101, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.720\n",
      "Epoch 102, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 102, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 102, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 102, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.720\n",
      "Epoch 102, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.715\n",
      "Epoch 103, CIFAR-10 Batch 1:  Minibatch Loss= 0.0006, Training Accuracy= 0.721\n",
      "Epoch 103, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 104, CIFAR-10 Batch 1:  Minibatch Loss= 0.0006, Training Accuracy= 0.715\n",
      "Epoch 104, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.725\n",
      "Epoch 104, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 104, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 104, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 105, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 105, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 105, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 105, CIFAR-10 Batch 4:  Minibatch Loss= 0.0006, Training Accuracy= 0.722\n",
      "Epoch 105, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.716\n",
      "Epoch 106, CIFAR-10 Batch 1:  Minibatch Loss= 0.0014, Training Accuracy= 0.713\n",
      "Epoch 106, CIFAR-10 Batch 2:  Minibatch Loss= 0.0004, Training Accuracy= 0.721\n",
      "Epoch 106, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.719\n",
      "Epoch 106, CIFAR-10 Batch 4:  Minibatch Loss= 0.0006, Training Accuracy= 0.719\n",
      "Epoch 106, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 107, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 107, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.726\n",
      "Epoch 107, CIFAR-10 Batch 3:  Minibatch Loss= 0.0006, Training Accuracy= 0.716\n",
      "Epoch 107, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.713\n",
      "Epoch 107, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.718\n",
      "Epoch 108, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.721\n",
      "Epoch 108, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.726\n",
      "Epoch 108, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 108, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.718\n",
      "Epoch 108, CIFAR-10 Batch 5:  Minibatch Loss= 0.0007, Training Accuracy= 0.719\n",
      "Epoch 109, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.718\n",
      "Epoch 109, CIFAR-10 Batch 2:  Minibatch Loss= 0.0004, Training Accuracy= 0.724\n",
      "Epoch 109, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.725\n",
      "Epoch 109, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 109, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 110, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.724\n",
      "Epoch 110, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 110, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 110, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 110, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 111, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 111, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.723\n",
      "Epoch 111, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.720\n",
      "Epoch 111, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.723\n",
      "Epoch 111, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.711\n",
      "Epoch 112, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.720\n",
      "Epoch 112, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 112, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.720\n",
      "Epoch 112, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.722\n",
      "Epoch 112, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 113, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 113, CIFAR-10 Batch 2:  Minibatch Loss= 0.0005, Training Accuracy= 0.717\n",
      "Epoch 113, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 113, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.716\n",
      "Epoch 113, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 114, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.722\n",
      "Epoch 114, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.725\n",
      "Epoch 114, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.721\n",
      "Epoch 114, CIFAR-10 Batch 4:  Minibatch Loss= 0.0009, Training Accuracy= 0.716\n",
      "Epoch 114, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 115, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.715\n",
      "Epoch 115, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 115, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.725\n",
      "Epoch 115, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 115, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 116, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 116, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 116, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 116, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.716\n",
      "Epoch 116, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.721\n",
      "Epoch 117, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 117, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 117, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.724\n",
      "Epoch 117, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.722\n",
      "Epoch 117, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 118, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 118, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.726\n",
      "Epoch 118, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 118, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.723\n",
      "Epoch 118, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.721\n",
      "Epoch 119, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.724\n",
      "Epoch 119, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.724\n",
      "Epoch 119, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.719\n",
      "Epoch 119, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 119, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 120, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.724\n",
      "Epoch 120, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 120, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 120, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.714\n",
      "Epoch 120, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 121, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 121, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 121, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 121, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 121, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 122, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.722\n",
      "Epoch 122, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 122, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 122, CIFAR-10 Batch 4:  Minibatch Loss= 0.0007, Training Accuracy= 0.719\n",
      "Epoch 122, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 123, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 123, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.722\n",
      "Epoch 123, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 123, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 123, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 124, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 124, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 124, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 124, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.728\n",
      "Epoch 124, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 125, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 125, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.724\n",
      "Epoch 125, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.725\n",
      "Epoch 125, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.728\n",
      "Epoch 125, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 126, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 126, CIFAR-10 Batch 2:  Minibatch Loss= 0.0004, Training Accuracy= 0.726\n",
      "Epoch 126, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 126, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 126, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 127, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 127, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 127, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 127, CIFAR-10 Batch 4:  Minibatch Loss= 0.0012, Training Accuracy= 0.714\n",
      "Epoch 127, CIFAR-10 Batch 5:  Minibatch Loss= 0.0006, Training Accuracy= 0.725\n",
      "Epoch 128, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 128, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 128, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.724\n",
      "Epoch 128, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.715\n",
      "Epoch 128, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 129, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 129, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.716\n",
      "Epoch 129, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 129, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 130, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 130, CIFAR-10 Batch 2:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 130, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.720\n",
      "Epoch 130, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 130, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 131, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 131, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.718\n",
      "Epoch 131, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.725\n",
      "Epoch 131, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 131, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 132, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.722\n",
      "Epoch 132, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 132, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 132, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 132, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 133, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 133, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 133, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 133, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 133, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.716\n",
      "Epoch 134, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 134, CIFAR-10 Batch 2:  Minibatch Loss= 0.0009, Training Accuracy= 0.719\n",
      "Epoch 134, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.728\n",
      "Epoch 134, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.717\n",
      "Epoch 134, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 135, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 135, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 135, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 135, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 135, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 136, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.724\n",
      "Epoch 136, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 136, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 136, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 136, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.718\n",
      "Epoch 137, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 137, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 137, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 137, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 137, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 138, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 138, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 138, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 139, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 139, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 139, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 139, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 139, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 140, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 140, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 140, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 140, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.716\n",
      "Epoch 140, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 141, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 141, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 141, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 141, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 141, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.716\n",
      "Epoch 142, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 142, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 142, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 142, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.712\n",
      "Epoch 142, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 143, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 143, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 143, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 143, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.715\n",
      "Epoch 143, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.712\n",
      "Epoch 144, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 144, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 144, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 144, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 144, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 145, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 145, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 145, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 145, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.712\n",
      "Epoch 145, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 146, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.716\n",
      "Epoch 146, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 146, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 146, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.709\n",
      "Epoch 146, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.724\n",
      "Epoch 147, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 147, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 147, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 147, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.728\n",
      "Epoch 147, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 148, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 148, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 148, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 148, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 148, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.711\n",
      "Epoch 149, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.723\n",
      "Epoch 149, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 149, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 149, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 149, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 150, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 150, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.724\n",
      "Epoch 150, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 150, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 150, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 151, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 151, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.724\n",
      "Epoch 151, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 151, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 151, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 152, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 152, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 152, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 152, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 152, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 153, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.715\n",
      "Epoch 153, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 153, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 153, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 153, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 154, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 154, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 154, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 154, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 154, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 155, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 155, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 155, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 155, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.722\n",
      "Epoch 155, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 156, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 156, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 156, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 156, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 156, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 157, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.723\n",
      "Epoch 157, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 157, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 157, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 157, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 158, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 158, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 158, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 158, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 158, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 159, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 159, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 159, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.708\n",
      "Epoch 159, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.721\n",
      "Epoch 159, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.714\n",
      "Epoch 160, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 160, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 160, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 160, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 160, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 161, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.713\n",
      "Epoch 161, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 161, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 161, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.714\n",
      "Epoch 161, CIFAR-10 Batch 5:  Minibatch Loss= 0.0007, Training Accuracy= 0.709\n",
      "Epoch 162, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.714\n",
      "Epoch 162, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 162, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 162, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.719\n",
      "Epoch 162, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 163, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 163, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 163, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 163, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 163, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 164, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 164, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 164, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 164, CIFAR-10 Batch 4:  Minibatch Loss= 0.0006, Training Accuracy= 0.716\n",
      "Epoch 164, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.711\n",
      "Epoch 165, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 165, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 165, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 165, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.715\n",
      "Epoch 165, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 166, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 166, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 166, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 166, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.728\n",
      "Epoch 166, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.708\n",
      "Epoch 167, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 167, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 167, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 167, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 167, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 168, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 168, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 168, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 168, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.707\n",
      "Epoch 168, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 169, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 169, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 169, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 169, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 169, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 170, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 170, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 170, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 170, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.717\n",
      "Epoch 170, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 171, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 171, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 171, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 171, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 171, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 172, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 172, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 172, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 172, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 172, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 173, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 173, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 173, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 173, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 173, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.715\n",
      "Epoch 174, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 174, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 174, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 174, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 174, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 175, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 175, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 175, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 175, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.714\n",
      "Epoch 175, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 176, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 176, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 176, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 176, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 176, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.707\n",
      "Epoch 177, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 177, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 177, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 177, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 177, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 178, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 178, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 178, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 178, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 178, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.710\n",
      "Epoch 179, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 179, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 179, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 179, CIFAR-10 Batch 4:  Minibatch Loss= 0.0010, Training Accuracy= 0.719\n",
      "Epoch 179, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 180, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 180, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 180, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 180, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 180, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 181, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 181, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 181, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 181, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.710\n",
      "Epoch 181, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 182, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 182, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 182, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 182, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.715\n",
      "Epoch 182, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 183, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 183, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 183, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 183, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.715\n",
      "Epoch 183, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 184, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 184, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 184, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 184, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 184, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 185, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 185, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 185, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 185, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 185, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 186, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 186, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 186, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 186, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 186, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 187, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 187, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 187, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 187, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 187, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 188, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 188, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 188, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 188, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 188, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 189, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 189, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 189, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 189, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 189, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.703\n",
      "Epoch 190, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 190, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 190, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 190, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 190, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 191, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.711\n",
      "Epoch 191, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 191, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 191, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.716\n",
      "Epoch 191, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 192, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 192, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 192, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 192, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.713\n",
      "Epoch 192, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 193, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 193, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 193, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 193, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 193, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 194, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 194, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 194, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 194, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 194, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 195, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 195, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 195, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 195, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 195, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 196, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 196, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 196, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 196, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 196, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 197, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 197, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 197, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 197, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 197, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 198, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 198, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 198, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 198, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 198, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 199, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 199, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 199, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 199, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.716\n",
      "Epoch 199, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 200, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 200, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 200, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 200, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.717\n",
      "Epoch 200, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 201, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 201, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 201, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 201, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 201, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 202, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 202, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 202, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 202, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 202, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 203, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 203, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 203, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 203, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 203, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 204, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 204, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 204, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 204, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 204, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 205, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 205, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 205, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 205, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.708\n",
      "Epoch 205, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 206, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 206, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 206, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 206, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 206, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 207, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 207, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 207, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 207, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 207, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 208, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 208, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 208, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 208, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 208, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 209, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 209, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 209, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 209, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 209, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 210, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 210, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 210, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 210, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 210, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 211, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 211, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 211, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 211, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 211, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 212, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 212, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 212, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 212, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 212, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 213, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 213, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 213, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 213, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 213, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 214, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 214, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 214, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 214, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 214, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 215, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 215, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 215, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 215, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 215, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 216, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 216, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 216, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 216, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 216, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 217, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 217, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 217, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 217, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 217, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 218, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 218, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 218, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 218, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 218, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 219, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 219, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 219, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 219, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 219, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.709\n",
      "Epoch 220, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 220, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 220, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 220, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 220, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 221, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 221, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 221, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 221, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.712\n",
      "Epoch 221, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 222, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 222, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 222, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 222, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 222, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 223, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 223, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 223, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 223, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 223, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 224, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 224, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 224, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.725\n",
      "Epoch 224, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 224, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 225, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 225, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 225, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 225, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 225, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 226, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 226, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 226, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 226, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 226, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 227, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 227, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 227, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 227, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 227, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 228, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 228, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 228, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 228, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 228, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 229, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 229, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 229, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 229, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 229, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 230, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 230, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 230, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 230, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 230, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 231, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 231, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 231, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 231, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 231, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 232, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 232, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 232, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 232, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 232, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 233, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 233, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.709\n",
      "Epoch 233, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 233, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.709\n",
      "Epoch 233, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 234, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 234, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 234, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 234, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 234, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 235, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 235, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 235, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 235, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 235, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 236, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 236, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 236, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 236, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 236, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 237, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 237, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 237, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 237, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 237, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 238, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 238, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 238, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 238, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 238, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 239, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 239, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 239, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 239, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 239, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 240, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 240, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 240, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 240, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 240, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 241, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 241, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 241, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 241, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 241, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 242, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 242, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 242, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 242, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 242, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 243, CIFAR-10 Batch 1:  Minibatch Loss= 0.0014, Training Accuracy= 0.712\n",
      "Epoch 243, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 243, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 243, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 243, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 244, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 244, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 244, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 244, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 244, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 245, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 245, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 245, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 245, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 245, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 246, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 246, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 246, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 246, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 246, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 247, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 247, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 247, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.716\n",
      "Epoch 247, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 247, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 248, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 248, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 248, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 248, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 248, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 249, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 249, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 249, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 249, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 249, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 250, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 250, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 250, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 250, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 250, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 251, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 251, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 251, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 251, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 251, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 252, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 252, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 252, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 252, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 252, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 253, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 253, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 253, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 253, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 253, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 254, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 254, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 254, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 254, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 254, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 255, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 255, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 255, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 255, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 255, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 256, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 256, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 256, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 256, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 256, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 257, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 257, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 257, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 257, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 257, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 258, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 258, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 258, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 258, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 258, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 259, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 259, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 259, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 259, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 259, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 260, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 260, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 260, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 260, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 260, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 261, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 261, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 261, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 261, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 261, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 262, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 262, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 262, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 262, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 262, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 263, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 263, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 263, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 263, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 263, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 264, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 264, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 264, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 264, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 264, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 265, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 265, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 265, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 265, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 265, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 266, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 266, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 266, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 266, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 266, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 267, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 267, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 267, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 267, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 267, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 268, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 268, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 268, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 268, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 268, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 269, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 269, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 269, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 269, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 269, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 270, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 270, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 270, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 270, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 270, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 271, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 271, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 271, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 271, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.709\n",
      "Epoch 271, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 272, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 272, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 272, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 272, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 272, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 273, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 273, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 273, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 273, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 273, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 274, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 274, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 274, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 274, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 274, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 275, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 275, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 275, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 275, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 275, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.713\n",
      "Epoch 276, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 276, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 276, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 276, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 276, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 277, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 277, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 277, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 277, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 277, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 278, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 278, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 278, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 278, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 278, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 279, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 279, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 279, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 279, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 279, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 280, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 280, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 280, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 280, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 280, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 281, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 281, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 281, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.721\n",
      "Epoch 281, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 281, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 282, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 282, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 282, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 282, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 282, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.707\n",
      "Epoch 283, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 283, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 283, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 283, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 283, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 284, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 284, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 284, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 284, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 284, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 285, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 285, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 285, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 285, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 285, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 286, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 286, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 286, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 286, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 286, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 287, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 287, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 287, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 287, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 287, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 288, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 288, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 288, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 288, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 288, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 289, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 289, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 289, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.730\n",
      "Epoch 289, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 289, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 290, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 290, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.728\n",
      "Epoch 290, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 290, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 290, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 291, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 291, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 291, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 291, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 291, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 292, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 292, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 292, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 292, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 292, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 293, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 293, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 293, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 294, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 294, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 294, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 294, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 294, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 295, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 295, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 295, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 295, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 295, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 296, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 296, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 296, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 296, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 296, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 297, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 297, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 297, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 297, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 297, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 298, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 298, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 298, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.728\n",
      "Epoch 298, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 298, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 299, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 299, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 299, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 299, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 299, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 300, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 300, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 300, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 300, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 300, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Max_Acc = 72.960  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    accuracy_max = 0.0\n",
    "    max_ep = 20\n",
    "    i = 0\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            #返回当前学习率\n",
    "            current_acc = print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            #超过20次迭代，学习率不增加，则不再进行训练    ？？ 为什么没有跳出循环\n",
    "            if(accuracy_max <= current_acc):\n",
    "                accuracy_max = current_acc\n",
    "                i = 0\n",
    "            else:\n",
    "                i += 1\n",
    "                if(i==max_ep):\n",
    "                    break\n",
    "    print('Max_Acc ='+' {:.3f}  '.format(accuracy_max*100))        \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "模型已保存到本地。\n",
    "\n",
    "## 测试模型\n",
    "\n",
    "利用测试数据集测试你的模型。这将是最终的准确率。你的准确率应该高于 50%。如果没达到，请继续调整模型结构和参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.7221123417721519\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAJ/CAYAAAB4GhsgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2952c647e80>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 349
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为何准确率只有50-80%？\n",
    "\n",
    "你可能想问，为何准确率不能更高了？首先，对于简单的 CNN 网络来说，50% 已经不低了。纯粹猜测的准确率为10%。但是，你可能注意到有人的准确率[远远超过 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)。这是因为我们还没有介绍所有的神经网络知识。我们还需要掌握一些其他技巧。\n",
    "\n",
    "## 提交项目\n",
    "\n",
    "提交项目时，确保先运行所有单元，然后再保存记事本。将 notebook 文件另存为“dlnd_image_classification.ipynb”，再在目录 \"File\" -> \"Download as\" 另存为 HTML 格式。请在提交的项目中包含 “helper.py” 和 “problem_unittests.py” 文件。\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
