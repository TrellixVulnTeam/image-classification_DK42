{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 图像分类\n",
    "\n",
    "在此项目中，你将对 [CIFAR-10 数据集](https://www.cs.toronto.edu/~kriz/cifar.html) 中的图片进行分类。该数据集包含飞机、猫狗和其他物体。你需要预处理这些图片，然后用所有样本训练一个卷积神经网络。图片需要标准化（normalized），标签需要采用 one-hot 编码。你需要应用所学的知识构建卷积的、最大池化（max pooling）、丢弃（dropout）和完全连接（fully connected）的层。最后，你需要在样本图片上看到神经网络的预测结果。\n",
    "\n",
    "\n",
    "## 获取数据\n",
    "\n",
    "请运行以下单元，以下载 [CIFAR-10 数据集（Python版）](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据\n",
    "\n",
    "该数据集分成了几部分／批次（batches），以免你的机器在计算时内存不足。CIFAR-10 数据集包含 5 个部分，名称分别为 `data_batch_1`、`data_batch_2`，以此类推。每个部分都包含以下某个类别的标签和图片：\n",
    "\n",
    "* 飞机\n",
    "* 汽车\n",
    "* 鸟类\n",
    "* 猫\n",
    "* 鹿\n",
    "* 狗\n",
    "* 青蛙\n",
    "* 马\n",
    "* 船只\n",
    "* 卡车\n",
    "\n",
    "了解数据集也是对数据进行预测的必经步骤。你可以通过更改 `batch_id` 和 `sample_id` 探索下面的代码单元。`batch_id` 是数据集一个部分的 ID（1 到 5）。`sample_id` 是该部分中图片和标签对（label pair）的 ID。\n",
    "\n",
    "问问你自己：“可能的标签有哪些？”、“图片数据的值范围是多少？”、“标签是按顺序排列，还是随机排列的？”。思考类似的问题，有助于你预处理数据，并使预测结果更准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 2:\n",
      "Image - Min Value: 0 Max Value: 240\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKEzRA0Bh/xOcxDplVQV0VQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2952c647c88>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 2\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现预处理函数\n",
    "\n",
    "### 标准化\n",
    "\n",
    "在下面的单元中，实现 `normalize` 函数，传入图片数据 `x`，并返回标准化 Numpy 数组。值应该在 0 到 1 的范围内（含 0 和 1）。返回对象应该和 `x` 的形状一样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a = 0\n",
    "    b = 1\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    \n",
    "    a += ((x - grayscale_min)*(b - a))/(grayscale_max - grayscale_min)\n",
    "    return a\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot 编码\n",
    "\n",
    "和之前的代码单元一样，你将为预处理实现一个函数。这次，你将实现 `one_hot_encode` 函数。输入，也就是 `x`，是一个标签列表。实现该函数，以返回为 one_hot 编码的 Numpy 数组的标签列表。标签的可能值为 0 到 9。每次调用 `one_hot_encode` 时，对于每个值，one_hot 编码函数应该返回相同的编码。确保将编码映射保存到该函数外面。\n",
    "\n",
    "提示：不要重复发明轮子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x = np.array(x).reshape((-1,1))\n",
    "    onehot = helper.LabelBinarizer()\n",
    "    onehot.fit([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    a = onehot.transform(x)\n",
    "    return a\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机化数据\n",
    "\n",
    "之前探索数据时，你已经了解到，样本的顺序是随机的。再随机化一次也不会有什么关系，但是对于这个数据集没有必要。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理所有数据并保存\n",
    "\n",
    "运行下方的代码单元，将预处理所有 CIFAR-10 数据，并保存到文件中。下面的代码还使用了 10% 的训练数据，用来验证。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "这是你的第一个检查点。如果你什么时候决定再回到该记事本，或需要重新启动该记事本，你可以从这里开始。预处理的数据已保存到本地。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels[122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.54901961,  0.49019608,  0.45098039],\n",
       "         [ 0.57254902,  0.50980392,  0.47843137],\n",
       "         [ 0.56078431,  0.49803922,  0.47843137],\n",
       "         ..., \n",
       "         [ 0.66666667,  0.56862745,  0.51372549],\n",
       "         [ 0.69019608,  0.58823529,  0.5254902 ],\n",
       "         [ 0.66666667,  0.57647059,  0.52156863]],\n",
       "\n",
       "        [[ 0.4745098 ,  0.42352941,  0.50588235],\n",
       "         [ 0.50980392,  0.4627451 ,  0.54509804],\n",
       "         [ 0.5254902 ,  0.4745098 ,  0.56078431],\n",
       "         ..., \n",
       "         [ 0.63921569,  0.55294118,  0.61568627],\n",
       "         [ 0.66666667,  0.57254902,  0.63137255],\n",
       "         [ 0.66666667,  0.58039216,  0.63137255]],\n",
       "\n",
       "        [[ 0.59607843,  0.54509804,  0.68235294],\n",
       "         [ 0.61568627,  0.56862745,  0.70196078],\n",
       "         [ 0.60784314,  0.56078431,  0.68627451],\n",
       "         ..., \n",
       "         [ 0.69411765,  0.60392157,  0.75686275],\n",
       "         [ 0.70980392,  0.61176471,  0.76078431],\n",
       "         [ 0.71764706,  0.62745098,  0.76078431]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.49019608,  0.43137255,  0.4       ],\n",
       "         [ 0.50588235,  0.43921569,  0.40392157],\n",
       "         [ 0.29803922,  0.2627451 ,  0.18431373],\n",
       "         ..., \n",
       "         [ 0.65882353,  0.5372549 ,  0.47058824],\n",
       "         [ 0.61960784,  0.49411765,  0.40392157],\n",
       "         [ 0.57254902,  0.45490196,  0.34117647]],\n",
       "\n",
       "        [[ 0.33333333,  0.30196078,  0.2745098 ],\n",
       "         [ 0.36862745,  0.31764706,  0.27843137],\n",
       "         [ 0.29019608,  0.25490196,  0.17647059],\n",
       "         ..., \n",
       "         [ 0.63529412,  0.51764706,  0.41568627],\n",
       "         [ 0.65098039,  0.5254902 ,  0.39215686],\n",
       "         [ 0.61960784,  0.50196078,  0.36078431]],\n",
       "\n",
       "        [[ 0.49019608,  0.43921569,  0.43529412],\n",
       "         [ 0.50980392,  0.44313725,  0.43529412],\n",
       "         [ 0.41176471,  0.35686275,  0.29411765],\n",
       "         ..., \n",
       "         [ 0.51764706,  0.41568627,  0.30588235],\n",
       "         [ 0.50980392,  0.39607843,  0.25098039],\n",
       "         [ 0.55686275,  0.45098039,  0.30588235]]],\n",
       "\n",
       "\n",
       "       [[[ 0.39215686,  0.42745098,  0.32941176],\n",
       "         [ 0.47843137,  0.49411765,  0.42745098],\n",
       "         [ 0.34117647,  0.34117647,  0.29803922],\n",
       "         ..., \n",
       "         [ 0.29411765,  0.30588235,  0.27058824],\n",
       "         [ 0.2745098 ,  0.28627451,  0.25098039],\n",
       "         [ 0.2745098 ,  0.28627451,  0.25098039]],\n",
       "\n",
       "        [[ 0.3372549 ,  0.38823529,  0.27843137],\n",
       "         [ 0.29803922,  0.32941176,  0.25882353],\n",
       "         [ 0.23529412,  0.25098039,  0.21176471],\n",
       "         ..., \n",
       "         [ 0.30588235,  0.31764706,  0.28235294],\n",
       "         [ 0.29803922,  0.30980392,  0.2745098 ],\n",
       "         [ 0.32156863,  0.33333333,  0.29803922]],\n",
       "\n",
       "        [[ 0.32941176,  0.39215686,  0.28627451],\n",
       "         [ 0.3254902 ,  0.37254902,  0.29411765],\n",
       "         [ 0.30196078,  0.3372549 ,  0.28235294],\n",
       "         ..., \n",
       "         [ 0.29019608,  0.30196078,  0.26666667],\n",
       "         [ 0.28627451,  0.29803922,  0.2627451 ],\n",
       "         [ 0.3254902 ,  0.3372549 ,  0.30196078]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.25098039,  0.30196078,  0.30980392],\n",
       "         [ 0.47843137,  0.52156863,  0.56470588],\n",
       "         [ 0.5254902 ,  0.56862745,  0.61176471],\n",
       "         ..., \n",
       "         [ 0.41176471,  0.48235294,  0.47058824],\n",
       "         [ 0.32941176,  0.40392157,  0.35686275],\n",
       "         [ 0.23529412,  0.34509804,  0.24705882]],\n",
       "\n",
       "        [[ 0.17254902,  0.2       ,  0.21960784],\n",
       "         [ 0.30588235,  0.32941176,  0.36862745],\n",
       "         [ 0.37647059,  0.39607843,  0.43137255],\n",
       "         ..., \n",
       "         [ 0.57647059,  0.64705882,  0.69803922],\n",
       "         [ 0.49411765,  0.56078431,  0.58431373],\n",
       "         [ 0.36862745,  0.45882353,  0.44313725]],\n",
       "\n",
       "        [[ 0.14117647,  0.1372549 ,  0.15294118],\n",
       "         [ 0.23137255,  0.22745098,  0.25882353],\n",
       "         [ 0.32156863,  0.31764706,  0.33333333],\n",
       "         ..., \n",
       "         [ 0.5254902 ,  0.6       ,  0.62745098],\n",
       "         [ 0.54117647,  0.59607843,  0.61960784],\n",
       "         [ 0.50980392,  0.58039216,  0.58823529]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0745098 ,  0.1254902 ,  0.05882353],\n",
       "         [ 0.08235294,  0.14901961,  0.08235294],\n",
       "         [ 0.10588235,  0.19215686,  0.1254902 ],\n",
       "         ..., \n",
       "         [ 0.29411765,  0.48627451,  0.51372549],\n",
       "         [ 0.29803922,  0.48627451,  0.50980392],\n",
       "         [ 0.27843137,  0.4627451 ,  0.48627451]],\n",
       "\n",
       "        [[ 0.09019608,  0.12156863,  0.05490196],\n",
       "         [ 0.08235294,  0.11764706,  0.04705882],\n",
       "         [ 0.09019608,  0.1372549 ,  0.05490196],\n",
       "         ..., \n",
       "         [ 0.28235294,  0.4627451 ,  0.49411765],\n",
       "         [ 0.29411765,  0.46666667,  0.48627451],\n",
       "         [ 0.26666667,  0.43529412,  0.44705882]],\n",
       "\n",
       "        [[ 0.09411765,  0.14509804,  0.06666667],\n",
       "         [ 0.08627451,  0.1372549 ,  0.0627451 ],\n",
       "         [ 0.09411765,  0.14117647,  0.07058824],\n",
       "         ..., \n",
       "         [ 0.25098039,  0.42745098,  0.43921569],\n",
       "         [ 0.2627451 ,  0.42745098,  0.43529412],\n",
       "         [ 0.25098039,  0.41176471,  0.41176471]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.24313725,  0.18039216,  0.09019608],\n",
       "         [ 0.23529412,  0.18039216,  0.10588235],\n",
       "         [ 0.21568627,  0.18823529,  0.10980392],\n",
       "         ..., \n",
       "         [ 0.05098039,  0.02352941,  0.01568627],\n",
       "         [ 0.04705882,  0.05490196,  0.03137255],\n",
       "         [ 0.09803922,  0.15686275,  0.11764706]],\n",
       "\n",
       "        [[ 0.24705882,  0.20784314,  0.11764706],\n",
       "         [ 0.19215686,  0.17647059,  0.08627451],\n",
       "         [ 0.17647059,  0.18039216,  0.09019608],\n",
       "         ..., \n",
       "         [ 0.11372549,  0.1372549 ,  0.12156863],\n",
       "         [ 0.11764706,  0.16470588,  0.14509804],\n",
       "         [ 0.10588235,  0.19607843,  0.16862745]],\n",
       "\n",
       "        [[ 0.27058824,  0.20392157,  0.11372549],\n",
       "         [ 0.19215686,  0.14901961,  0.07843137],\n",
       "         [ 0.21176471,  0.18039216,  0.10588235],\n",
       "         ..., \n",
       "         [ 0.25882353,  0.34509804,  0.33333333],\n",
       "         [ 0.15686275,  0.26666667,  0.25098039],\n",
       "         [ 0.11372549,  0.24313725,  0.22745098]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.1372549 ,  0.69803922,  0.92156863],\n",
       "         [ 0.15686275,  0.69019608,  0.9372549 ],\n",
       "         [ 0.16470588,  0.69019608,  0.94509804],\n",
       "         ..., \n",
       "         [ 0.38823529,  0.69411765,  0.85882353],\n",
       "         [ 0.30980392,  0.57647059,  0.77254902],\n",
       "         [ 0.34901961,  0.58039216,  0.74117647]],\n",
       "\n",
       "        [[ 0.22352941,  0.71372549,  0.91764706],\n",
       "         [ 0.17254902,  0.72156863,  0.98039216],\n",
       "         [ 0.19607843,  0.71764706,  0.94117647],\n",
       "         ..., \n",
       "         [ 0.61176471,  0.71372549,  0.78431373],\n",
       "         [ 0.55294118,  0.69411765,  0.80784314],\n",
       "         [ 0.45490196,  0.58431373,  0.68627451]],\n",
       "\n",
       "        [[ 0.38431373,  0.77254902,  0.92941176],\n",
       "         [ 0.25098039,  0.74117647,  0.98823529],\n",
       "         [ 0.27058824,  0.75294118,  0.96078431],\n",
       "         ..., \n",
       "         [ 0.7372549 ,  0.76470588,  0.80784314],\n",
       "         [ 0.46666667,  0.52941176,  0.57647059],\n",
       "         [ 0.23921569,  0.30980392,  0.35294118]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.28627451,  0.30980392,  0.30196078],\n",
       "         [ 0.20784314,  0.24705882,  0.26666667],\n",
       "         [ 0.21176471,  0.26666667,  0.31372549],\n",
       "         ..., \n",
       "         [ 0.06666667,  0.15686275,  0.25098039],\n",
       "         [ 0.08235294,  0.14117647,  0.2       ],\n",
       "         [ 0.12941176,  0.18823529,  0.19215686]],\n",
       "\n",
       "        [[ 0.23921569,  0.26666667,  0.29411765],\n",
       "         [ 0.21568627,  0.2745098 ,  0.3372549 ],\n",
       "         [ 0.22352941,  0.30980392,  0.40392157],\n",
       "         ..., \n",
       "         [ 0.09411765,  0.18823529,  0.28235294],\n",
       "         [ 0.06666667,  0.1372549 ,  0.20784314],\n",
       "         [ 0.02745098,  0.09019608,  0.1254902 ]],\n",
       "\n",
       "        [[ 0.17254902,  0.21960784,  0.28627451],\n",
       "         [ 0.18039216,  0.25882353,  0.34509804],\n",
       "         [ 0.19215686,  0.30196078,  0.41176471],\n",
       "         ..., \n",
       "         [ 0.10588235,  0.20392157,  0.30196078],\n",
       "         [ 0.08235294,  0.16862745,  0.25882353],\n",
       "         [ 0.04705882,  0.12156863,  0.19607843]]],\n",
       "\n",
       "\n",
       "       [[[ 0.74117647,  0.82745098,  0.94117647],\n",
       "         [ 0.72941176,  0.81568627,  0.9254902 ],\n",
       "         [ 0.7254902 ,  0.81176471,  0.92156863],\n",
       "         ..., \n",
       "         [ 0.68627451,  0.76470588,  0.87843137],\n",
       "         [ 0.6745098 ,  0.76078431,  0.87058824],\n",
       "         [ 0.6627451 ,  0.76078431,  0.8627451 ]],\n",
       "\n",
       "        [[ 0.76078431,  0.82352941,  0.9372549 ],\n",
       "         [ 0.74901961,  0.81176471,  0.9254902 ],\n",
       "         [ 0.74509804,  0.80784314,  0.92156863],\n",
       "         ..., \n",
       "         [ 0.67843137,  0.75294118,  0.8627451 ],\n",
       "         [ 0.67058824,  0.74901961,  0.85490196],\n",
       "         [ 0.65490196,  0.74509804,  0.84705882]],\n",
       "\n",
       "        [[ 0.81568627,  0.85882353,  0.95686275],\n",
       "         [ 0.80392157,  0.84705882,  0.94117647],\n",
       "         [ 0.8       ,  0.84313725,  0.9372549 ],\n",
       "         ..., \n",
       "         [ 0.68627451,  0.74901961,  0.85098039],\n",
       "         [ 0.6745098 ,  0.74509804,  0.84705882],\n",
       "         [ 0.6627451 ,  0.74901961,  0.84313725]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.81176471,  0.78039216,  0.70980392],\n",
       "         [ 0.79607843,  0.76470588,  0.68627451],\n",
       "         [ 0.79607843,  0.76862745,  0.67843137],\n",
       "         ..., \n",
       "         [ 0.52941176,  0.51764706,  0.49803922],\n",
       "         [ 0.63529412,  0.61960784,  0.58823529],\n",
       "         [ 0.65882353,  0.63921569,  0.59215686]],\n",
       "\n",
       "        [[ 0.77647059,  0.74509804,  0.66666667],\n",
       "         [ 0.74117647,  0.70980392,  0.62352941],\n",
       "         [ 0.70588235,  0.6745098 ,  0.57647059],\n",
       "         ..., \n",
       "         [ 0.69803922,  0.67058824,  0.62745098],\n",
       "         [ 0.68627451,  0.6627451 ,  0.61176471],\n",
       "         [ 0.68627451,  0.6627451 ,  0.60392157]],\n",
       "\n",
       "        [[ 0.77647059,  0.74117647,  0.67843137],\n",
       "         [ 0.74117647,  0.70980392,  0.63529412],\n",
       "         [ 0.69803922,  0.66666667,  0.58431373],\n",
       "         ..., \n",
       "         [ 0.76470588,  0.72156863,  0.6627451 ],\n",
       "         [ 0.76862745,  0.74117647,  0.67058824],\n",
       "         [ 0.76470588,  0.74509804,  0.67058824]]],\n",
       "\n",
       "\n",
       "       [[[ 0.89803922,  0.89803922,  0.9372549 ],\n",
       "         [ 0.9254902 ,  0.92941176,  0.96862745],\n",
       "         [ 0.91764706,  0.9254902 ,  0.96862745],\n",
       "         ..., \n",
       "         [ 0.85098039,  0.85882353,  0.91372549],\n",
       "         [ 0.86666667,  0.8745098 ,  0.91764706],\n",
       "         [ 0.87058824,  0.8745098 ,  0.91372549]],\n",
       "\n",
       "        [[ 0.87058824,  0.86666667,  0.89803922],\n",
       "         [ 0.9372549 ,  0.9372549 ,  0.97647059],\n",
       "         [ 0.91372549,  0.91764706,  0.96470588],\n",
       "         ..., \n",
       "         [ 0.8745098 ,  0.8745098 ,  0.9254902 ],\n",
       "         [ 0.89019608,  0.89411765,  0.93333333],\n",
       "         [ 0.82352941,  0.82745098,  0.8627451 ]],\n",
       "\n",
       "        [[ 0.83529412,  0.80784314,  0.82745098],\n",
       "         [ 0.91764706,  0.90980392,  0.9372549 ],\n",
       "         [ 0.90588235,  0.91372549,  0.95686275],\n",
       "         ..., \n",
       "         [ 0.8627451 ,  0.8627451 ,  0.90980392],\n",
       "         [ 0.8627451 ,  0.85882353,  0.90980392],\n",
       "         [ 0.79215686,  0.79607843,  0.84313725]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.58823529,  0.56078431,  0.52941176],\n",
       "         [ 0.54901961,  0.52941176,  0.49803922],\n",
       "         [ 0.51764706,  0.49803922,  0.47058824],\n",
       "         ..., \n",
       "         [ 0.87843137,  0.87058824,  0.85490196],\n",
       "         [ 0.90196078,  0.89411765,  0.88235294],\n",
       "         [ 0.94509804,  0.94509804,  0.93333333]],\n",
       "\n",
       "        [[ 0.5372549 ,  0.51764706,  0.49411765],\n",
       "         [ 0.50980392,  0.49803922,  0.47058824],\n",
       "         [ 0.49019608,  0.4745098 ,  0.45098039],\n",
       "         ..., \n",
       "         [ 0.70980392,  0.70588235,  0.69803922],\n",
       "         [ 0.79215686,  0.78823529,  0.77647059],\n",
       "         [ 0.83137255,  0.82745098,  0.81176471]],\n",
       "\n",
       "        [[ 0.47843137,  0.46666667,  0.44705882],\n",
       "         [ 0.4627451 ,  0.45490196,  0.43137255],\n",
       "         [ 0.47058824,  0.45490196,  0.43529412],\n",
       "         ..., \n",
       "         [ 0.70196078,  0.69411765,  0.67843137],\n",
       "         [ 0.64313725,  0.64313725,  0.63529412],\n",
       "         [ 0.63921569,  0.63921569,  0.63137255]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "\n",
    "对于该神经网络，你需要将每层都构建为一个函数。你看到的大部分代码都位于函数外面。要更全面地测试你的代码，我们需要你将每层放入一个函数中。这样使我们能够提供更好的反馈，并使用我们的统一测试检测简单的错误，然后再提交项目。\n",
    "\n",
    ">**注意**：如果你觉得每周很难抽出足够的时间学习这门课程，我们为此项目提供了一个小捷径。对于接下来的几个问题，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 程序包中的类来构建每个层级，但是“卷积和最大池化层级”部分的层级除外。TF Layers 和 Keras 及 TFLearn 层级类似，因此很容易学会。\n",
    "\n",
    ">但是，如果你想充分利用这门课程，请尝试自己解决所有问题，不使用 TF Layers 程序包中的任何类。你依然可以使用其他程序包中的类，这些类和你在 TF Layers 中的类名称是一样的！例如，你可以使用 TF Neural Network 版本的 `conv2d` 类 [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)，而不是 TF Layers 版本的 `conv2d` 类 [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d)。\n",
    "\n",
    "我们开始吧！\n",
    "\n",
    "\n",
    "### 输入\n",
    "\n",
    "神经网络需要读取图片数据、one-hot 编码标签和丢弃保留概率（dropout keep probability）。请实现以下函数：\n",
    "\n",
    "* 实现 `neural_net_image_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `image_shape` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"x\" 命名\n",
    "* 实现 `neural_net_label_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `n_classes` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"y\" 命名\n",
    "* 实现 `neural_net_keep_prob_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)，用于丢弃保留概率\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"keep_prob\" 命名\n",
    "\n",
    "这些名称将在项目结束时，用于加载保存的模型。\n",
    "\n",
    "注意：TensorFlow 中的 `None` 表示形状可以是动态大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x = tf.placeholder(tf.float32, [None, image_shape[0],image_shape[1], image_shape[2]],name = 'x')\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes], name = 'y')\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积和最大池化层\n",
    "\n",
    "卷积层级适合处理图片。对于此代码单元，你应该实现函数 `conv2d_maxpool` 以便应用卷积然后进行最大池化：\n",
    "\n",
    "* 使用 `conv_ksize`、`conv_num_outputs` 和 `x_tensor` 的形状创建权重（weight）和偏置（bias）。\n",
    "* 使用权重和 `conv_strides` 对 `x_tensor` 应用卷积。\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "* 添加偏置\n",
    "* 向卷积中添加非线性激活（nonlinear activation）\n",
    "* 使用 `pool_ksize` 和 `pool_strides` 应用最大池化\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "\n",
    "**注意**：对于**此层**，**请勿使用** [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers)，但是仍然可以使用 TensorFlow 的 [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) 包。对于所有**其他层**，你依然可以使用快捷方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    a = x_tensor.get_shape()[3]\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1],int(a), conv_num_outputs], stddev=0.1))\n",
    "    \n",
    "    biases = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    conv = tf.nn.conv2d(x_tensor, weight, strides=[1,conv_strides[0],conv_strides[1],1], padding = 'SAME')\n",
    "    conv = tf.nn.bias_add(conv, biases)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    pool = tf.nn.max_pool(conv,ksize=[1,pool_ksize[0],pool_ksize[1],1] ,strides=[1,pool_strides[0],pool_strides[1],1], padding = 'SAME')\n",
    "    # TODO: Implement Function\n",
    "    return pool\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扁平化层\n",
    "\n",
    "实现 `flatten` 函数，将 `x_tensor` 的维度从四维张量（4-D tensor）变成二维张量。输出应该是形状（*部分大小（Batch Size）*，*扁平化图片大小（Flattened Image Size）*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    shape = x_tensor.get_shape().as_list() \n",
    "    dim = np.prod(shape[1:])            \n",
    "    x_tensor = tf.reshape(x_tensor, [-1, dim])\n",
    "    \n",
    "    #x_tensor = tf.contrib.layers.flatten(x_tensor)     # 或者这么写\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层\n",
    "\n",
    "实现 `fully_conn` 函数，以向 `x_tensor` 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    fc = tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "    # TODO: Implement Function\n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出层\n",
    "\n",
    "实现 `output` 函数，向 x_tensor 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n",
    "\n",
    "**注意**：该层级不应应用 Activation、softmax 或交叉熵（cross entropy）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    fc = tf.contrib.layers.fully_connected(x_tensor, num_outputs,activation_fn=None)\n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建卷积模型\n",
    "\n",
    "实现函数 `conv_net`， 创建卷积神经网络模型。该函数传入一批图片 `x`，并输出对数（logits）。使用你在上方创建的层创建此模型：\n",
    "\n",
    "* 应用 1、2 或 3 个卷积和最大池化层（Convolution and Max Pool layers）\n",
    "* 应用一个扁平层（Flatten Layer）\n",
    "* 应用 1、2 或 3 个完全连接层（Fully Connected Layers）\n",
    "* 应用一个输出层（Output Layer）\n",
    "* 返回输出\n",
    "* 使用 `keep_prob` 向模型中的一个或多个层应用 [TensorFlow 的 Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 32, (3,3), (1,1),(2,2),(2,2))\n",
    "    \n",
    "    conv2 = conv2d_maxpool(conv1, 64,(3,3), (1,1),(2,2),(2,2))\n",
    "    \n",
    "    conv2 = flatten(conv2)\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc1 = fully_conn(conv2, 512)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    fc2 = fully_conn(fc1, 256)\n",
    "    fc3 = fully_conn(fc2, 128)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    logits = output(fc3, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络\n",
    "\n",
    "### 单次优化\n",
    "\n",
    "实现函数 `train_neural_network` 以进行单次优化（single optimization）。该优化应该使用 `optimizer` 优化 `session`，其中 `feed_dict` 具有以下参数：\n",
    "\n",
    "* `x` 表示图片输入\n",
    "* `y` 表示标签\n",
    "* `keep_prob` 表示丢弃的保留率\n",
    "\n",
    "每个部分都会调用该函数，所以 `tf.global_variables_initializer()` 已经被调用。\n",
    "\n",
    "注意：不需要返回任何内容。该函数只是用来优化神经网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    # TODO: Implement Function\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示数据\n",
    "\n",
    "实现函数 `print_stats` 以输出损失和验证准确率。使用全局变量 `valid_features` 和 `valid_labels` 计算验证准确率。使用保留率 `1.0` 计算损失和验证准确率（loss and validation accuracy）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = session.run(cost, feed_dict={x: feature_batch,\n",
    "                                        y: label_batch,\n",
    "                                        keep_prob: 1.0})\n",
    "    \n",
    "    accuracy = session.run(accuracy, feed_dict={ x: valid_features,\n",
    "                                                 y: valid_labels,\n",
    "                                                 keep_prob: 1.0})\n",
    "\n",
    "\n",
    "    print( \"Minibatch Loss= \" +\"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(accuracy ))\n",
    "\n",
    "    # TODO: Implement Function\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数\n",
    "\n",
    "调试以下超参数：\n",
    "* 设置 `epochs` 表示神经网络停止学习或开始过拟合的迭代次数\n",
    "* 设置 `batch_size`，表示机器内存允许的部分最大体积。大部分人设为以下常见内存大小：\n",
    "\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* 设置 `keep_probability` 表示使用丢弃时保留节点的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs =300\n",
    "batch_size = 128\n",
    "keep_probability =0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在单个 CIFAR-10 部分上训练\n",
    "\n",
    "我们先用单个部分，而不是用所有的 CIFAR-10 批次训练神经网络。这样可以节省时间，并对模型进行迭代，以提高准确率。最终验证准确率达到 50% 或以上之后，在下一部分对所有数据运行模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Minibatch Loss= 2.1680, Training Accuracy= 0.290\n",
      "Epoch  2, CIFAR-10 Batch 1:  Minibatch Loss= 2.1013, Training Accuracy= 0.360\n",
      "Epoch  3, CIFAR-10 Batch 1:  Minibatch Loss= 2.0453, Training Accuracy= 0.391\n",
      "Epoch  4, CIFAR-10 Batch 1:  Minibatch Loss= 1.9363, Training Accuracy= 0.423\n",
      "Epoch  5, CIFAR-10 Batch 1:  Minibatch Loss= 1.8470, Training Accuracy= 0.436\n",
      "Epoch  6, CIFAR-10 Batch 1:  Minibatch Loss= 1.7607, Training Accuracy= 0.451\n",
      "Epoch  7, CIFAR-10 Batch 1:  Minibatch Loss= 1.6903, Training Accuracy= 0.472\n",
      "Epoch  8, CIFAR-10 Batch 1:  Minibatch Loss= 1.6087, Training Accuracy= 0.484\n",
      "Epoch  9, CIFAR-10 Batch 1:  Minibatch Loss= 1.5530, Training Accuracy= 0.489\n",
      "Epoch 10, CIFAR-10 Batch 1:  Minibatch Loss= 1.4907, Training Accuracy= 0.501\n",
      "Epoch 11, CIFAR-10 Batch 1:  Minibatch Loss= 1.4214, Training Accuracy= 0.507\n",
      "Epoch 12, CIFAR-10 Batch 1:  Minibatch Loss= 1.3279, Training Accuracy= 0.517\n",
      "Epoch 13, CIFAR-10 Batch 1:  Minibatch Loss= 1.2774, Training Accuracy= 0.519\n",
      "Epoch 14, CIFAR-10 Batch 1:  Minibatch Loss= 1.2425, Training Accuracy= 0.530\n",
      "Epoch 15, CIFAR-10 Batch 1:  Minibatch Loss= 1.2059, Training Accuracy= 0.530\n",
      "Epoch 16, CIFAR-10 Batch 1:  Minibatch Loss= 1.1362, Training Accuracy= 0.535\n",
      "Epoch 17, CIFAR-10 Batch 1:  Minibatch Loss= 1.0561, Training Accuracy= 0.545\n",
      "Epoch 18, CIFAR-10 Batch 1:  Minibatch Loss= 1.0071, Training Accuracy= 0.544\n",
      "Epoch 19, CIFAR-10 Batch 1:  Minibatch Loss= 0.9671, Training Accuracy= 0.551\n",
      "Epoch 20, CIFAR-10 Batch 1:  Minibatch Loss= 0.9425, Training Accuracy= 0.555\n",
      "Epoch 21, CIFAR-10 Batch 1:  Minibatch Loss= 0.8923, Training Accuracy= 0.555\n",
      "Epoch 22, CIFAR-10 Batch 1:  Minibatch Loss= 0.8512, Training Accuracy= 0.559\n",
      "Epoch 23, CIFAR-10 Batch 1:  Minibatch Loss= 0.7950, Training Accuracy= 0.562\n",
      "Epoch 24, CIFAR-10 Batch 1:  Minibatch Loss= 0.7556, Training Accuracy= 0.569\n",
      "Epoch 25, CIFAR-10 Batch 1:  Minibatch Loss= 0.7095, Training Accuracy= 0.571\n",
      "Epoch 26, CIFAR-10 Batch 1:  Minibatch Loss= 0.6698, Training Accuracy= 0.577\n",
      "Epoch 27, CIFAR-10 Batch 1:  Minibatch Loss= 0.6106, Training Accuracy= 0.576\n",
      "Epoch 28, CIFAR-10 Batch 1:  Minibatch Loss= 0.5967, Training Accuracy= 0.577\n",
      "Epoch 29, CIFAR-10 Batch 1:  Minibatch Loss= 0.5586, Training Accuracy= 0.580\n",
      "Epoch 30, CIFAR-10 Batch 1:  Minibatch Loss= 0.5348, Training Accuracy= 0.585\n",
      "Epoch 31, CIFAR-10 Batch 1:  Minibatch Loss= 0.4843, Training Accuracy= 0.584\n",
      "Epoch 32, CIFAR-10 Batch 1:  Minibatch Loss= 0.4672, Training Accuracy= 0.586\n",
      "Epoch 33, CIFAR-10 Batch 1:  Minibatch Loss= 0.4335, Training Accuracy= 0.587\n",
      "Epoch 34, CIFAR-10 Batch 1:  Minibatch Loss= 0.4097, Training Accuracy= 0.589\n",
      "Epoch 35, CIFAR-10 Batch 1:  Minibatch Loss= 0.3696, Training Accuracy= 0.590\n",
      "Epoch 36, CIFAR-10 Batch 1:  Minibatch Loss= 0.3586, Training Accuracy= 0.599\n",
      "Epoch 37, CIFAR-10 Batch 1:  Minibatch Loss= 0.3284, Training Accuracy= 0.598\n",
      "Epoch 38, CIFAR-10 Batch 1:  Minibatch Loss= 0.2869, Training Accuracy= 0.599\n",
      "Epoch 39, CIFAR-10 Batch 1:  Minibatch Loss= 0.2846, Training Accuracy= 0.606\n",
      "Epoch 40, CIFAR-10 Batch 1:  Minibatch Loss= 0.2608, Training Accuracy= 0.605\n",
      "Epoch 41, CIFAR-10 Batch 1:  Minibatch Loss= 0.2512, Training Accuracy= 0.608\n",
      "Epoch 42, CIFAR-10 Batch 1:  Minibatch Loss= 0.2222, Training Accuracy= 0.607\n",
      "Epoch 43, CIFAR-10 Batch 1:  Minibatch Loss= 0.1901, Training Accuracy= 0.605\n",
      "Epoch 44, CIFAR-10 Batch 1:  Minibatch Loss= 0.1851, Training Accuracy= 0.610\n",
      "Epoch 45, CIFAR-10 Batch 1:  Minibatch Loss= 0.1716, Training Accuracy= 0.614\n",
      "Epoch 46, CIFAR-10 Batch 1:  Minibatch Loss= 0.1522, Training Accuracy= 0.607\n",
      "Epoch 47, CIFAR-10 Batch 1:  Minibatch Loss= 0.1496, Training Accuracy= 0.610\n",
      "Epoch 48, CIFAR-10 Batch 1:  Minibatch Loss= 0.1321, Training Accuracy= 0.614\n",
      "Epoch 49, CIFAR-10 Batch 1:  Minibatch Loss= 0.1274, Training Accuracy= 0.614\n",
      "Epoch 50, CIFAR-10 Batch 1:  Minibatch Loss= 0.1361, Training Accuracy= 0.610\n",
      "Epoch 51, CIFAR-10 Batch 1:  Minibatch Loss= 0.1199, Training Accuracy= 0.604\n",
      "Epoch 52, CIFAR-10 Batch 1:  Minibatch Loss= 0.1001, Training Accuracy= 0.612\n",
      "Epoch 53, CIFAR-10 Batch 1:  Minibatch Loss= 0.0893, Training Accuracy= 0.608\n",
      "Epoch 54, CIFAR-10 Batch 1:  Minibatch Loss= 0.0748, Training Accuracy= 0.605\n",
      "Epoch 55, CIFAR-10 Batch 1:  Minibatch Loss= 0.0676, Training Accuracy= 0.602\n",
      "Epoch 56, CIFAR-10 Batch 1:  Minibatch Loss= 0.0619, Training Accuracy= 0.609\n",
      "Epoch 57, CIFAR-10 Batch 1:  Minibatch Loss= 0.0551, Training Accuracy= 0.611\n",
      "Max_Acc = 61.440  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    accuracy_max = 0.0\n",
    "    max_ep = 15\n",
    "    i = 0\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        current_acc = print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "        if(accuracy_max <= current_acc):\n",
    "            accuracy_max = current_acc\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1\n",
    "            if(i==max_ep):\n",
    "                break     \n",
    "    print('Max_Acc ='+' {:.3f}  '.format(accuracy_max*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全训练模型\n",
    "\n",
    "现在，单个 CIFAR-10 部分的准确率已经不错了，试试所有五个部分吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Minibatch Loss= 2.2224, Training Accuracy= 0.222\n",
      "Epoch  1, CIFAR-10 Batch 2:  Minibatch Loss= 1.9716, Training Accuracy= 0.323\n",
      "Epoch  1, CIFAR-10 Batch 3:  Minibatch Loss= 1.6804, Training Accuracy= 0.351\n",
      "Epoch  1, CIFAR-10 Batch 4:  Minibatch Loss= 1.6512, Training Accuracy= 0.394\n",
      "Epoch  1, CIFAR-10 Batch 5:  Minibatch Loss= 1.6249, Training Accuracy= 0.423\n",
      "Epoch  2, CIFAR-10 Batch 1:  Minibatch Loss= 1.9171, Training Accuracy= 0.439\n",
      "Epoch  2, CIFAR-10 Batch 2:  Minibatch Loss= 1.5884, Training Accuracy= 0.451\n",
      "Epoch  2, CIFAR-10 Batch 3:  Minibatch Loss= 1.2886, Training Accuracy= 0.456\n",
      "Epoch  2, CIFAR-10 Batch 4:  Minibatch Loss= 1.4850, Training Accuracy= 0.477\n",
      "Epoch  2, CIFAR-10 Batch 5:  Minibatch Loss= 1.3735, Training Accuracy= 0.489\n",
      "Epoch  3, CIFAR-10 Batch 1:  Minibatch Loss= 1.7123, Training Accuracy= 0.503\n",
      "Epoch  3, CIFAR-10 Batch 2:  Minibatch Loss= 1.3925, Training Accuracy= 0.503\n",
      "Epoch  3, CIFAR-10 Batch 3:  Minibatch Loss= 1.1235, Training Accuracy= 0.507\n",
      "Epoch  3, CIFAR-10 Batch 4:  Minibatch Loss= 1.3505, Training Accuracy= 0.520\n",
      "Epoch  3, CIFAR-10 Batch 5:  Minibatch Loss= 1.2465, Training Accuracy= 0.526\n",
      "Epoch  4, CIFAR-10 Batch 1:  Minibatch Loss= 1.5317, Training Accuracy= 0.534\n",
      "Epoch  4, CIFAR-10 Batch 2:  Minibatch Loss= 1.2370, Training Accuracy= 0.540\n",
      "Epoch  4, CIFAR-10 Batch 3:  Minibatch Loss= 0.9960, Training Accuracy= 0.531\n",
      "Epoch  4, CIFAR-10 Batch 4:  Minibatch Loss= 1.2237, Training Accuracy= 0.551\n",
      "Epoch  4, CIFAR-10 Batch 5:  Minibatch Loss= 1.1778, Training Accuracy= 0.555\n",
      "Epoch  5, CIFAR-10 Batch 1:  Minibatch Loss= 1.3897, Training Accuracy= 0.565\n",
      "Epoch  5, CIFAR-10 Batch 2:  Minibatch Loss= 1.1406, Training Accuracy= 0.568\n",
      "Epoch  5, CIFAR-10 Batch 3:  Minibatch Loss= 0.8915, Training Accuracy= 0.562\n",
      "Epoch  5, CIFAR-10 Batch 4:  Minibatch Loss= 1.1138, Training Accuracy= 0.575\n",
      "Epoch  5, CIFAR-10 Batch 5:  Minibatch Loss= 1.0501, Training Accuracy= 0.578\n",
      "Epoch  6, CIFAR-10 Batch 1:  Minibatch Loss= 1.3124, Training Accuracy= 0.579\n",
      "Epoch  6, CIFAR-10 Batch 2:  Minibatch Loss= 1.0378, Training Accuracy= 0.583\n",
      "Epoch  6, CIFAR-10 Batch 3:  Minibatch Loss= 0.8231, Training Accuracy= 0.580\n",
      "Epoch  6, CIFAR-10 Batch 4:  Minibatch Loss= 1.0203, Training Accuracy= 0.587\n",
      "Epoch  6, CIFAR-10 Batch 5:  Minibatch Loss= 0.9771, Training Accuracy= 0.595\n",
      "Epoch  7, CIFAR-10 Batch 1:  Minibatch Loss= 1.2285, Training Accuracy= 0.595\n",
      "Epoch  7, CIFAR-10 Batch 2:  Minibatch Loss= 0.9741, Training Accuracy= 0.598\n",
      "Epoch  7, CIFAR-10 Batch 3:  Minibatch Loss= 0.7594, Training Accuracy= 0.593\n",
      "Epoch  7, CIFAR-10 Batch 4:  Minibatch Loss= 0.9327, Training Accuracy= 0.610\n",
      "Epoch  7, CIFAR-10 Batch 5:  Minibatch Loss= 0.8926, Training Accuracy= 0.609\n",
      "Epoch  8, CIFAR-10 Batch 1:  Minibatch Loss= 1.1293, Training Accuracy= 0.615\n",
      "Epoch  8, CIFAR-10 Batch 2:  Minibatch Loss= 0.9030, Training Accuracy= 0.615\n",
      "Epoch  8, CIFAR-10 Batch 3:  Minibatch Loss= 0.7031, Training Accuracy= 0.615\n",
      "Epoch  8, CIFAR-10 Batch 4:  Minibatch Loss= 0.8531, Training Accuracy= 0.619\n",
      "Epoch  8, CIFAR-10 Batch 5:  Minibatch Loss= 0.7919, Training Accuracy= 0.617\n",
      "Epoch  9, CIFAR-10 Batch 1:  Minibatch Loss= 1.0701, Training Accuracy= 0.622\n",
      "Epoch  9, CIFAR-10 Batch 2:  Minibatch Loss= 0.8446, Training Accuracy= 0.622\n",
      "Epoch  9, CIFAR-10 Batch 3:  Minibatch Loss= 0.6185, Training Accuracy= 0.620\n",
      "Epoch  9, CIFAR-10 Batch 4:  Minibatch Loss= 0.7894, Training Accuracy= 0.626\n",
      "Epoch  9, CIFAR-10 Batch 5:  Minibatch Loss= 0.7579, Training Accuracy= 0.628\n",
      "Epoch 10, CIFAR-10 Batch 1:  Minibatch Loss= 1.0167, Training Accuracy= 0.638\n",
      "Epoch 10, CIFAR-10 Batch 2:  Minibatch Loss= 0.7524, Training Accuracy= 0.636\n",
      "Epoch 10, CIFAR-10 Batch 3:  Minibatch Loss= 0.5973, Training Accuracy= 0.638\n",
      "Epoch 10, CIFAR-10 Batch 4:  Minibatch Loss= 0.7130, Training Accuracy= 0.641\n",
      "Epoch 10, CIFAR-10 Batch 5:  Minibatch Loss= 0.6827, Training Accuracy= 0.634\n",
      "Epoch 11, CIFAR-10 Batch 1:  Minibatch Loss= 0.9143, Training Accuracy= 0.646\n",
      "Epoch 11, CIFAR-10 Batch 2:  Minibatch Loss= 0.6795, Training Accuracy= 0.644\n",
      "Epoch 11, CIFAR-10 Batch 3:  Minibatch Loss= 0.5468, Training Accuracy= 0.642\n",
      "Epoch 11, CIFAR-10 Batch 4:  Minibatch Loss= 0.6588, Training Accuracy= 0.653\n",
      "Epoch 11, CIFAR-10 Batch 5:  Minibatch Loss= 0.6197, Training Accuracy= 0.647\n",
      "Epoch 12, CIFAR-10 Batch 1:  Minibatch Loss= 0.8561, Training Accuracy= 0.656\n",
      "Epoch 12, CIFAR-10 Batch 2:  Minibatch Loss= 0.6434, Training Accuracy= 0.650\n",
      "Epoch 12, CIFAR-10 Batch 3:  Minibatch Loss= 0.4975, Training Accuracy= 0.649\n",
      "Epoch 12, CIFAR-10 Batch 4:  Minibatch Loss= 0.6221, Training Accuracy= 0.653\n",
      "Epoch 12, CIFAR-10 Batch 5:  Minibatch Loss= 0.5554, Training Accuracy= 0.657\n",
      "Epoch 13, CIFAR-10 Batch 1:  Minibatch Loss= 0.8130, Training Accuracy= 0.661\n",
      "Epoch 13, CIFAR-10 Batch 2:  Minibatch Loss= 0.5986, Training Accuracy= 0.661\n",
      "Epoch 13, CIFAR-10 Batch 3:  Minibatch Loss= 0.4701, Training Accuracy= 0.662\n",
      "Epoch 13, CIFAR-10 Batch 4:  Minibatch Loss= 0.5407, Training Accuracy= 0.665\n",
      "Epoch 13, CIFAR-10 Batch 5:  Minibatch Loss= 0.5242, Training Accuracy= 0.664\n",
      "Epoch 14, CIFAR-10 Batch 1:  Minibatch Loss= 0.7522, Training Accuracy= 0.666\n",
      "Epoch 14, CIFAR-10 Batch 2:  Minibatch Loss= 0.5561, Training Accuracy= 0.669\n",
      "Epoch 14, CIFAR-10 Batch 3:  Minibatch Loss= 0.4212, Training Accuracy= 0.664\n",
      "Epoch 14, CIFAR-10 Batch 4:  Minibatch Loss= 0.4890, Training Accuracy= 0.669\n",
      "Epoch 14, CIFAR-10 Batch 5:  Minibatch Loss= 0.4911, Training Accuracy= 0.669\n",
      "Epoch 15, CIFAR-10 Batch 1:  Minibatch Loss= 0.6843, Training Accuracy= 0.675\n",
      "Epoch 15, CIFAR-10 Batch 2:  Minibatch Loss= 0.4998, Training Accuracy= 0.670\n",
      "Epoch 15, CIFAR-10 Batch 3:  Minibatch Loss= 0.3863, Training Accuracy= 0.672\n",
      "Epoch 15, CIFAR-10 Batch 4:  Minibatch Loss= 0.4367, Training Accuracy= 0.674\n",
      "Epoch 15, CIFAR-10 Batch 5:  Minibatch Loss= 0.4309, Training Accuracy= 0.680\n",
      "Epoch 16, CIFAR-10 Batch 1:  Minibatch Loss= 0.6221, Training Accuracy= 0.676\n",
      "Epoch 16, CIFAR-10 Batch 2:  Minibatch Loss= 0.4562, Training Accuracy= 0.678\n",
      "Epoch 16, CIFAR-10 Batch 3:  Minibatch Loss= 0.3500, Training Accuracy= 0.679\n",
      "Epoch 16, CIFAR-10 Batch 4:  Minibatch Loss= 0.4246, Training Accuracy= 0.675\n",
      "Epoch 16, CIFAR-10 Batch 5:  Minibatch Loss= 0.4071, Training Accuracy= 0.683\n",
      "Epoch 17, CIFAR-10 Batch 1:  Minibatch Loss= 0.6192, Training Accuracy= 0.685\n",
      "Epoch 17, CIFAR-10 Batch 2:  Minibatch Loss= 0.4343, Training Accuracy= 0.683\n",
      "Epoch 17, CIFAR-10 Batch 3:  Minibatch Loss= 0.3344, Training Accuracy= 0.678\n",
      "Epoch 17, CIFAR-10 Batch 4:  Minibatch Loss= 0.3830, Training Accuracy= 0.681\n",
      "Epoch 17, CIFAR-10 Batch 5:  Minibatch Loss= 0.3595, Training Accuracy= 0.686\n",
      "Epoch 18, CIFAR-10 Batch 1:  Minibatch Loss= 0.5756, Training Accuracy= 0.687\n",
      "Epoch 18, CIFAR-10 Batch 2:  Minibatch Loss= 0.3944, Training Accuracy= 0.689\n",
      "Epoch 18, CIFAR-10 Batch 3:  Minibatch Loss= 0.2958, Training Accuracy= 0.685\n",
      "Epoch 18, CIFAR-10 Batch 4:  Minibatch Loss= 0.3319, Training Accuracy= 0.687\n",
      "Epoch 18, CIFAR-10 Batch 5:  Minibatch Loss= 0.3316, Training Accuracy= 0.687\n",
      "Epoch 19, CIFAR-10 Batch 1:  Minibatch Loss= 0.5174, Training Accuracy= 0.693\n",
      "Epoch 19, CIFAR-10 Batch 2:  Minibatch Loss= 0.3531, Training Accuracy= 0.688\n",
      "Epoch 19, CIFAR-10 Batch 3:  Minibatch Loss= 0.2894, Training Accuracy= 0.692\n",
      "Epoch 19, CIFAR-10 Batch 4:  Minibatch Loss= 0.2938, Training Accuracy= 0.690\n",
      "Epoch 19, CIFAR-10 Batch 5:  Minibatch Loss= 0.2789, Training Accuracy= 0.695\n",
      "Epoch 20, CIFAR-10 Batch 1:  Minibatch Loss= 0.4661, Training Accuracy= 0.693\n",
      "Epoch 20, CIFAR-10 Batch 2:  Minibatch Loss= 0.3265, Training Accuracy= 0.696\n",
      "Epoch 20, CIFAR-10 Batch 3:  Minibatch Loss= 0.2611, Training Accuracy= 0.693\n",
      "Epoch 20, CIFAR-10 Batch 4:  Minibatch Loss= 0.2972, Training Accuracy= 0.692\n",
      "Epoch 20, CIFAR-10 Batch 5:  Minibatch Loss= 0.2585, Training Accuracy= 0.695\n",
      "Epoch 21, CIFAR-10 Batch 1:  Minibatch Loss= 0.4397, Training Accuracy= 0.695\n",
      "Epoch 21, CIFAR-10 Batch 2:  Minibatch Loss= 0.2866, Training Accuracy= 0.698\n",
      "Epoch 21, CIFAR-10 Batch 3:  Minibatch Loss= 0.2372, Training Accuracy= 0.701\n",
      "Epoch 21, CIFAR-10 Batch 4:  Minibatch Loss= 0.2754, Training Accuracy= 0.691\n",
      "Epoch 21, CIFAR-10 Batch 5:  Minibatch Loss= 0.2515, Training Accuracy= 0.697\n",
      "Epoch 22, CIFAR-10 Batch 1:  Minibatch Loss= 0.3898, Training Accuracy= 0.701\n",
      "Epoch 22, CIFAR-10 Batch 2:  Minibatch Loss= 0.2533, Training Accuracy= 0.698\n",
      "Epoch 22, CIFAR-10 Batch 3:  Minibatch Loss= 0.2268, Training Accuracy= 0.696\n",
      "Epoch 22, CIFAR-10 Batch 4:  Minibatch Loss= 0.2546, Training Accuracy= 0.693\n",
      "Epoch 22, CIFAR-10 Batch 5:  Minibatch Loss= 0.2122, Training Accuracy= 0.700\n",
      "Epoch 23, CIFAR-10 Batch 1:  Minibatch Loss= 0.3671, Training Accuracy= 0.700\n",
      "Epoch 23, CIFAR-10 Batch 2:  Minibatch Loss= 0.2279, Training Accuracy= 0.703\n",
      "Epoch 23, CIFAR-10 Batch 3:  Minibatch Loss= 0.2042, Training Accuracy= 0.708\n",
      "Epoch 23, CIFAR-10 Batch 4:  Minibatch Loss= 0.2315, Training Accuracy= 0.696\n",
      "Epoch 23, CIFAR-10 Batch 5:  Minibatch Loss= 0.1957, Training Accuracy= 0.703\n",
      "Epoch 24, CIFAR-10 Batch 1:  Minibatch Loss= 0.3389, Training Accuracy= 0.706\n",
      "Epoch 24, CIFAR-10 Batch 2:  Minibatch Loss= 0.2027, Training Accuracy= 0.706\n",
      "Epoch 24, CIFAR-10 Batch 3:  Minibatch Loss= 0.1932, Training Accuracy= 0.702\n",
      "Epoch 24, CIFAR-10 Batch 4:  Minibatch Loss= 0.2134, Training Accuracy= 0.696\n",
      "Epoch 24, CIFAR-10 Batch 5:  Minibatch Loss= 0.1777, Training Accuracy= 0.707\n",
      "Epoch 25, CIFAR-10 Batch 1:  Minibatch Loss= 0.2925, Training Accuracy= 0.708\n",
      "Epoch 25, CIFAR-10 Batch 2:  Minibatch Loss= 0.1856, Training Accuracy= 0.705\n",
      "Epoch 25, CIFAR-10 Batch 3:  Minibatch Loss= 0.1537, Training Accuracy= 0.707\n",
      "Epoch 25, CIFAR-10 Batch 4:  Minibatch Loss= 0.1763, Training Accuracy= 0.708\n",
      "Epoch 25, CIFAR-10 Batch 5:  Minibatch Loss= 0.1583, Training Accuracy= 0.707\n",
      "Epoch 26, CIFAR-10 Batch 1:  Minibatch Loss= 0.2703, Training Accuracy= 0.706\n",
      "Epoch 26, CIFAR-10 Batch 2:  Minibatch Loss= 0.1594, Training Accuracy= 0.708\n",
      "Epoch 26, CIFAR-10 Batch 3:  Minibatch Loss= 0.1296, Training Accuracy= 0.712\n",
      "Epoch 26, CIFAR-10 Batch 4:  Minibatch Loss= 0.1584, Training Accuracy= 0.700\n",
      "Epoch 26, CIFAR-10 Batch 5:  Minibatch Loss= 0.1354, Training Accuracy= 0.706\n",
      "Epoch 27, CIFAR-10 Batch 1:  Minibatch Loss= 0.2445, Training Accuracy= 0.712\n",
      "Epoch 27, CIFAR-10 Batch 2:  Minibatch Loss= 0.1451, Training Accuracy= 0.712\n",
      "Epoch 27, CIFAR-10 Batch 3:  Minibatch Loss= 0.1346, Training Accuracy= 0.711\n",
      "Epoch 27, CIFAR-10 Batch 4:  Minibatch Loss= 0.1394, Training Accuracy= 0.705\n",
      "Epoch 27, CIFAR-10 Batch 5:  Minibatch Loss= 0.1230, Training Accuracy= 0.711\n",
      "Epoch 28, CIFAR-10 Batch 1:  Minibatch Loss= 0.2137, Training Accuracy= 0.711\n",
      "Epoch 28, CIFAR-10 Batch 2:  Minibatch Loss= 0.1417, Training Accuracy= 0.712\n",
      "Epoch 28, CIFAR-10 Batch 3:  Minibatch Loss= 0.1315, Training Accuracy= 0.712\n",
      "Epoch 28, CIFAR-10 Batch 4:  Minibatch Loss= 0.1418, Training Accuracy= 0.708\n",
      "Epoch 28, CIFAR-10 Batch 5:  Minibatch Loss= 0.1040, Training Accuracy= 0.712\n",
      "Epoch 29, CIFAR-10 Batch 1:  Minibatch Loss= 0.1879, Training Accuracy= 0.718\n",
      "Epoch 29, CIFAR-10 Batch 2:  Minibatch Loss= 0.1379, Training Accuracy= 0.710\n",
      "Epoch 29, CIFAR-10 Batch 3:  Minibatch Loss= 0.1195, Training Accuracy= 0.709\n",
      "Epoch 29, CIFAR-10 Batch 4:  Minibatch Loss= 0.1207, Training Accuracy= 0.708\n",
      "Epoch 29, CIFAR-10 Batch 5:  Minibatch Loss= 0.0999, Training Accuracy= 0.706\n",
      "Epoch 30, CIFAR-10 Batch 1:  Minibatch Loss= 0.1614, Training Accuracy= 0.718\n",
      "Epoch 30, CIFAR-10 Batch 2:  Minibatch Loss= 0.1270, Training Accuracy= 0.711\n",
      "Epoch 30, CIFAR-10 Batch 3:  Minibatch Loss= 0.0977, Training Accuracy= 0.717\n",
      "Epoch 30, CIFAR-10 Batch 4:  Minibatch Loss= 0.1079, Training Accuracy= 0.710\n",
      "Epoch 30, CIFAR-10 Batch 5:  Minibatch Loss= 0.0780, Training Accuracy= 0.712\n",
      "Epoch 31, CIFAR-10 Batch 1:  Minibatch Loss= 0.1509, Training Accuracy= 0.713\n",
      "Epoch 31, CIFAR-10 Batch 2:  Minibatch Loss= 0.1043, Training Accuracy= 0.714\n",
      "Epoch 31, CIFAR-10 Batch 3:  Minibatch Loss= 0.0960, Training Accuracy= 0.708\n",
      "Epoch 31, CIFAR-10 Batch 4:  Minibatch Loss= 0.1015, Training Accuracy= 0.709\n",
      "Epoch 31, CIFAR-10 Batch 5:  Minibatch Loss= 0.0595, Training Accuracy= 0.712\n",
      "Epoch 32, CIFAR-10 Batch 1:  Minibatch Loss= 0.1182, Training Accuracy= 0.717\n",
      "Epoch 32, CIFAR-10 Batch 2:  Minibatch Loss= 0.0968, Training Accuracy= 0.719\n",
      "Epoch 32, CIFAR-10 Batch 3:  Minibatch Loss= 0.0846, Training Accuracy= 0.717\n",
      "Epoch 32, CIFAR-10 Batch 4:  Minibatch Loss= 0.0964, Training Accuracy= 0.709\n",
      "Epoch 32, CIFAR-10 Batch 5:  Minibatch Loss= 0.0570, Training Accuracy= 0.718\n",
      "Epoch 33, CIFAR-10 Batch 1:  Minibatch Loss= 0.1274, Training Accuracy= 0.717\n",
      "Epoch 33, CIFAR-10 Batch 2:  Minibatch Loss= 0.0892, Training Accuracy= 0.716\n",
      "Epoch 33, CIFAR-10 Batch 3:  Minibatch Loss= 0.0766, Training Accuracy= 0.718\n",
      "Epoch 33, CIFAR-10 Batch 4:  Minibatch Loss= 0.0878, Training Accuracy= 0.714\n",
      "Epoch 33, CIFAR-10 Batch 5:  Minibatch Loss= 0.0639, Training Accuracy= 0.714\n",
      "Epoch 34, CIFAR-10 Batch 1:  Minibatch Loss= 0.1067, Training Accuracy= 0.718\n",
      "Epoch 34, CIFAR-10 Batch 2:  Minibatch Loss= 0.0824, Training Accuracy= 0.711\n",
      "Epoch 34, CIFAR-10 Batch 3:  Minibatch Loss= 0.0690, Training Accuracy= 0.706\n",
      "Epoch 34, CIFAR-10 Batch 4:  Minibatch Loss= 0.0569, Training Accuracy= 0.709\n",
      "Epoch 34, CIFAR-10 Batch 5:  Minibatch Loss= 0.0568, Training Accuracy= 0.715\n",
      "Epoch 35, CIFAR-10 Batch 1:  Minibatch Loss= 0.0841, Training Accuracy= 0.714\n",
      "Epoch 35, CIFAR-10 Batch 2:  Minibatch Loss= 0.0738, Training Accuracy= 0.717\n",
      "Epoch 35, CIFAR-10 Batch 3:  Minibatch Loss= 0.0766, Training Accuracy= 0.708\n",
      "Epoch 35, CIFAR-10 Batch 4:  Minibatch Loss= 0.0586, Training Accuracy= 0.714\n",
      "Epoch 35, CIFAR-10 Batch 5:  Minibatch Loss= 0.0464, Training Accuracy= 0.715\n",
      "Epoch 36, CIFAR-10 Batch 1:  Minibatch Loss= 0.0935, Training Accuracy= 0.716\n",
      "Epoch 36, CIFAR-10 Batch 2:  Minibatch Loss= 0.0529, Training Accuracy= 0.710\n",
      "Epoch 37, CIFAR-10 Batch 1:  Minibatch Loss= 0.0821, Training Accuracy= 0.711\n",
      "Epoch 37, CIFAR-10 Batch 2:  Minibatch Loss= 0.0475, Training Accuracy= 0.714\n",
      "Epoch 37, CIFAR-10 Batch 3:  Minibatch Loss= 0.0665, Training Accuracy= 0.707\n",
      "Epoch 37, CIFAR-10 Batch 4:  Minibatch Loss= 0.0577, Training Accuracy= 0.713\n",
      "Epoch 37, CIFAR-10 Batch 5:  Minibatch Loss= 0.0485, Training Accuracy= 0.717\n",
      "Epoch 38, CIFAR-10 Batch 1:  Minibatch Loss= 0.0735, Training Accuracy= 0.716\n",
      "Epoch 38, CIFAR-10 Batch 2:  Minibatch Loss= 0.0445, Training Accuracy= 0.707\n",
      "Epoch 38, CIFAR-10 Batch 3:  Minibatch Loss= 0.0423, Training Accuracy= 0.709\n",
      "Epoch 38, CIFAR-10 Batch 4:  Minibatch Loss= 0.0525, Training Accuracy= 0.722\n",
      "Epoch 38, CIFAR-10 Batch 5:  Minibatch Loss= 0.0328, Training Accuracy= 0.712\n",
      "Epoch 39, CIFAR-10 Batch 1:  Minibatch Loss= 0.0668, Training Accuracy= 0.718\n",
      "Epoch 39, CIFAR-10 Batch 2:  Minibatch Loss= 0.0441, Training Accuracy= 0.709\n",
      "Epoch 39, CIFAR-10 Batch 3:  Minibatch Loss= 0.0594, Training Accuracy= 0.703\n",
      "Epoch 39, CIFAR-10 Batch 4:  Minibatch Loss= 0.0428, Training Accuracy= 0.716\n",
      "Epoch 39, CIFAR-10 Batch 5:  Minibatch Loss= 0.0340, Training Accuracy= 0.714\n",
      "Epoch 40, CIFAR-10 Batch 1:  Minibatch Loss= 0.0653, Training Accuracy= 0.719\n",
      "Epoch 40, CIFAR-10 Batch 2:  Minibatch Loss= 0.0399, Training Accuracy= 0.719\n",
      "Epoch 40, CIFAR-10 Batch 3:  Minibatch Loss= 0.0503, Training Accuracy= 0.700\n",
      "Epoch 40, CIFAR-10 Batch 4:  Minibatch Loss= 0.0317, Training Accuracy= 0.718\n",
      "Epoch 40, CIFAR-10 Batch 5:  Minibatch Loss= 0.0274, Training Accuracy= 0.714\n",
      "Epoch 41, CIFAR-10 Batch 1:  Minibatch Loss= 0.0558, Training Accuracy= 0.720\n",
      "Epoch 41, CIFAR-10 Batch 2:  Minibatch Loss= 0.0320, Training Accuracy= 0.705\n",
      "Epoch 41, CIFAR-10 Batch 3:  Minibatch Loss= 0.0310, Training Accuracy= 0.706\n",
      "Epoch 41, CIFAR-10 Batch 4:  Minibatch Loss= 0.0279, Training Accuracy= 0.721\n",
      "Epoch 41, CIFAR-10 Batch 5:  Minibatch Loss= 0.0294, Training Accuracy= 0.706\n",
      "Epoch 42, CIFAR-10 Batch 1:  Minibatch Loss= 0.0369, Training Accuracy= 0.723\n",
      "Epoch 42, CIFAR-10 Batch 2:  Minibatch Loss= 0.0368, Training Accuracy= 0.714\n",
      "Epoch 42, CIFAR-10 Batch 3:  Minibatch Loss= 0.0288, Training Accuracy= 0.705\n",
      "Epoch 42, CIFAR-10 Batch 4:  Minibatch Loss= 0.0293, Training Accuracy= 0.719\n",
      "Epoch 42, CIFAR-10 Batch 5:  Minibatch Loss= 0.0256, Training Accuracy= 0.716\n",
      "Epoch 43, CIFAR-10 Batch 1:  Minibatch Loss= 0.0388, Training Accuracy= 0.717\n",
      "Epoch 43, CIFAR-10 Batch 2:  Minibatch Loss= 0.0273, Training Accuracy= 0.714\n",
      "Epoch 43, CIFAR-10 Batch 3:  Minibatch Loss= 0.0219, Training Accuracy= 0.707\n",
      "Epoch 43, CIFAR-10 Batch 4:  Minibatch Loss= 0.0253, Training Accuracy= 0.720\n",
      "Epoch 43, CIFAR-10 Batch 5:  Minibatch Loss= 0.0221, Training Accuracy= 0.713\n",
      "Epoch 44, CIFAR-10 Batch 1:  Minibatch Loss= 0.0345, Training Accuracy= 0.716\n",
      "Epoch 44, CIFAR-10 Batch 2:  Minibatch Loss= 0.0278, Training Accuracy= 0.714\n",
      "Epoch 44, CIFAR-10 Batch 3:  Minibatch Loss= 0.0221, Training Accuracy= 0.715\n",
      "Epoch 44, CIFAR-10 Batch 4:  Minibatch Loss= 0.0245, Training Accuracy= 0.715\n",
      "Epoch 44, CIFAR-10 Batch 5:  Minibatch Loss= 0.0226, Training Accuracy= 0.714\n",
      "Epoch 45, CIFAR-10 Batch 1:  Minibatch Loss= 0.0277, Training Accuracy= 0.717\n",
      "Epoch 45, CIFAR-10 Batch 2:  Minibatch Loss= 0.0231, Training Accuracy= 0.714\n",
      "Epoch 45, CIFAR-10 Batch 3:  Minibatch Loss= 0.0209, Training Accuracy= 0.703\n",
      "Epoch 45, CIFAR-10 Batch 4:  Minibatch Loss= 0.0206, Training Accuracy= 0.717\n",
      "Epoch 45, CIFAR-10 Batch 5:  Minibatch Loss= 0.0198, Training Accuracy= 0.713\n",
      "Epoch 46, CIFAR-10 Batch 1:  Minibatch Loss= 0.0223, Training Accuracy= 0.722\n",
      "Epoch 47, CIFAR-10 Batch 1:  Minibatch Loss= 0.0157, Training Accuracy= 0.716\n",
      "Epoch 47, CIFAR-10 Batch 2:  Minibatch Loss= 0.0228, Training Accuracy= 0.717\n",
      "Epoch 47, CIFAR-10 Batch 3:  Minibatch Loss= 0.0179, Training Accuracy= 0.718\n",
      "Epoch 47, CIFAR-10 Batch 4:  Minibatch Loss= 0.0191, Training Accuracy= 0.700\n",
      "Epoch 47, CIFAR-10 Batch 5:  Minibatch Loss= 0.0149, Training Accuracy= 0.714\n",
      "Epoch 48, CIFAR-10 Batch 1:  Minibatch Loss= 0.0222, Training Accuracy= 0.715\n",
      "Epoch 48, CIFAR-10 Batch 2:  Minibatch Loss= 0.0181, Training Accuracy= 0.720\n",
      "Epoch 48, CIFAR-10 Batch 3:  Minibatch Loss= 0.0160, Training Accuracy= 0.709\n",
      "Epoch 48, CIFAR-10 Batch 4:  Minibatch Loss= 0.0212, Training Accuracy= 0.719\n",
      "Epoch 48, CIFAR-10 Batch 5:  Minibatch Loss= 0.0137, Training Accuracy= 0.713\n",
      "Epoch 49, CIFAR-10 Batch 1:  Minibatch Loss= 0.0245, Training Accuracy= 0.715\n",
      "Epoch 49, CIFAR-10 Batch 2:  Minibatch Loss= 0.0161, Training Accuracy= 0.718\n",
      "Epoch 49, CIFAR-10 Batch 3:  Minibatch Loss= 0.0139, Training Accuracy= 0.717\n",
      "Epoch 49, CIFAR-10 Batch 4:  Minibatch Loss= 0.0183, Training Accuracy= 0.716\n",
      "Epoch 49, CIFAR-10 Batch 5:  Minibatch Loss= 0.0135, Training Accuracy= 0.713\n",
      "Epoch 50, CIFAR-10 Batch 1:  Minibatch Loss= 0.0123, Training Accuracy= 0.719\n",
      "Epoch 50, CIFAR-10 Batch 2:  Minibatch Loss= 0.0135, Training Accuracy= 0.718\n",
      "Epoch 50, CIFAR-10 Batch 3:  Minibatch Loss= 0.0107, Training Accuracy= 0.723\n",
      "Epoch 50, CIFAR-10 Batch 4:  Minibatch Loss= 0.0136, Training Accuracy= 0.713\n",
      "Epoch 50, CIFAR-10 Batch 5:  Minibatch Loss= 0.0095, Training Accuracy= 0.720\n",
      "Epoch 51, CIFAR-10 Batch 1:  Minibatch Loss= 0.0142, Training Accuracy= 0.717\n",
      "Epoch 51, CIFAR-10 Batch 2:  Minibatch Loss= 0.0128, Training Accuracy= 0.716\n",
      "Epoch 51, CIFAR-10 Batch 3:  Minibatch Loss= 0.0122, Training Accuracy= 0.717\n",
      "Epoch 51, CIFAR-10 Batch 4:  Minibatch Loss= 0.0112, Training Accuracy= 0.722\n",
      "Epoch 51, CIFAR-10 Batch 5:  Minibatch Loss= 0.0109, Training Accuracy= 0.715\n",
      "Epoch 52, CIFAR-10 Batch 1:  Minibatch Loss= 0.0099, Training Accuracy= 0.721\n",
      "Epoch 52, CIFAR-10 Batch 2:  Minibatch Loss= 0.0166, Training Accuracy= 0.710\n",
      "Epoch 52, CIFAR-10 Batch 3:  Minibatch Loss= 0.0112, Training Accuracy= 0.721\n",
      "Epoch 52, CIFAR-10 Batch 4:  Minibatch Loss= 0.0114, Training Accuracy= 0.719\n",
      "Epoch 52, CIFAR-10 Batch 5:  Minibatch Loss= 0.0081, Training Accuracy= 0.718\n",
      "Epoch 53, CIFAR-10 Batch 1:  Minibatch Loss= 0.0119, Training Accuracy= 0.721\n",
      "Epoch 53, CIFAR-10 Batch 2:  Minibatch Loss= 0.0103, Training Accuracy= 0.716\n",
      "Epoch 53, CIFAR-10 Batch 3:  Minibatch Loss= 0.0083, Training Accuracy= 0.721\n",
      "Epoch 53, CIFAR-10 Batch 4:  Minibatch Loss= 0.0116, Training Accuracy= 0.721\n",
      "Epoch 53, CIFAR-10 Batch 5:  Minibatch Loss= 0.0078, Training Accuracy= 0.716\n",
      "Epoch 54, CIFAR-10 Batch 1:  Minibatch Loss= 0.0162, Training Accuracy= 0.706\n",
      "Epoch 54, CIFAR-10 Batch 2:  Minibatch Loss= 0.0114, Training Accuracy= 0.711\n",
      "Epoch 54, CIFAR-10 Batch 3:  Minibatch Loss= 0.0121, Training Accuracy= 0.715\n",
      "Epoch 54, CIFAR-10 Batch 4:  Minibatch Loss= 0.0067, Training Accuracy= 0.721\n",
      "Epoch 54, CIFAR-10 Batch 5:  Minibatch Loss= 0.0069, Training Accuracy= 0.721\n",
      "Epoch 55, CIFAR-10 Batch 1:  Minibatch Loss= 0.0087, Training Accuracy= 0.714\n",
      "Epoch 55, CIFAR-10 Batch 2:  Minibatch Loss= 0.0088, Training Accuracy= 0.711\n",
      "Epoch 55, CIFAR-10 Batch 3:  Minibatch Loss= 0.0127, Training Accuracy= 0.721\n",
      "Epoch 55, CIFAR-10 Batch 4:  Minibatch Loss= 0.0101, Training Accuracy= 0.718\n",
      "Epoch 55, CIFAR-10 Batch 5:  Minibatch Loss= 0.0065, Training Accuracy= 0.714\n",
      "Epoch 56, CIFAR-10 Batch 1:  Minibatch Loss= 0.0094, Training Accuracy= 0.710\n",
      "Epoch 56, CIFAR-10 Batch 2:  Minibatch Loss= 0.0113, Training Accuracy= 0.714\n",
      "Epoch 56, CIFAR-10 Batch 3:  Minibatch Loss= 0.0096, Training Accuracy= 0.719\n",
      "Epoch 56, CIFAR-10 Batch 4:  Minibatch Loss= 0.0120, Training Accuracy= 0.714\n",
      "Epoch 56, CIFAR-10 Batch 5:  Minibatch Loss= 0.0053, Training Accuracy= 0.712\n",
      "Epoch 57, CIFAR-10 Batch 1:  Minibatch Loss= 0.0060, Training Accuracy= 0.714\n",
      "Epoch 57, CIFAR-10 Batch 2:  Minibatch Loss= 0.0073, Training Accuracy= 0.715\n",
      "Epoch 57, CIFAR-10 Batch 3:  Minibatch Loss= 0.0065, Training Accuracy= 0.719\n",
      "Epoch 57, CIFAR-10 Batch 4:  Minibatch Loss= 0.0092, Training Accuracy= 0.718\n",
      "Epoch 57, CIFAR-10 Batch 5:  Minibatch Loss= 0.0053, Training Accuracy= 0.718\n",
      "Epoch 58, CIFAR-10 Batch 1:  Minibatch Loss= 0.0090, Training Accuracy= 0.701\n",
      "Epoch 58, CIFAR-10 Batch 2:  Minibatch Loss= 0.0086, Training Accuracy= 0.708\n",
      "Epoch 58, CIFAR-10 Batch 3:  Minibatch Loss= 0.0073, Training Accuracy= 0.716\n",
      "Epoch 58, CIFAR-10 Batch 4:  Minibatch Loss= 0.0091, Training Accuracy= 0.713\n",
      "Epoch 58, CIFAR-10 Batch 5:  Minibatch Loss= 0.0045, Training Accuracy= 0.715\n",
      "Epoch 59, CIFAR-10 Batch 1:  Minibatch Loss= 0.0062, Training Accuracy= 0.703\n",
      "Epoch 59, CIFAR-10 Batch 2:  Minibatch Loss= 0.0062, Training Accuracy= 0.717\n",
      "Epoch 59, CIFAR-10 Batch 3:  Minibatch Loss= 0.0089, Training Accuracy= 0.718\n",
      "Epoch 59, CIFAR-10 Batch 4:  Minibatch Loss= 0.0082, Training Accuracy= 0.715\n",
      "Epoch 59, CIFAR-10 Batch 5:  Minibatch Loss= 0.0050, Training Accuracy= 0.715\n",
      "Epoch 60, CIFAR-10 Batch 1:  Minibatch Loss= 0.0056, Training Accuracy= 0.709\n",
      "Epoch 60, CIFAR-10 Batch 2:  Minibatch Loss= 0.0055, Training Accuracy= 0.719\n",
      "Epoch 60, CIFAR-10 Batch 3:  Minibatch Loss= 0.0049, Training Accuracy= 0.712\n",
      "Epoch 60, CIFAR-10 Batch 4:  Minibatch Loss= 0.0050, Training Accuracy= 0.710\n",
      "Epoch 60, CIFAR-10 Batch 5:  Minibatch Loss= 0.0032, Training Accuracy= 0.713\n",
      "Epoch 61, CIFAR-10 Batch 1:  Minibatch Loss= 0.0034, Training Accuracy= 0.715\n",
      "Epoch 61, CIFAR-10 Batch 2:  Minibatch Loss= 0.0037, Training Accuracy= 0.715\n",
      "Epoch 61, CIFAR-10 Batch 3:  Minibatch Loss= 0.0074, Training Accuracy= 0.713\n",
      "Epoch 61, CIFAR-10 Batch 4:  Minibatch Loss= 0.0064, Training Accuracy= 0.709\n",
      "Epoch 61, CIFAR-10 Batch 5:  Minibatch Loss= 0.0053, Training Accuracy= 0.714\n",
      "Epoch 62, CIFAR-10 Batch 1:  Minibatch Loss= 0.0063, Training Accuracy= 0.707\n",
      "Epoch 62, CIFAR-10 Batch 2:  Minibatch Loss= 0.0047, Training Accuracy= 0.714\n",
      "Epoch 62, CIFAR-10 Batch 3:  Minibatch Loss= 0.0049, Training Accuracy= 0.715\n",
      "Epoch 62, CIFAR-10 Batch 4:  Minibatch Loss= 0.0068, Training Accuracy= 0.715\n",
      "Epoch 62, CIFAR-10 Batch 5:  Minibatch Loss= 0.0024, Training Accuracy= 0.716\n",
      "Epoch 63, CIFAR-10 Batch 1:  Minibatch Loss= 0.0039, Training Accuracy= 0.710\n",
      "Epoch 63, CIFAR-10 Batch 2:  Minibatch Loss= 0.0024, Training Accuracy= 0.711\n",
      "Epoch 63, CIFAR-10 Batch 3:  Minibatch Loss= 0.0068, Training Accuracy= 0.715\n",
      "Epoch 63, CIFAR-10 Batch 4:  Minibatch Loss= 0.0060, Training Accuracy= 0.716\n",
      "Epoch 63, CIFAR-10 Batch 5:  Minibatch Loss= 0.0038, Training Accuracy= 0.719\n",
      "Epoch 64, CIFAR-10 Batch 1:  Minibatch Loss= 0.0055, Training Accuracy= 0.708\n",
      "Epoch 64, CIFAR-10 Batch 2:  Minibatch Loss= 0.0032, Training Accuracy= 0.722\n",
      "Epoch 64, CIFAR-10 Batch 3:  Minibatch Loss= 0.0048, Training Accuracy= 0.719\n",
      "Epoch 64, CIFAR-10 Batch 4:  Minibatch Loss= 0.0039, Training Accuracy= 0.713\n",
      "Epoch 64, CIFAR-10 Batch 5:  Minibatch Loss= 0.0043, Training Accuracy= 0.711\n",
      "Epoch 65, CIFAR-10 Batch 1:  Minibatch Loss= 0.0029, Training Accuracy= 0.711\n",
      "Epoch 65, CIFAR-10 Batch 2:  Minibatch Loss= 0.0018, Training Accuracy= 0.712\n",
      "Epoch 65, CIFAR-10 Batch 3:  Minibatch Loss= 0.0049, Training Accuracy= 0.713\n",
      "Epoch 65, CIFAR-10 Batch 4:  Minibatch Loss= 0.0038, Training Accuracy= 0.711\n",
      "Epoch 65, CIFAR-10 Batch 5:  Minibatch Loss= 0.0024, Training Accuracy= 0.714\n",
      "Epoch 66, CIFAR-10 Batch 1:  Minibatch Loss= 0.0077, Training Accuracy= 0.695\n",
      "Epoch 66, CIFAR-10 Batch 2:  Minibatch Loss= 0.0031, Training Accuracy= 0.717\n",
      "Epoch 66, CIFAR-10 Batch 3:  Minibatch Loss= 0.0042, Training Accuracy= 0.711\n",
      "Epoch 66, CIFAR-10 Batch 4:  Minibatch Loss= 0.0026, Training Accuracy= 0.714\n",
      "Epoch 66, CIFAR-10 Batch 5:  Minibatch Loss= 0.0017, Training Accuracy= 0.716\n",
      "Epoch 67, CIFAR-10 Batch 1:  Minibatch Loss= 0.0067, Training Accuracy= 0.706\n",
      "Epoch 67, CIFAR-10 Batch 2:  Minibatch Loss= 0.0028, Training Accuracy= 0.712\n",
      "Epoch 67, CIFAR-10 Batch 3:  Minibatch Loss= 0.0053, Training Accuracy= 0.705\n",
      "Epoch 67, CIFAR-10 Batch 4:  Minibatch Loss= 0.0043, Training Accuracy= 0.709\n",
      "Epoch 67, CIFAR-10 Batch 5:  Minibatch Loss= 0.0021, Training Accuracy= 0.714\n",
      "Epoch 68, CIFAR-10 Batch 1:  Minibatch Loss= 0.0030, Training Accuracy= 0.704\n",
      "Epoch 68, CIFAR-10 Batch 2:  Minibatch Loss= 0.0028, Training Accuracy= 0.716\n",
      "Epoch 68, CIFAR-10 Batch 3:  Minibatch Loss= 0.0031, Training Accuracy= 0.716\n",
      "Epoch 68, CIFAR-10 Batch 4:  Minibatch Loss= 0.0050, Training Accuracy= 0.708\n",
      "Epoch 68, CIFAR-10 Batch 5:  Minibatch Loss= 0.0018, Training Accuracy= 0.715\n",
      "Epoch 69, CIFAR-10 Batch 1:  Minibatch Loss= 0.0022, Training Accuracy= 0.705\n",
      "Epoch 69, CIFAR-10 Batch 2:  Minibatch Loss= 0.0021, Training Accuracy= 0.718\n",
      "Epoch 69, CIFAR-10 Batch 3:  Minibatch Loss= 0.0025, Training Accuracy= 0.716\n",
      "Epoch 69, CIFAR-10 Batch 4:  Minibatch Loss= 0.0037, Training Accuracy= 0.716\n",
      "Epoch 69, CIFAR-10 Batch 5:  Minibatch Loss= 0.0040, Training Accuracy= 0.719\n",
      "Epoch 70, CIFAR-10 Batch 1:  Minibatch Loss= 0.0017, Training Accuracy= 0.703\n",
      "Epoch 70, CIFAR-10 Batch 2:  Minibatch Loss= 0.0018, Training Accuracy= 0.712\n",
      "Epoch 70, CIFAR-10 Batch 3:  Minibatch Loss= 0.0024, Training Accuracy= 0.712\n",
      "Epoch 70, CIFAR-10 Batch 4:  Minibatch Loss= 0.0019, Training Accuracy= 0.713\n",
      "Epoch 70, CIFAR-10 Batch 5:  Minibatch Loss= 0.0018, Training Accuracy= 0.715\n",
      "Epoch 71, CIFAR-10 Batch 1:  Minibatch Loss= 0.0046, Training Accuracy= 0.701\n",
      "Epoch 71, CIFAR-10 Batch 2:  Minibatch Loss= 0.0012, Training Accuracy= 0.716\n",
      "Epoch 71, CIFAR-10 Batch 3:  Minibatch Loss= 0.0029, Training Accuracy= 0.722\n",
      "Epoch 71, CIFAR-10 Batch 4:  Minibatch Loss= 0.0028, Training Accuracy= 0.717\n",
      "Epoch 71, CIFAR-10 Batch 5:  Minibatch Loss= 0.0019, Training Accuracy= 0.717\n",
      "Epoch 72, CIFAR-10 Batch 1:  Minibatch Loss= 0.0049, Training Accuracy= 0.712\n",
      "Epoch 72, CIFAR-10 Batch 2:  Minibatch Loss= 0.0021, Training Accuracy= 0.714\n",
      "Epoch 72, CIFAR-10 Batch 3:  Minibatch Loss= 0.0023, Training Accuracy= 0.718\n",
      "Epoch 72, CIFAR-10 Batch 4:  Minibatch Loss= 0.0043, Training Accuracy= 0.722\n",
      "Epoch 72, CIFAR-10 Batch 5:  Minibatch Loss= 0.0018, Training Accuracy= 0.719\n",
      "Epoch 73, CIFAR-10 Batch 1:  Minibatch Loss= 0.0017, Training Accuracy= 0.715\n",
      "Epoch 73, CIFAR-10 Batch 2:  Minibatch Loss= 0.0039, Training Accuracy= 0.718\n",
      "Epoch 73, CIFAR-10 Batch 3:  Minibatch Loss= 0.0033, Training Accuracy= 0.715\n",
      "Epoch 73, CIFAR-10 Batch 4:  Minibatch Loss= 0.0023, Training Accuracy= 0.721\n",
      "Epoch 73, CIFAR-10 Batch 5:  Minibatch Loss= 0.0034, Training Accuracy= 0.717\n",
      "Epoch 74, CIFAR-10 Batch 1:  Minibatch Loss= 0.0018, Training Accuracy= 0.719\n",
      "Epoch 74, CIFAR-10 Batch 2:  Minibatch Loss= 0.0015, Training Accuracy= 0.718\n",
      "Epoch 74, CIFAR-10 Batch 3:  Minibatch Loss= 0.0018, Training Accuracy= 0.716\n",
      "Epoch 74, CIFAR-10 Batch 4:  Minibatch Loss= 0.0029, Training Accuracy= 0.719\n",
      "Epoch 74, CIFAR-10 Batch 5:  Minibatch Loss= 0.0011, Training Accuracy= 0.721\n",
      "Epoch 75, CIFAR-10 Batch 1:  Minibatch Loss= 0.0019, Training Accuracy= 0.718\n",
      "Epoch 75, CIFAR-10 Batch 2:  Minibatch Loss= 0.0018, Training Accuracy= 0.717\n",
      "Epoch 75, CIFAR-10 Batch 3:  Minibatch Loss= 0.0026, Training Accuracy= 0.717\n",
      "Epoch 75, CIFAR-10 Batch 4:  Minibatch Loss= 0.0014, Training Accuracy= 0.721\n",
      "Epoch 75, CIFAR-10 Batch 5:  Minibatch Loss= 0.0014, Training Accuracy= 0.721\n",
      "Epoch 76, CIFAR-10 Batch 1:  Minibatch Loss= 0.0040, Training Accuracy= 0.713\n",
      "Epoch 76, CIFAR-10 Batch 2:  Minibatch Loss= 0.0019, Training Accuracy= 0.724\n",
      "Epoch 76, CIFAR-10 Batch 3:  Minibatch Loss= 0.0013, Training Accuracy= 0.714\n",
      "Epoch 76, CIFAR-10 Batch 4:  Minibatch Loss= 0.0019, Training Accuracy= 0.719\n",
      "Epoch 76, CIFAR-10 Batch 5:  Minibatch Loss= 0.0013, Training Accuracy= 0.718\n",
      "Epoch 77, CIFAR-10 Batch 1:  Minibatch Loss= 0.0012, Training Accuracy= 0.714\n",
      "Epoch 77, CIFAR-10 Batch 2:  Minibatch Loss= 0.0017, Training Accuracy= 0.721\n",
      "Epoch 77, CIFAR-10 Batch 3:  Minibatch Loss= 0.0013, Training Accuracy= 0.718\n",
      "Epoch 77, CIFAR-10 Batch 4:  Minibatch Loss= 0.0020, Training Accuracy= 0.723\n",
      "Epoch 77, CIFAR-10 Batch 5:  Minibatch Loss= 0.0014, Training Accuracy= 0.719\n",
      "Epoch 78, CIFAR-10 Batch 1:  Minibatch Loss= 0.0020, Training Accuracy= 0.712\n",
      "Epoch 78, CIFAR-10 Batch 2:  Minibatch Loss= 0.0007, Training Accuracy= 0.720\n",
      "Epoch 78, CIFAR-10 Batch 3:  Minibatch Loss= 0.0008, Training Accuracy= 0.720\n",
      "Epoch 78, CIFAR-10 Batch 4:  Minibatch Loss= 0.0011, Training Accuracy= 0.717\n",
      "Epoch 78, CIFAR-10 Batch 5:  Minibatch Loss= 0.0008, Training Accuracy= 0.720\n",
      "Epoch 79, CIFAR-10 Batch 1:  Minibatch Loss= 0.0014, Training Accuracy= 0.714\n",
      "Epoch 79, CIFAR-10 Batch 2:  Minibatch Loss= 0.0011, Training Accuracy= 0.712\n",
      "Epoch 79, CIFAR-10 Batch 3:  Minibatch Loss= 0.0009, Training Accuracy= 0.721\n",
      "Epoch 79, CIFAR-10 Batch 4:  Minibatch Loss= 0.0024, Training Accuracy= 0.715\n",
      "Epoch 79, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 80, CIFAR-10 Batch 1:  Minibatch Loss= 0.0020, Training Accuracy= 0.704\n",
      "Epoch 80, CIFAR-10 Batch 2:  Minibatch Loss= 0.0010, Training Accuracy= 0.718\n",
      "Epoch 81, CIFAR-10 Batch 1:  Minibatch Loss= 0.0022, Training Accuracy= 0.715\n",
      "Epoch 81, CIFAR-10 Batch 2:  Minibatch Loss= 0.0008, Training Accuracy= 0.723\n",
      "Epoch 81, CIFAR-10 Batch 3:  Minibatch Loss= 0.0009, Training Accuracy= 0.722\n",
      "Epoch 81, CIFAR-10 Batch 4:  Minibatch Loss= 0.0010, Training Accuracy= 0.718\n",
      "Epoch 81, CIFAR-10 Batch 5:  Minibatch Loss= 0.0013, Training Accuracy= 0.716\n",
      "Epoch 82, CIFAR-10 Batch 1:  Minibatch Loss= 0.0012, Training Accuracy= 0.725\n",
      "Epoch 82, CIFAR-10 Batch 2:  Minibatch Loss= 0.0010, Training Accuracy= 0.712\n",
      "Epoch 82, CIFAR-10 Batch 3:  Minibatch Loss= 0.0009, Training Accuracy= 0.719\n",
      "Epoch 82, CIFAR-10 Batch 4:  Minibatch Loss= 0.0020, Training Accuracy= 0.722\n",
      "Epoch 82, CIFAR-10 Batch 5:  Minibatch Loss= 0.0007, Training Accuracy= 0.717\n",
      "Epoch 83, CIFAR-10 Batch 1:  Minibatch Loss= 0.0019, Training Accuracy= 0.718\n",
      "Epoch 83, CIFAR-10 Batch 2:  Minibatch Loss= 0.0009, Training Accuracy= 0.721\n",
      "Epoch 83, CIFAR-10 Batch 3:  Minibatch Loss= 0.0013, Training Accuracy= 0.725\n",
      "Epoch 83, CIFAR-10 Batch 4:  Minibatch Loss= 0.0013, Training Accuracy= 0.721\n",
      "Epoch 83, CIFAR-10 Batch 5:  Minibatch Loss= 0.0013, Training Accuracy= 0.716\n",
      "Epoch 84, CIFAR-10 Batch 1:  Minibatch Loss= 0.0006, Training Accuracy= 0.715\n",
      "Epoch 84, CIFAR-10 Batch 2:  Minibatch Loss= 0.0010, Training Accuracy= 0.712\n",
      "Epoch 84, CIFAR-10 Batch 3:  Minibatch Loss= 0.0012, Training Accuracy= 0.721\n",
      "Epoch 84, CIFAR-10 Batch 4:  Minibatch Loss= 0.0009, Training Accuracy= 0.713\n",
      "Epoch 84, CIFAR-10 Batch 5:  Minibatch Loss= 0.0007, Training Accuracy= 0.715\n",
      "Epoch 85, CIFAR-10 Batch 1:  Minibatch Loss= 0.0007, Training Accuracy= 0.712\n",
      "Epoch 85, CIFAR-10 Batch 2:  Minibatch Loss= 0.0031, Training Accuracy= 0.719\n",
      "Epoch 85, CIFAR-10 Batch 3:  Minibatch Loss= 0.0005, Training Accuracy= 0.725\n",
      "Epoch 85, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.722\n",
      "Epoch 85, CIFAR-10 Batch 5:  Minibatch Loss= 0.0010, Training Accuracy= 0.716\n",
      "Epoch 86, CIFAR-10 Batch 1:  Minibatch Loss= 0.0008, Training Accuracy= 0.722\n",
      "Epoch 86, CIFAR-10 Batch 2:  Minibatch Loss= 0.0007, Training Accuracy= 0.718\n",
      "Epoch 86, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.724\n",
      "Epoch 86, CIFAR-10 Batch 4:  Minibatch Loss= 0.0006, Training Accuracy= 0.724\n",
      "Epoch 86, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.722\n",
      "Epoch 87, CIFAR-10 Batch 1:  Minibatch Loss= 0.0011, Training Accuracy= 0.717\n",
      "Epoch 87, CIFAR-10 Batch 2:  Minibatch Loss= 0.0011, Training Accuracy= 0.725\n",
      "Epoch 87, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.722\n",
      "Epoch 87, CIFAR-10 Batch 4:  Minibatch Loss= 0.0010, Training Accuracy= 0.718\n",
      "Epoch 87, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.716\n",
      "Epoch 88, CIFAR-10 Batch 1:  Minibatch Loss= 0.0007, Training Accuracy= 0.720\n",
      "Epoch 88, CIFAR-10 Batch 2:  Minibatch Loss= 0.0006, Training Accuracy= 0.717\n",
      "Epoch 88, CIFAR-10 Batch 3:  Minibatch Loss= 0.0006, Training Accuracy= 0.714\n",
      "Epoch 88, CIFAR-10 Batch 4:  Minibatch Loss= 0.0015, Training Accuracy= 0.718\n",
      "Epoch 88, CIFAR-10 Batch 5:  Minibatch Loss= 0.0009, Training Accuracy= 0.718\n",
      "Epoch 89, CIFAR-10 Batch 1:  Minibatch Loss= 0.0007, Training Accuracy= 0.720\n",
      "Epoch 89, CIFAR-10 Batch 2:  Minibatch Loss= 0.0006, Training Accuracy= 0.723\n",
      "Epoch 89, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.723\n",
      "Epoch 90, CIFAR-10 Batch 1:  Minibatch Loss= 0.0008, Training Accuracy= 0.725\n",
      "Epoch 90, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 90, CIFAR-10 Batch 3:  Minibatch Loss= 0.0005, Training Accuracy= 0.718\n",
      "Epoch 90, CIFAR-10 Batch 4:  Minibatch Loss= 0.0010, Training Accuracy= 0.711\n",
      "Epoch 90, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.712\n",
      "Epoch 91, CIFAR-10 Batch 1:  Minibatch Loss= 0.0005, Training Accuracy= 0.719\n",
      "Epoch 91, CIFAR-10 Batch 2:  Minibatch Loss= 0.0004, Training Accuracy= 0.724\n",
      "Epoch 91, CIFAR-10 Batch 3:  Minibatch Loss= 0.0010, Training Accuracy= 0.711\n",
      "Epoch 91, CIFAR-10 Batch 4:  Minibatch Loss= 0.0020, Training Accuracy= 0.714\n",
      "Epoch 91, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 92, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.724\n",
      "Epoch 92, CIFAR-10 Batch 2:  Minibatch Loss= 0.0005, Training Accuracy= 0.719\n",
      "Epoch 92, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 92, CIFAR-10 Batch 4:  Minibatch Loss= 0.0012, Training Accuracy= 0.720\n",
      "Epoch 92, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.713\n",
      "Epoch 93, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 93, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 93, CIFAR-10 Batch 3:  Minibatch Loss= 0.0013, Training Accuracy= 0.718\n",
      "Epoch 93, CIFAR-10 Batch 4:  Minibatch Loss= 0.0011, Training Accuracy= 0.720\n",
      "Epoch 93, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.716\n",
      "Epoch 94, CIFAR-10 Batch 1:  Minibatch Loss= 0.0008, Training Accuracy= 0.715\n",
      "Epoch 94, CIFAR-10 Batch 2:  Minibatch Loss= 0.0005, Training Accuracy= 0.718\n",
      "Epoch 94, CIFAR-10 Batch 3:  Minibatch Loss= 0.0006, Training Accuracy= 0.709\n",
      "Epoch 94, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.710\n",
      "Epoch 94, CIFAR-10 Batch 5:  Minibatch Loss= 0.0008, Training Accuracy= 0.712\n",
      "Epoch 95, CIFAR-10 Batch 1:  Minibatch Loss= 0.0005, Training Accuracy= 0.719\n",
      "Epoch 95, CIFAR-10 Batch 2:  Minibatch Loss= 0.0009, Training Accuracy= 0.716\n",
      "Epoch 95, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 95, CIFAR-10 Batch 4:  Minibatch Loss= 0.0007, Training Accuracy= 0.707\n",
      "Epoch 95, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 96, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.715\n",
      "Epoch 96, CIFAR-10 Batch 2:  Minibatch Loss= 0.0005, Training Accuracy= 0.722\n",
      "Epoch 96, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 96, CIFAR-10 Batch 4:  Minibatch Loss= 0.0012, Training Accuracy= 0.720\n",
      "Epoch 96, CIFAR-10 Batch 5:  Minibatch Loss= 0.0009, Training Accuracy= 0.714\n",
      "Epoch 97, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.724\n",
      "Epoch 97, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 97, CIFAR-10 Batch 3:  Minibatch Loss= 0.0006, Training Accuracy= 0.718\n",
      "Epoch 97, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.716\n",
      "Epoch 97, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.721\n",
      "Epoch 98, CIFAR-10 Batch 1:  Minibatch Loss= 0.0008, Training Accuracy= 0.719\n",
      "Epoch 98, CIFAR-10 Batch 2:  Minibatch Loss= 0.0006, Training Accuracy= 0.725\n",
      "Epoch 98, CIFAR-10 Batch 3:  Minibatch Loss= 0.0005, Training Accuracy= 0.720\n",
      "Epoch 98, CIFAR-10 Batch 4:  Minibatch Loss= 0.0030, Training Accuracy= 0.717\n",
      "Epoch 98, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.715\n",
      "Epoch 99, CIFAR-10 Batch 1:  Minibatch Loss= 0.0006, Training Accuracy= 0.713\n",
      "Epoch 99, CIFAR-10 Batch 2:  Minibatch Loss= 0.0005, Training Accuracy= 0.726\n",
      "Epoch 99, CIFAR-10 Batch 3:  Minibatch Loss= 0.0005, Training Accuracy= 0.717\n",
      "Epoch 99, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.720\n",
      "Epoch 99, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 100, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 100, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.724\n",
      "Epoch 100, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.721\n",
      "Epoch 100, CIFAR-10 Batch 4:  Minibatch Loss= 0.0007, Training Accuracy= 0.718\n",
      "Epoch 100, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.713\n",
      "Epoch 101, CIFAR-10 Batch 1:  Minibatch Loss= 0.0008, Training Accuracy= 0.717\n",
      "Epoch 101, CIFAR-10 Batch 2:  Minibatch Loss= 0.0006, Training Accuracy= 0.719\n",
      "Epoch 101, CIFAR-10 Batch 3:  Minibatch Loss= 0.0006, Training Accuracy= 0.724\n",
      "Epoch 101, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.723\n",
      "Epoch 101, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.720\n",
      "Epoch 102, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 102, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 102, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 102, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.720\n",
      "Epoch 102, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.715\n",
      "Epoch 103, CIFAR-10 Batch 1:  Minibatch Loss= 0.0006, Training Accuracy= 0.721\n",
      "Epoch 103, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 104, CIFAR-10 Batch 1:  Minibatch Loss= 0.0006, Training Accuracy= 0.715\n",
      "Epoch 104, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.725\n",
      "Epoch 104, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 104, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 104, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 105, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 105, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 105, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 105, CIFAR-10 Batch 4:  Minibatch Loss= 0.0006, Training Accuracy= 0.722\n",
      "Epoch 105, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.716\n",
      "Epoch 106, CIFAR-10 Batch 1:  Minibatch Loss= 0.0014, Training Accuracy= 0.713\n",
      "Epoch 106, CIFAR-10 Batch 2:  Minibatch Loss= 0.0004, Training Accuracy= 0.721\n",
      "Epoch 106, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.719\n",
      "Epoch 106, CIFAR-10 Batch 4:  Minibatch Loss= 0.0006, Training Accuracy= 0.719\n",
      "Epoch 106, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 107, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 107, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.726\n",
      "Epoch 107, CIFAR-10 Batch 3:  Minibatch Loss= 0.0006, Training Accuracy= 0.716\n",
      "Epoch 107, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.713\n",
      "Epoch 107, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.718\n",
      "Epoch 108, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.721\n",
      "Epoch 108, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.726\n",
      "Epoch 108, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 108, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.718\n",
      "Epoch 108, CIFAR-10 Batch 5:  Minibatch Loss= 0.0007, Training Accuracy= 0.719\n",
      "Epoch 109, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.718\n",
      "Epoch 109, CIFAR-10 Batch 2:  Minibatch Loss= 0.0004, Training Accuracy= 0.724\n",
      "Epoch 109, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.725\n",
      "Epoch 109, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 109, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 110, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.724\n",
      "Epoch 110, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 110, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 110, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 110, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 111, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 111, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.723\n",
      "Epoch 111, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.720\n",
      "Epoch 111, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.723\n",
      "Epoch 111, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.711\n",
      "Epoch 112, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.720\n",
      "Epoch 112, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 112, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.720\n",
      "Epoch 112, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.722\n",
      "Epoch 112, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 113, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 113, CIFAR-10 Batch 2:  Minibatch Loss= 0.0005, Training Accuracy= 0.717\n",
      "Epoch 113, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 113, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.716\n",
      "Epoch 113, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 114, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.722\n",
      "Epoch 114, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.725\n",
      "Epoch 114, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.721\n",
      "Epoch 114, CIFAR-10 Batch 4:  Minibatch Loss= 0.0009, Training Accuracy= 0.716\n",
      "Epoch 114, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 115, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.715\n",
      "Epoch 115, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 115, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.725\n",
      "Epoch 115, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 115, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 116, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 116, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 116, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 116, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.716\n",
      "Epoch 116, CIFAR-10 Batch 5:  Minibatch Loss= 0.0005, Training Accuracy= 0.721\n",
      "Epoch 117, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 117, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 117, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.724\n",
      "Epoch 117, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.722\n",
      "Epoch 117, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 118, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 118, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.726\n",
      "Epoch 118, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 118, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.723\n",
      "Epoch 118, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.721\n",
      "Epoch 119, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.724\n",
      "Epoch 119, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.724\n",
      "Epoch 119, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.719\n",
      "Epoch 119, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 119, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 120, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.724\n",
      "Epoch 120, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 120, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 120, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.714\n",
      "Epoch 120, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 121, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 121, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 121, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 121, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 121, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 122, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.722\n",
      "Epoch 122, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 122, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 122, CIFAR-10 Batch 4:  Minibatch Loss= 0.0007, Training Accuracy= 0.719\n",
      "Epoch 122, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 123, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 123, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.722\n",
      "Epoch 123, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 123, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 123, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 124, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 124, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 124, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 124, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.728\n",
      "Epoch 124, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 125, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 125, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.724\n",
      "Epoch 125, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.725\n",
      "Epoch 125, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.728\n",
      "Epoch 125, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 126, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 126, CIFAR-10 Batch 2:  Minibatch Loss= 0.0004, Training Accuracy= 0.726\n",
      "Epoch 126, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 126, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 126, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 127, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 127, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 127, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 127, CIFAR-10 Batch 4:  Minibatch Loss= 0.0012, Training Accuracy= 0.714\n",
      "Epoch 127, CIFAR-10 Batch 5:  Minibatch Loss= 0.0006, Training Accuracy= 0.725\n",
      "Epoch 128, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 128, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 128, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.724\n",
      "Epoch 128, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.715\n",
      "Epoch 128, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 129, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 129, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.716\n",
      "Epoch 129, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 129, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 130, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 130, CIFAR-10 Batch 2:  Minibatch Loss= 0.0004, Training Accuracy= 0.717\n",
      "Epoch 130, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.720\n",
      "Epoch 130, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 130, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 131, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 131, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.718\n",
      "Epoch 131, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.725\n",
      "Epoch 131, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 131, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 132, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.722\n",
      "Epoch 132, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 132, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 132, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 132, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 133, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 133, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 133, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 133, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 133, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.716\n",
      "Epoch 134, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 134, CIFAR-10 Batch 2:  Minibatch Loss= 0.0009, Training Accuracy= 0.719\n",
      "Epoch 134, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.728\n",
      "Epoch 134, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.717\n",
      "Epoch 134, CIFAR-10 Batch 5:  Minibatch Loss= 0.0004, Training Accuracy= 0.718\n",
      "Epoch 135, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 135, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 135, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 135, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 135, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 136, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.724\n",
      "Epoch 136, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 136, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 136, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 136, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.718\n",
      "Epoch 137, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 137, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 137, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 137, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 137, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 138, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 138, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 138, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 139, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 139, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 139, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 139, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 139, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 140, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 140, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 140, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 140, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.716\n",
      "Epoch 140, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 141, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 141, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 141, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 141, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 141, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.716\n",
      "Epoch 142, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 142, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 142, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 142, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.712\n",
      "Epoch 142, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 143, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 143, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 143, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 143, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.715\n",
      "Epoch 143, CIFAR-10 Batch 5:  Minibatch Loss= 0.0003, Training Accuracy= 0.712\n",
      "Epoch 144, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 144, CIFAR-10 Batch 2:  Minibatch Loss= 0.0003, Training Accuracy= 0.719\n",
      "Epoch 144, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 144, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 144, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 145, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 145, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 145, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 145, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.712\n",
      "Epoch 145, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 146, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.716\n",
      "Epoch 146, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 146, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 146, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.709\n",
      "Epoch 146, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.724\n",
      "Epoch 147, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 147, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 147, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 147, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.728\n",
      "Epoch 147, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 148, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 148, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 148, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 148, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 148, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.711\n",
      "Epoch 149, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.723\n",
      "Epoch 149, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 149, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 149, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 149, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 150, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 150, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.724\n",
      "Epoch 150, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 150, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 150, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 151, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 151, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.724\n",
      "Epoch 151, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 151, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.720\n",
      "Epoch 151, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 152, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 152, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 152, CIFAR-10 Batch 3:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 152, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 152, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 153, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.715\n",
      "Epoch 153, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 153, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 153, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.723\n",
      "Epoch 153, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 154, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 154, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 154, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 154, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 154, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 155, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 155, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 155, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 155, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.722\n",
      "Epoch 155, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 156, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 156, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 156, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 156, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 156, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 157, CIFAR-10 Batch 1:  Minibatch Loss= 0.0004, Training Accuracy= 0.723\n",
      "Epoch 157, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 157, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 157, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.721\n",
      "Epoch 157, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 158, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 158, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 158, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 158, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 158, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 159, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 159, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 159, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.708\n",
      "Epoch 159, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.721\n",
      "Epoch 159, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.714\n",
      "Epoch 160, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 160, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 160, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 160, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 160, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 161, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.713\n",
      "Epoch 161, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 161, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 161, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.714\n",
      "Epoch 161, CIFAR-10 Batch 5:  Minibatch Loss= 0.0007, Training Accuracy= 0.709\n",
      "Epoch 162, CIFAR-10 Batch 1:  Minibatch Loss= 0.0003, Training Accuracy= 0.714\n",
      "Epoch 162, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 162, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 162, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.719\n",
      "Epoch 162, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 163, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 163, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 163, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 163, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 163, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 164, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 164, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 164, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 164, CIFAR-10 Batch 4:  Minibatch Loss= 0.0006, Training Accuracy= 0.716\n",
      "Epoch 164, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.711\n",
      "Epoch 165, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 165, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 165, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 165, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.715\n",
      "Epoch 165, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 166, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 166, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 166, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 166, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.728\n",
      "Epoch 166, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.708\n",
      "Epoch 167, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 167, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 167, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 167, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 167, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 168, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 168, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 168, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 168, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.707\n",
      "Epoch 168, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 169, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 169, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 169, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 169, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 169, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 170, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 170, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 170, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 170, CIFAR-10 Batch 4:  Minibatch Loss= 0.0005, Training Accuracy= 0.717\n",
      "Epoch 170, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 171, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 171, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 171, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 171, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 171, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 172, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 172, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 172, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 172, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 172, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 173, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 173, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 173, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 173, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 173, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.715\n",
      "Epoch 174, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 174, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 174, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 174, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 174, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 175, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 175, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 175, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 175, CIFAR-10 Batch 4:  Minibatch Loss= 0.0004, Training Accuracy= 0.714\n",
      "Epoch 175, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 176, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 176, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 176, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 176, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 176, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.707\n",
      "Epoch 177, CIFAR-10 Batch 1:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 177, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 177, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 177, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.719\n",
      "Epoch 177, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 178, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 178, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 178, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 178, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 178, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.710\n",
      "Epoch 179, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 179, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 179, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 179, CIFAR-10 Batch 4:  Minibatch Loss= 0.0010, Training Accuracy= 0.719\n",
      "Epoch 179, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 180, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 180, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 180, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 180, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 180, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 181, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 181, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 181, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 181, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.710\n",
      "Epoch 181, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 182, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 182, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 182, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 182, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.715\n",
      "Epoch 182, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 183, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 183, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 183, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 183, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.715\n",
      "Epoch 183, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 184, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 184, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 184, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 184, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 184, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 185, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 185, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 185, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 185, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 185, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 186, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 186, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 186, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 186, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 186, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 187, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 187, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 187, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 187, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.718\n",
      "Epoch 187, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 188, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 188, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 188, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 188, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 188, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 189, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 189, CIFAR-10 Batch 2:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 189, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 189, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 189, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.703\n",
      "Epoch 190, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 190, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 190, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 190, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 190, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 191, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.711\n",
      "Epoch 191, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 191, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 191, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.716\n",
      "Epoch 191, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 192, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 192, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 192, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 192, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.713\n",
      "Epoch 192, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 193, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 193, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 193, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 193, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 193, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 194, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 194, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 194, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 194, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 194, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 195, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 195, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 195, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 195, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 195, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 196, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 196, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 196, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 196, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 196, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 197, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 197, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 197, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 197, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 197, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 198, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 198, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 198, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 198, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 198, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 199, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 199, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 199, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 199, CIFAR-10 Batch 4:  Minibatch Loss= 0.0003, Training Accuracy= 0.716\n",
      "Epoch 199, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 200, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 200, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 200, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 200, CIFAR-10 Batch 4:  Minibatch Loss= 0.0008, Training Accuracy= 0.717\n",
      "Epoch 200, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 201, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 201, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 201, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 201, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 201, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 202, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 202, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 202, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 202, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 202, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 203, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 203, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 203, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 203, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.726\n",
      "Epoch 203, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 204, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 204, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 204, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 204, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 204, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 205, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 205, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 205, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.723\n",
      "Epoch 205, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.708\n",
      "Epoch 205, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 206, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 206, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 206, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 206, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 206, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 207, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 207, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 207, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 207, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 207, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 208, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 208, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 208, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.717\n",
      "Epoch 208, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 208, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 209, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 209, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 209, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 209, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 209, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 210, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 210, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 210, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 210, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 210, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 211, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 211, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 211, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 211, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 211, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 212, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 212, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 212, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 212, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 212, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 213, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 213, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 213, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 213, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 213, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 214, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 214, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 214, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 214, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 214, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 215, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 215, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 215, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 215, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 215, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 216, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 216, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 216, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 216, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 216, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 217, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 217, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 217, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 217, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 217, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 218, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 218, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 218, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 218, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 218, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 219, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 219, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 219, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 219, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 219, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.709\n",
      "Epoch 220, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 220, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 220, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 220, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 220, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 221, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 221, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 221, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 221, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.712\n",
      "Epoch 221, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 222, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 222, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 222, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 222, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 222, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 223, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 223, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 223, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 223, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.713\n",
      "Epoch 223, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 224, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 224, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 224, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.725\n",
      "Epoch 224, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 224, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 225, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 225, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 225, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 225, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 225, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 226, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 226, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 226, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 226, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 226, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 227, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 227, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 227, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 227, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 227, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 228, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 228, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 228, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 228, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 228, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 229, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 229, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 229, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 229, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 229, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 230, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 230, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 230, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 230, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 230, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 231, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 231, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 231, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 231, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 231, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 232, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 232, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 232, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 232, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 232, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 233, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 233, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.709\n",
      "Epoch 233, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.715\n",
      "Epoch 233, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.709\n",
      "Epoch 233, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 234, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 234, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 234, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 234, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 234, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 235, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 235, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 235, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 235, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 235, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 236, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 236, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 236, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 236, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 236, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 237, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 237, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 237, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 237, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 237, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 238, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 238, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 238, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 238, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 238, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 239, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 239, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 239, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 239, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 239, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 240, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 240, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 240, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 240, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 240, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 241, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 241, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 241, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 241, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 241, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 242, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 242, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 242, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 242, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 242, CIFAR-10 Batch 5:  Minibatch Loss= 0.0001, Training Accuracy= 0.716\n",
      "Epoch 243, CIFAR-10 Batch 1:  Minibatch Loss= 0.0014, Training Accuracy= 0.712\n",
      "Epoch 243, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 243, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 243, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 243, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 244, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 244, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 244, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 244, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 244, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 245, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 245, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 245, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 245, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 245, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 246, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 246, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 246, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 246, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 246, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 247, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 247, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 247, CIFAR-10 Batch 3:  Minibatch Loss= 0.0004, Training Accuracy= 0.716\n",
      "Epoch 247, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 247, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 248, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.714\n",
      "Epoch 248, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 248, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 248, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 248, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 249, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 249, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 249, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 249, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 249, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 250, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 250, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 250, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 250, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 250, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 251, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 251, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 251, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 251, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 251, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 252, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 252, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 252, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 252, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 252, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 253, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 253, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 253, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 253, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 253, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 254, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 254, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 254, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 254, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 254, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 255, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 255, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 255, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 255, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 255, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 256, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 256, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 256, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 256, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 256, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 257, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 257, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 257, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 257, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 257, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 258, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 258, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 258, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 258, CIFAR-10 Batch 4:  Minibatch Loss= 0.0002, Training Accuracy= 0.717\n",
      "Epoch 258, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 259, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 259, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 259, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 259, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 259, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 260, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 260, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 260, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 260, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 260, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 261, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 261, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 261, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 261, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 261, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 262, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 262, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 262, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 262, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 262, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.710\n",
      "Epoch 263, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 263, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 263, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 263, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 263, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 264, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 264, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 264, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 264, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.720\n",
      "Epoch 264, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 265, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 265, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 265, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 265, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 265, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 266, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 266, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 266, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 266, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 266, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 267, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 267, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 267, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 267, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 267, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 268, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 268, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 268, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 268, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 268, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 269, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 269, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 269, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 269, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 269, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 270, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 270, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 270, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.722\n",
      "Epoch 270, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 270, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 271, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 271, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 271, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 271, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.709\n",
      "Epoch 271, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 272, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 272, CIFAR-10 Batch 2:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 272, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 272, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 272, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 273, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.712\n",
      "Epoch 273, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 273, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 273, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 273, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 274, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 274, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 274, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 274, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 274, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 275, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 275, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 275, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 275, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 275, CIFAR-10 Batch 5:  Minibatch Loss= 0.0002, Training Accuracy= 0.713\n",
      "Epoch 276, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 276, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 276, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 276, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 276, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 277, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 277, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 277, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 277, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 277, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 278, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 278, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 278, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 278, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 278, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 279, CIFAR-10 Batch 1:  Minibatch Loss= 0.0001, Training Accuracy= 0.718\n",
      "Epoch 279, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 279, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 279, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 279, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 280, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 280, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 280, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 280, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 280, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 281, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 281, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 281, CIFAR-10 Batch 3:  Minibatch Loss= 0.0003, Training Accuracy= 0.721\n",
      "Epoch 281, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.711\n",
      "Epoch 281, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 282, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 282, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 282, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 282, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 282, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.707\n",
      "Epoch 283, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 283, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 283, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 283, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 283, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 284, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 284, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 284, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 284, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.719\n",
      "Epoch 284, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 285, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 285, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 285, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 285, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 285, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 286, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 286, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 286, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 286, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 286, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 287, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 287, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 287, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 287, CIFAR-10 Batch 4:  Minibatch Loss= 0.0001, Training Accuracy= 0.717\n",
      "Epoch 287, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 288, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 288, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 288, CIFAR-10 Batch 3:  Minibatch Loss= 0.0001, Training Accuracy= 0.721\n",
      "Epoch 288, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 288, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 289, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 289, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 289, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.730\n",
      "Epoch 289, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 289, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.726\n",
      "Epoch 290, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 290, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.728\n",
      "Epoch 290, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 290, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 290, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 291, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 291, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 291, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 291, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 291, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 292, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 292, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 292, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 292, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 292, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 293, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 293, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 293, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 294, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.713\n",
      "Epoch 294, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 294, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 294, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 294, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 295, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 295, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 295, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 295, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 295, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 296, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 296, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 296, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 296, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 296, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 297, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 297, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 297, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.725\n",
      "Epoch 297, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 297, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 298, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 298, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Epoch 298, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.728\n",
      "Epoch 298, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.716\n",
      "Epoch 298, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 299, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.723\n",
      "Epoch 299, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.721\n",
      "Epoch 299, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.722\n",
      "Epoch 299, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.724\n",
      "Epoch 299, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.714\n",
      "Epoch 300, CIFAR-10 Batch 1:  Minibatch Loss= 0.0000, Training Accuracy= 0.715\n",
      "Epoch 300, CIFAR-10 Batch 2:  Minibatch Loss= 0.0000, Training Accuracy= 0.719\n",
      "Epoch 300, CIFAR-10 Batch 3:  Minibatch Loss= 0.0000, Training Accuracy= 0.718\n",
      "Epoch 300, CIFAR-10 Batch 4:  Minibatch Loss= 0.0000, Training Accuracy= 0.717\n",
      "Epoch 300, CIFAR-10 Batch 5:  Minibatch Loss= 0.0000, Training Accuracy= 0.720\n",
      "Max_Acc = 72.960  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    accuracy_max = 0.0\n",
    "    max_ep = 20\n",
    "    i = 0\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            #返回当前学习率\n",
    "            current_acc = print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            #超过20次迭代，学习率不增加，则不再进行训练    ？？ 为什么没有跳出循环\n",
    "            if(accuracy_max <= current_acc):\n",
    "                accuracy_max = current_acc\n",
    "                i = 0\n",
    "            else:\n",
    "                i += 1\n",
    "                if(i==max_ep):\n",
    "                    break\n",
    "    print('Max_Acc ='+' {:.3f}  '.format(accuracy_max*100))        \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "模型已保存到本地。\n",
    "\n",
    "## 测试模型\n",
    "\n",
    "利用测试数据集测试你的模型。这将是最终的准确率。你的准确率应该高于 50%。如果没达到，请继续调整模型结构和参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6337859424920128\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2755e9ac438>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为何准确率只有50-80%？\n",
    "\n",
    "你可能想问，为何准确率不能更高了？首先，对于简单的 CNN 网络来说，50% 已经不低了。纯粹猜测的准确率为10%。但是，你可能注意到有人的准确率[远远超过 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)。这是因为我们还没有介绍所有的神经网络知识。我们还需要掌握一些其他技巧。\n",
    "\n",
    "## 提交项目\n",
    "\n",
    "提交项目时，确保先运行所有单元，然后再保存记事本。将 notebook 文件另存为“dlnd_image_classification.ipynb”，再在目录 \"File\" -> \"Download as\" 另存为 HTML 格式。请在提交的项目中包含 “helper.py” 和 “problem_unittests.py” 文件。\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
